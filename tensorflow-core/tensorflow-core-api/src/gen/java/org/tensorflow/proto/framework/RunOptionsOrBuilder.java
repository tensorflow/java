// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: tensorflow/core/protobuf/config.proto

package org.tensorflow.proto.framework;

public interface RunOptionsOrBuilder extends
    // @@protoc_insertion_point(interface_extends:tensorflow.RunOptions)
    com.google.protobuf.MessageOrBuilder {

  /**
   * <code>.tensorflow.RunOptions.TraceLevel trace_level = 1;</code>
   */
  int getTraceLevelValue();
  /**
   * <code>.tensorflow.RunOptions.TraceLevel trace_level = 1;</code>
   */
  org.tensorflow.proto.framework.RunOptions.TraceLevel getTraceLevel();

  /**
   * <pre>
   * Time to wait for operation to complete in milliseconds.
   * </pre>
   *
   * <code>int64 timeout_in_ms = 2;</code>
   */
  long getTimeoutInMs();

  /**
   * <pre>
   * The thread pool to use, if session_inter_op_thread_pool is configured.
   * To use the caller thread set this to -1 - this uses the caller thread
   * to execute Session::Run() and thus avoids a context switch. Using the
   * caller thread to execute Session::Run() should be done ONLY for simple
   * graphs, where the overhead of an additional context switch is
   * comparable with the overhead of Session::Run().
   * </pre>
   *
   * <code>int32 inter_op_thread_pool = 3;</code>
   */
  int getInterOpThreadPool();

  /**
   * <pre>
   * Whether the partition graph(s) executed by the executor(s) should be
   * outputted via RunMetadata.
   * </pre>
   *
   * <code>bool output_partition_graphs = 5;</code>
   */
  boolean getOutputPartitionGraphs();

  /**
   * <pre>
   * EXPERIMENTAL.  Options used to initialize DebuggerState, if enabled.
   * </pre>
   *
   * <code>.tensorflow.DebugOptions debug_options = 6;</code>
   */
  boolean hasDebugOptions();
  /**
   * <pre>
   * EXPERIMENTAL.  Options used to initialize DebuggerState, if enabled.
   * </pre>
   *
   * <code>.tensorflow.DebugOptions debug_options = 6;</code>
   */
  org.tensorflow.proto.framework.DebugOptions getDebugOptions();
  /**
   * <pre>
   * EXPERIMENTAL.  Options used to initialize DebuggerState, if enabled.
   * </pre>
   *
   * <code>.tensorflow.DebugOptions debug_options = 6;</code>
   */
  org.tensorflow.proto.framework.DebugOptionsOrBuilder getDebugOptionsOrBuilder();

  /**
   * <pre>
   * When enabled, causes tensor allocation information to be included in
   * the error message when the Run() call fails because the allocator ran
   * out of memory (OOM).
   * Enabling this option can slow down the Run() call.
   * </pre>
   *
   * <code>bool report_tensor_allocations_upon_oom = 7;</code>
   */
  boolean getReportTensorAllocationsUponOom();

  /**
   * <code>.tensorflow.RunOptions.Experimental experimental = 8;</code>
   */
  boolean hasExperimental();
  /**
   * <code>.tensorflow.RunOptions.Experimental experimental = 8;</code>
   */
  org.tensorflow.proto.framework.RunOptions.Experimental getExperimental();
  /**
   * <code>.tensorflow.RunOptions.Experimental experimental = 8;</code>
   */
  org.tensorflow.proto.framework.RunOptions.ExperimentalOrBuilder getExperimentalOrBuilder();
}
