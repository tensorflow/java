// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: tensorflow/core/protobuf/config.proto

package org.tensorflow.proto.framework;

/**
 * Protobuf type {@code tensorflow.GPUOptions}
 */
public  final class GPUOptions extends
    com.google.protobuf.GeneratedMessageV3 implements
    // @@protoc_insertion_point(message_implements:tensorflow.GPUOptions)
    GPUOptionsOrBuilder {
private static final long serialVersionUID = 0L;
  // Use GPUOptions.newBuilder() to construct.
  private GPUOptions(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
    super(builder);
  }
  private GPUOptions() {
    allocatorType_ = "";
    visibleDeviceList_ = "";
  }

  @java.lang.Override
  @SuppressWarnings({"unused"})
  protected java.lang.Object newInstance(
      UnusedPrivateParameter unused) {
    return new GPUOptions();
  }

  @java.lang.Override
  public final com.google.protobuf.UnknownFieldSet
  getUnknownFields() {
    return this.unknownFields;
  }
  private GPUOptions(
      com.google.protobuf.CodedInputStream input,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws com.google.protobuf.InvalidProtocolBufferException {
    this();
    if (extensionRegistry == null) {
      throw new java.lang.NullPointerException();
    }
    com.google.protobuf.UnknownFieldSet.Builder unknownFields =
        com.google.protobuf.UnknownFieldSet.newBuilder();
    try {
      boolean done = false;
      while (!done) {
        int tag = input.readTag();
        switch (tag) {
          case 0:
            done = true;
            break;
          case 9: {

            perProcessGpuMemoryFraction_ = input.readDouble();
            break;
          }
          case 18: {
            java.lang.String s = input.readStringRequireUtf8();

            allocatorType_ = s;
            break;
          }
          case 24: {

            deferredDeletionBytes_ = input.readInt64();
            break;
          }
          case 32: {

            allowGrowth_ = input.readBool();
            break;
          }
          case 42: {
            java.lang.String s = input.readStringRequireUtf8();

            visibleDeviceList_ = s;
            break;
          }
          case 48: {

            pollingActiveDelayUsecs_ = input.readInt32();
            break;
          }
          case 56: {

            pollingInactiveDelayMsecs_ = input.readInt32();
            break;
          }
          case 64: {

            forceGpuCompatible_ = input.readBool();
            break;
          }
          case 74: {
            org.tensorflow.proto.framework.GPUOptions.Experimental.Builder subBuilder = null;
            if (experimental_ != null) {
              subBuilder = experimental_.toBuilder();
            }
            experimental_ = input.readMessage(org.tensorflow.proto.framework.GPUOptions.Experimental.parser(), extensionRegistry);
            if (subBuilder != null) {
              subBuilder.mergeFrom(experimental_);
              experimental_ = subBuilder.buildPartial();
            }

            break;
          }
          default: {
            if (!parseUnknownField(
                input, unknownFields, extensionRegistry, tag)) {
              done = true;
            }
            break;
          }
        }
      }
    } catch (com.google.protobuf.InvalidProtocolBufferException e) {
      throw e.setUnfinishedMessage(this);
    } catch (java.io.IOException e) {
      throw new com.google.protobuf.InvalidProtocolBufferException(
          e).setUnfinishedMessage(this);
    } finally {
      this.unknownFields = unknownFields.build();
      makeExtensionsImmutable();
    }
  }
  public static final com.google.protobuf.Descriptors.Descriptor
      getDescriptor() {
    return org.tensorflow.proto.framework.ConfigProtos.internal_static_tensorflow_GPUOptions_descriptor;
  }

  @java.lang.Override
  protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internalGetFieldAccessorTable() {
    return org.tensorflow.proto.framework.ConfigProtos.internal_static_tensorflow_GPUOptions_fieldAccessorTable
        .ensureFieldAccessorsInitialized(
            org.tensorflow.proto.framework.GPUOptions.class, org.tensorflow.proto.framework.GPUOptions.Builder.class);
  }

  public interface ExperimentalOrBuilder extends
      // @@protoc_insertion_point(interface_extends:tensorflow.GPUOptions.Experimental)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     * The multi virtual device settings. If empty (not set), it will create
     * single virtual device on each visible GPU, according to the settings
     * in "visible_device_list" above. Otherwise, the number of elements in the
     * list must be the same as the number of visible GPUs (after
     * "visible_device_list" filtering if it is set), and the string represented
     * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
     * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
     * according to the order they appear in this list and the "memory_limit"
     * list inside each element. For example,
     *   visible_device_list = "1,0"
     *   virtual_devices { memory_limit: 1GB memory_limit: 2GB }
     *   virtual_devices {}
     * will create three virtual devices as:
     *   /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
     *   /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
     *   /device:GPU:2 -&gt; visible GPU 0 with all available memory
     * NOTE:
     * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
     *    at the same time.
     * 2. Currently this setting is per-process, not per-session. Using
     *    different settings in different sessions within same process will
     *    result in undefined behavior.
     * </pre>
     *
     * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
     */
    java.util.List<org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices> 
        getVirtualDevicesList();
    /**
     * <pre>
     * The multi virtual device settings. If empty (not set), it will create
     * single virtual device on each visible GPU, according to the settings
     * in "visible_device_list" above. Otherwise, the number of elements in the
     * list must be the same as the number of visible GPUs (after
     * "visible_device_list" filtering if it is set), and the string represented
     * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
     * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
     * according to the order they appear in this list and the "memory_limit"
     * list inside each element. For example,
     *   visible_device_list = "1,0"
     *   virtual_devices { memory_limit: 1GB memory_limit: 2GB }
     *   virtual_devices {}
     * will create three virtual devices as:
     *   /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
     *   /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
     *   /device:GPU:2 -&gt; visible GPU 0 with all available memory
     * NOTE:
     * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
     *    at the same time.
     * 2. Currently this setting is per-process, not per-session. Using
     *    different settings in different sessions within same process will
     *    result in undefined behavior.
     * </pre>
     *
     * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
     */
    org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices getVirtualDevices(int index);
    /**
     * <pre>
     * The multi virtual device settings. If empty (not set), it will create
     * single virtual device on each visible GPU, according to the settings
     * in "visible_device_list" above. Otherwise, the number of elements in the
     * list must be the same as the number of visible GPUs (after
     * "visible_device_list" filtering if it is set), and the string represented
     * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
     * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
     * according to the order they appear in this list and the "memory_limit"
     * list inside each element. For example,
     *   visible_device_list = "1,0"
     *   virtual_devices { memory_limit: 1GB memory_limit: 2GB }
     *   virtual_devices {}
     * will create three virtual devices as:
     *   /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
     *   /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
     *   /device:GPU:2 -&gt; visible GPU 0 with all available memory
     * NOTE:
     * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
     *    at the same time.
     * 2. Currently this setting is per-process, not per-session. Using
     *    different settings in different sessions within same process will
     *    result in undefined behavior.
     * </pre>
     *
     * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
     */
    int getVirtualDevicesCount();
    /**
     * <pre>
     * The multi virtual device settings. If empty (not set), it will create
     * single virtual device on each visible GPU, according to the settings
     * in "visible_device_list" above. Otherwise, the number of elements in the
     * list must be the same as the number of visible GPUs (after
     * "visible_device_list" filtering if it is set), and the string represented
     * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
     * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
     * according to the order they appear in this list and the "memory_limit"
     * list inside each element. For example,
     *   visible_device_list = "1,0"
     *   virtual_devices { memory_limit: 1GB memory_limit: 2GB }
     *   virtual_devices {}
     * will create three virtual devices as:
     *   /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
     *   /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
     *   /device:GPU:2 -&gt; visible GPU 0 with all available memory
     * NOTE:
     * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
     *    at the same time.
     * 2. Currently this setting is per-process, not per-session. Using
     *    different settings in different sessions within same process will
     *    result in undefined behavior.
     * </pre>
     *
     * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
     */
    java.util.List<? extends org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevicesOrBuilder> 
        getVirtualDevicesOrBuilderList();
    /**
     * <pre>
     * The multi virtual device settings. If empty (not set), it will create
     * single virtual device on each visible GPU, according to the settings
     * in "visible_device_list" above. Otherwise, the number of elements in the
     * list must be the same as the number of visible GPUs (after
     * "visible_device_list" filtering if it is set), and the string represented
     * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
     * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
     * according to the order they appear in this list and the "memory_limit"
     * list inside each element. For example,
     *   visible_device_list = "1,0"
     *   virtual_devices { memory_limit: 1GB memory_limit: 2GB }
     *   virtual_devices {}
     * will create three virtual devices as:
     *   /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
     *   /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
     *   /device:GPU:2 -&gt; visible GPU 0 with all available memory
     * NOTE:
     * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
     *    at the same time.
     * 2. Currently this setting is per-process, not per-session. Using
     *    different settings in different sessions within same process will
     *    result in undefined behavior.
     * </pre>
     *
     * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
     */
    org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevicesOrBuilder getVirtualDevicesOrBuilder(
        int index);

    /**
     * <pre>
     * If true, uses CUDA unified memory for memory allocations. If
     * per_process_gpu_memory_fraction option is greater than 1.0, then unified
     * memory is used regardless of the value for this field. See comments for
     * per_process_gpu_memory_fraction field for more details and requirements
     * of the unified memory. This option is useful to oversubscribe memory if
     * multiple processes are sharing a single GPU while individually using less
     * than 1.0 per process memory fraction.
     * </pre>
     *
     * <code>bool use_unified_memory = 2;</code>
     */
    boolean getUseUnifiedMemory();

    /**
     * <pre>
     * If &gt; 1, the number of device-to-device copy streams to create
     * for each GPUDevice.  Default value is 0, which is automatically
     * converted to 1.
     * </pre>
     *
     * <code>int32 num_dev_to_dev_copy_streams = 3;</code>
     */
    int getNumDevToDevCopyStreams();

    /**
     * <pre>
     * If non-empty, defines a good GPU ring order on a single worker based on
     * device interconnect.  This assumes that all workers have the same GPU
     * topology.  Specify as a comma-separated string, e.g. "3,2,1,0,7,6,5,4".
     * This ring order is used by the RingReducer implementation of
     * CollectiveReduce, and serves as an override to automatic ring order
     * generation in OrderTaskDeviceMap() during CollectiveParam resolution.
     * </pre>
     *
     * <code>string collective_ring_order = 4;</code>
     */
    java.lang.String getCollectiveRingOrder();
    /**
     * <pre>
     * If non-empty, defines a good GPU ring order on a single worker based on
     * device interconnect.  This assumes that all workers have the same GPU
     * topology.  Specify as a comma-separated string, e.g. "3,2,1,0,7,6,5,4".
     * This ring order is used by the RingReducer implementation of
     * CollectiveReduce, and serves as an override to automatic ring order
     * generation in OrderTaskDeviceMap() during CollectiveParam resolution.
     * </pre>
     *
     * <code>string collective_ring_order = 4;</code>
     */
    com.google.protobuf.ByteString
        getCollectiveRingOrderBytes();

    /**
     * <pre>
     * If true then extra work is done by GPUDevice and GPUBFCAllocator to
     * keep track of when GPU memory is freed and when kernels actually
     * complete so that we can know when a nominally free memory chunk
     * is really not subject to pending use.
     * </pre>
     *
     * <code>bool timestamped_allocator = 5;</code>
     */
    boolean getTimestampedAllocator();

    /**
     * <pre>
     * Parameters for GPUKernelTracker.  By default no kernel tracking is done.
     * Note that timestamped_allocator is only effective if some tracking is
     * specified.
     * If kernel_tracker_max_interval = n &gt; 0, then a tracking event
     * is inserted after every n kernels without an event.
     * </pre>
     *
     * <code>int32 kernel_tracker_max_interval = 7;</code>
     */
    int getKernelTrackerMaxInterval();

    /**
     * <pre>
     * If kernel_tracker_max_bytes = n &gt; 0, then a tracking event is
     * inserted after every series of kernels allocating a sum of
     * memory &gt;= n.  If one kernel allocates b * n bytes, then one
     * event will be inserted after it, but it will count as b against
     * the pending limit.
     * </pre>
     *
     * <code>int32 kernel_tracker_max_bytes = 8;</code>
     */
    int getKernelTrackerMaxBytes();

    /**
     * <pre>
     * If kernel_tracker_max_pending &gt; 0 then no more than this many
     * tracking events can be outstanding at a time.  An attempt to
     * launch an additional kernel will stall until an event
     * completes.
     * </pre>
     *
     * <code>int32 kernel_tracker_max_pending = 9;</code>
     */
    int getKernelTrackerMaxPending();

    /**
     * <pre>
     * BFC Allocator can return an allocated chunk of memory upto 2x the
     * requested size. For virtual devices with tight memory constraints, and
     * proportionately large allocation requests, this can lead to a significant
     * reduction in available memory. The threshold below controls when a chunk
     * should be split if the chunk size exceeds requested memory size. It is
     * expressed as a fraction of total available memory for the tf device. For
     * example setting it to 0.05 would imply a chunk needs to be split if its
     * size exceeds the requested memory by 5% of the total virtual device/gpu
     * memory size.
     * </pre>
     *
     * <code>double internal_fragmentation_fraction = 10;</code>
     */
    double getInternalFragmentationFraction();

    /**
     * <pre>
     * When true, use CUDA cudaMallocAsync API instead of TF gpu allocator.
     * </pre>
     *
     * <code>bool use_cuda_malloc_async = 11;</code>
     */
    boolean getUseCudaMallocAsync();
  }
  /**
   * Protobuf type {@code tensorflow.GPUOptions.Experimental}
   */
  public  static final class Experimental extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:tensorflow.GPUOptions.Experimental)
      ExperimentalOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use Experimental.newBuilder() to construct.
    private Experimental(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private Experimental() {
      virtualDevices_ = java.util.Collections.emptyList();
      collectiveRingOrder_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new Experimental();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private Experimental(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                virtualDevices_ = new java.util.ArrayList<org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices>();
                mutable_bitField0_ |= 0x00000001;
              }
              virtualDevices_.add(
                  input.readMessage(org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices.parser(), extensionRegistry));
              break;
            }
            case 16: {

              useUnifiedMemory_ = input.readBool();
              break;
            }
            case 24: {

              numDevToDevCopyStreams_ = input.readInt32();
              break;
            }
            case 34: {
              java.lang.String s = input.readStringRequireUtf8();

              collectiveRingOrder_ = s;
              break;
            }
            case 40: {

              timestampedAllocator_ = input.readBool();
              break;
            }
            case 56: {

              kernelTrackerMaxInterval_ = input.readInt32();
              break;
            }
            case 64: {

              kernelTrackerMaxBytes_ = input.readInt32();
              break;
            }
            case 72: {

              kernelTrackerMaxPending_ = input.readInt32();
              break;
            }
            case 81: {

              internalFragmentationFraction_ = input.readDouble();
              break;
            }
            case 88: {

              useCudaMallocAsync_ = input.readBool();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          virtualDevices_ = java.util.Collections.unmodifiableList(virtualDevices_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.tensorflow.proto.framework.ConfigProtos.internal_static_tensorflow_GPUOptions_Experimental_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.tensorflow.proto.framework.ConfigProtos.internal_static_tensorflow_GPUOptions_Experimental_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.tensorflow.proto.framework.GPUOptions.Experimental.class, org.tensorflow.proto.framework.GPUOptions.Experimental.Builder.class);
    }

    public interface VirtualDevicesOrBuilder extends
        // @@protoc_insertion_point(interface_extends:tensorflow.GPUOptions.Experimental.VirtualDevices)
        com.google.protobuf.MessageOrBuilder {

      /**
       * <pre>
       * Per "virtual" device memory limit, in MB. The number of elements in
       * the list is the number of virtual devices to create on the
       * corresponding visible GPU (see "virtual_devices" below).
       * If empty, it will create single virtual device taking all available
       * memory from the device.
       * For the concept of "visible" and "virtual" GPU, see the comments for
       * "visible_device_list" above for more information.
       * </pre>
       *
       * <code>repeated float memory_limit_mb = 1;</code>
       */
      java.util.List<java.lang.Float> getMemoryLimitMbList();
      /**
       * <pre>
       * Per "virtual" device memory limit, in MB. The number of elements in
       * the list is the number of virtual devices to create on the
       * corresponding visible GPU (see "virtual_devices" below).
       * If empty, it will create single virtual device taking all available
       * memory from the device.
       * For the concept of "visible" and "virtual" GPU, see the comments for
       * "visible_device_list" above for more information.
       * </pre>
       *
       * <code>repeated float memory_limit_mb = 1;</code>
       */
      int getMemoryLimitMbCount();
      /**
       * <pre>
       * Per "virtual" device memory limit, in MB. The number of elements in
       * the list is the number of virtual devices to create on the
       * corresponding visible GPU (see "virtual_devices" below).
       * If empty, it will create single virtual device taking all available
       * memory from the device.
       * For the concept of "visible" and "virtual" GPU, see the comments for
       * "visible_device_list" above for more information.
       * </pre>
       *
       * <code>repeated float memory_limit_mb = 1;</code>
       */
      float getMemoryLimitMb(int index);

      /**
       * <pre>
       * Priority values to use with the virtual devices. Use the cuda function
       * cudaDeviceGetStreamPriorityRange to query for valid range of values for
       * priority.
       * On a P4000 GPU with cuda 10.1, the priority range reported was 0 for
       * least priority and -1 for greatest priority.
       * If this field is not specified, then the virtual devices will be
       * created with the default. If this field has values set, then the size
       * of this must match with the above memory_limit_mb.
       * </pre>
       *
       * <code>repeated int32 priority = 2;</code>
       */
      java.util.List<java.lang.Integer> getPriorityList();
      /**
       * <pre>
       * Priority values to use with the virtual devices. Use the cuda function
       * cudaDeviceGetStreamPriorityRange to query for valid range of values for
       * priority.
       * On a P4000 GPU with cuda 10.1, the priority range reported was 0 for
       * least priority and -1 for greatest priority.
       * If this field is not specified, then the virtual devices will be
       * created with the default. If this field has values set, then the size
       * of this must match with the above memory_limit_mb.
       * </pre>
       *
       * <code>repeated int32 priority = 2;</code>
       */
      int getPriorityCount();
      /**
       * <pre>
       * Priority values to use with the virtual devices. Use the cuda function
       * cudaDeviceGetStreamPriorityRange to query for valid range of values for
       * priority.
       * On a P4000 GPU with cuda 10.1, the priority range reported was 0 for
       * least priority and -1 for greatest priority.
       * If this field is not specified, then the virtual devices will be
       * created with the default. If this field has values set, then the size
       * of this must match with the above memory_limit_mb.
       * </pre>
       *
       * <code>repeated int32 priority = 2;</code>
       */
      int getPriority(int index);
    }
    /**
     * <pre>
     * Configuration for breaking down a visible GPU into multiple "virtual"
     * devices.
     * </pre>
     *
     * Protobuf type {@code tensorflow.GPUOptions.Experimental.VirtualDevices}
     */
    public  static final class VirtualDevices extends
        com.google.protobuf.GeneratedMessageV3 implements
        // @@protoc_insertion_point(message_implements:tensorflow.GPUOptions.Experimental.VirtualDevices)
        VirtualDevicesOrBuilder {
    private static final long serialVersionUID = 0L;
      // Use VirtualDevices.newBuilder() to construct.
      private VirtualDevices(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
        super(builder);
      }
      private VirtualDevices() {
        memoryLimitMb_ = emptyFloatList();
        priority_ = emptyIntList();
      }

      @java.lang.Override
      @SuppressWarnings({"unused"})
      protected java.lang.Object newInstance(
          UnusedPrivateParameter unused) {
        return new VirtualDevices();
      }

      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
      getUnknownFields() {
        return this.unknownFields;
      }
      private VirtualDevices(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        this();
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        int mutable_bitField0_ = 0;
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 13: {
                if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                  memoryLimitMb_ = newFloatList();
                  mutable_bitField0_ |= 0x00000001;
                }
                memoryLimitMb_.addFloat(input.readFloat());
                break;
              }
              case 10: {
                int length = input.readRawVarint32();
                int limit = input.pushLimit(length);
                if (!((mutable_bitField0_ & 0x00000001) != 0) && input.getBytesUntilLimit() > 0) {
                  memoryLimitMb_ = newFloatList();
                  mutable_bitField0_ |= 0x00000001;
                }
                while (input.getBytesUntilLimit() > 0) {
                  memoryLimitMb_.addFloat(input.readFloat());
                }
                input.popLimit(limit);
                break;
              }
              case 16: {
                if (!((mutable_bitField0_ & 0x00000002) != 0)) {
                  priority_ = newIntList();
                  mutable_bitField0_ |= 0x00000002;
                }
                priority_.addInt(input.readInt32());
                break;
              }
              case 18: {
                int length = input.readRawVarint32();
                int limit = input.pushLimit(length);
                if (!((mutable_bitField0_ & 0x00000002) != 0) && input.getBytesUntilLimit() > 0) {
                  priority_ = newIntList();
                  mutable_bitField0_ |= 0x00000002;
                }
                while (input.getBytesUntilLimit() > 0) {
                  priority_.addInt(input.readInt32());
                }
                input.popLimit(limit);
                break;
              }
              default: {
                if (!parseUnknownField(
                    input, unknownFields, extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e).setUnfinishedMessage(this);
        } finally {
          if (((mutable_bitField0_ & 0x00000001) != 0)) {
            memoryLimitMb_.makeImmutable(); // C
          }
          if (((mutable_bitField0_ & 0x00000002) != 0)) {
            priority_.makeImmutable(); // C
          }
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.tensorflow.proto.framework.ConfigProtos.internal_static_tensorflow_GPUOptions_Experimental_VirtualDevices_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.tensorflow.proto.framework.ConfigProtos.internal_static_tensorflow_GPUOptions_Experimental_VirtualDevices_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices.class, org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices.Builder.class);
      }

      public static final int MEMORY_LIMIT_MB_FIELD_NUMBER = 1;
      private com.google.protobuf.Internal.FloatList memoryLimitMb_;
      /**
       * <pre>
       * Per "virtual" device memory limit, in MB. The number of elements in
       * the list is the number of virtual devices to create on the
       * corresponding visible GPU (see "virtual_devices" below).
       * If empty, it will create single virtual device taking all available
       * memory from the device.
       * For the concept of "visible" and "virtual" GPU, see the comments for
       * "visible_device_list" above for more information.
       * </pre>
       *
       * <code>repeated float memory_limit_mb = 1;</code>
       */
      public java.util.List<java.lang.Float>
          getMemoryLimitMbList() {
        return memoryLimitMb_;
      }
      /**
       * <pre>
       * Per "virtual" device memory limit, in MB. The number of elements in
       * the list is the number of virtual devices to create on the
       * corresponding visible GPU (see "virtual_devices" below).
       * If empty, it will create single virtual device taking all available
       * memory from the device.
       * For the concept of "visible" and "virtual" GPU, see the comments for
       * "visible_device_list" above for more information.
       * </pre>
       *
       * <code>repeated float memory_limit_mb = 1;</code>
       */
      public int getMemoryLimitMbCount() {
        return memoryLimitMb_.size();
      }
      /**
       * <pre>
       * Per "virtual" device memory limit, in MB. The number of elements in
       * the list is the number of virtual devices to create on the
       * corresponding visible GPU (see "virtual_devices" below).
       * If empty, it will create single virtual device taking all available
       * memory from the device.
       * For the concept of "visible" and "virtual" GPU, see the comments for
       * "visible_device_list" above for more information.
       * </pre>
       *
       * <code>repeated float memory_limit_mb = 1;</code>
       */
      public float getMemoryLimitMb(int index) {
        return memoryLimitMb_.getFloat(index);
      }
      private int memoryLimitMbMemoizedSerializedSize = -1;

      public static final int PRIORITY_FIELD_NUMBER = 2;
      private com.google.protobuf.Internal.IntList priority_;
      /**
       * <pre>
       * Priority values to use with the virtual devices. Use the cuda function
       * cudaDeviceGetStreamPriorityRange to query for valid range of values for
       * priority.
       * On a P4000 GPU with cuda 10.1, the priority range reported was 0 for
       * least priority and -1 for greatest priority.
       * If this field is not specified, then the virtual devices will be
       * created with the default. If this field has values set, then the size
       * of this must match with the above memory_limit_mb.
       * </pre>
       *
       * <code>repeated int32 priority = 2;</code>
       */
      public java.util.List<java.lang.Integer>
          getPriorityList() {
        return priority_;
      }
      /**
       * <pre>
       * Priority values to use with the virtual devices. Use the cuda function
       * cudaDeviceGetStreamPriorityRange to query for valid range of values for
       * priority.
       * On a P4000 GPU with cuda 10.1, the priority range reported was 0 for
       * least priority and -1 for greatest priority.
       * If this field is not specified, then the virtual devices will be
       * created with the default. If this field has values set, then the size
       * of this must match with the above memory_limit_mb.
       * </pre>
       *
       * <code>repeated int32 priority = 2;</code>
       */
      public int getPriorityCount() {
        return priority_.size();
      }
      /**
       * <pre>
       * Priority values to use with the virtual devices. Use the cuda function
       * cudaDeviceGetStreamPriorityRange to query for valid range of values for
       * priority.
       * On a P4000 GPU with cuda 10.1, the priority range reported was 0 for
       * least priority and -1 for greatest priority.
       * If this field is not specified, then the virtual devices will be
       * created with the default. If this field has values set, then the size
       * of this must match with the above memory_limit_mb.
       * </pre>
       *
       * <code>repeated int32 priority = 2;</code>
       */
      public int getPriority(int index) {
        return priority_.getInt(index);
      }
      private int priorityMemoizedSerializedSize = -1;

      private byte memoizedIsInitialized = -1;
      @java.lang.Override
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized == 1) return true;
        if (isInitialized == 0) return false;

        memoizedIsInitialized = 1;
        return true;
      }

      @java.lang.Override
      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        getSerializedSize();
        if (getMemoryLimitMbList().size() > 0) {
          output.writeUInt32NoTag(10);
          output.writeUInt32NoTag(memoryLimitMbMemoizedSerializedSize);
        }
        for (int i = 0; i < memoryLimitMb_.size(); i++) {
          output.writeFloatNoTag(memoryLimitMb_.getFloat(i));
        }
        if (getPriorityList().size() > 0) {
          output.writeUInt32NoTag(18);
          output.writeUInt32NoTag(priorityMemoizedSerializedSize);
        }
        for (int i = 0; i < priority_.size(); i++) {
          output.writeInt32NoTag(priority_.getInt(i));
        }
        unknownFields.writeTo(output);
      }

      @java.lang.Override
      public int getSerializedSize() {
        int size = memoizedSize;
        if (size != -1) return size;

        size = 0;
        {
          int dataSize = 0;
          dataSize = 4 * getMemoryLimitMbList().size();
          size += dataSize;
          if (!getMemoryLimitMbList().isEmpty()) {
            size += 1;
            size += com.google.protobuf.CodedOutputStream
                .computeInt32SizeNoTag(dataSize);
          }
          memoryLimitMbMemoizedSerializedSize = dataSize;
        }
        {
          int dataSize = 0;
          for (int i = 0; i < priority_.size(); i++) {
            dataSize += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(priority_.getInt(i));
          }
          size += dataSize;
          if (!getPriorityList().isEmpty()) {
            size += 1;
            size += com.google.protobuf.CodedOutputStream
                .computeInt32SizeNoTag(dataSize);
          }
          priorityMemoizedSerializedSize = dataSize;
        }
        size += unknownFields.getSerializedSize();
        memoizedSize = size;
        return size;
      }

      @java.lang.Override
      public boolean equals(final java.lang.Object obj) {
        if (obj == this) {
         return true;
        }
        if (!(obj instanceof org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices)) {
          return super.equals(obj);
        }
        org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices other = (org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices) obj;

        if (!getMemoryLimitMbList()
            .equals(other.getMemoryLimitMbList())) return false;
        if (!getPriorityList()
            .equals(other.getPriorityList())) return false;
        if (!unknownFields.equals(other.unknownFields)) return false;
        return true;
      }

      @java.lang.Override
      public int hashCode() {
        if (memoizedHashCode != 0) {
          return memoizedHashCode;
        }
        int hash = 41;
        hash = (19 * hash) + getDescriptor().hashCode();
        if (getMemoryLimitMbCount() > 0) {
          hash = (37 * hash) + MEMORY_LIMIT_MB_FIELD_NUMBER;
          hash = (53 * hash) + getMemoryLimitMbList().hashCode();
        }
        if (getPriorityCount() > 0) {
          hash = (37 * hash) + PRIORITY_FIELD_NUMBER;
          hash = (53 * hash) + getPriorityList().hashCode();
        }
        hash = (29 * hash) + unknownFields.hashCode();
        memoizedHashCode = hash;
        return hash;
      }

      public static org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }
      public static org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input);
      }
      public static org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
      }
      public static org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      @java.lang.Override
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder() {
        return DEFAULT_INSTANCE.toBuilder();
      }
      public static Builder newBuilder(org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices prototype) {
        return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
      }
      @java.lang.Override
      public Builder toBuilder() {
        return this == DEFAULT_INSTANCE
            ? new Builder() : new Builder().mergeFrom(this);
      }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * <pre>
       * Configuration for breaking down a visible GPU into multiple "virtual"
       * devices.
       * </pre>
       *
       * Protobuf type {@code tensorflow.GPUOptions.Experimental.VirtualDevices}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
          // @@protoc_insertion_point(builder_implements:tensorflow.GPUOptions.Experimental.VirtualDevices)
          org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevicesOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return org.tensorflow.proto.framework.ConfigProtos.internal_static_tensorflow_GPUOptions_Experimental_VirtualDevices_descriptor;
        }

        @java.lang.Override
        protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return org.tensorflow.proto.framework.ConfigProtos.internal_static_tensorflow_GPUOptions_Experimental_VirtualDevices_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices.class, org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices.Builder.class);
        }

        // Construct using org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessageV3
                  .alwaysUseFieldBuilders) {
          }
        }
        @java.lang.Override
        public Builder clear() {
          super.clear();
          memoryLimitMb_ = emptyFloatList();
          bitField0_ = (bitField0_ & ~0x00000001);
          priority_ = emptyIntList();
          bitField0_ = (bitField0_ & ~0x00000002);
          return this;
        }

        @java.lang.Override
        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return org.tensorflow.proto.framework.ConfigProtos.internal_static_tensorflow_GPUOptions_Experimental_VirtualDevices_descriptor;
        }

        @java.lang.Override
        public org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices getDefaultInstanceForType() {
          return org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices.getDefaultInstance();
        }

        @java.lang.Override
        public org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices build() {
          org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        @java.lang.Override
        public org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices buildPartial() {
          org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices result = new org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices(this);
          int from_bitField0_ = bitField0_;
          if (((bitField0_ & 0x00000001) != 0)) {
            memoryLimitMb_.makeImmutable();
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.memoryLimitMb_ = memoryLimitMb_;
          if (((bitField0_ & 0x00000002) != 0)) {
            priority_.makeImmutable();
            bitField0_ = (bitField0_ & ~0x00000002);
          }
          result.priority_ = priority_;
          onBuilt();
          return result;
        }

        @java.lang.Override
        public Builder clone() {
          return super.clone();
        }
        @java.lang.Override
        public Builder setField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.setField(field, value);
        }
        @java.lang.Override
        public Builder clearField(
            com.google.protobuf.Descriptors.FieldDescriptor field) {
          return super.clearField(field);
        }
        @java.lang.Override
        public Builder clearOneof(
            com.google.protobuf.Descriptors.OneofDescriptor oneof) {
          return super.clearOneof(oneof);
        }
        @java.lang.Override
        public Builder setRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            int index, java.lang.Object value) {
          return super.setRepeatedField(field, index, value);
        }
        @java.lang.Override
        public Builder addRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.addRepeatedField(field, value);
        }
        @java.lang.Override
        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices) {
            return mergeFrom((org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices other) {
          if (other == org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices.getDefaultInstance()) return this;
          if (!other.memoryLimitMb_.isEmpty()) {
            if (memoryLimitMb_.isEmpty()) {
              memoryLimitMb_ = other.memoryLimitMb_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureMemoryLimitMbIsMutable();
              memoryLimitMb_.addAll(other.memoryLimitMb_);
            }
            onChanged();
          }
          if (!other.priority_.isEmpty()) {
            if (priority_.isEmpty()) {
              priority_ = other.priority_;
              bitField0_ = (bitField0_ & ~0x00000002);
            } else {
              ensurePriorityIsMutable();
              priority_.addAll(other.priority_);
            }
            onChanged();
          }
          this.mergeUnknownFields(other.unknownFields);
          onChanged();
          return this;
        }

        @java.lang.Override
        public final boolean isInitialized() {
          return true;
        }

        @java.lang.Override
        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices) e.getUnfinishedMessage();
            throw e.unwrapIOException();
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }
        private int bitField0_;

        private com.google.protobuf.Internal.FloatList memoryLimitMb_ = emptyFloatList();
        private void ensureMemoryLimitMbIsMutable() {
          if (!((bitField0_ & 0x00000001) != 0)) {
            memoryLimitMb_ = mutableCopy(memoryLimitMb_);
            bitField0_ |= 0x00000001;
           }
        }
        /**
         * <pre>
         * Per "virtual" device memory limit, in MB. The number of elements in
         * the list is the number of virtual devices to create on the
         * corresponding visible GPU (see "virtual_devices" below).
         * If empty, it will create single virtual device taking all available
         * memory from the device.
         * For the concept of "visible" and "virtual" GPU, see the comments for
         * "visible_device_list" above for more information.
         * </pre>
         *
         * <code>repeated float memory_limit_mb = 1;</code>
         */
        public java.util.List<java.lang.Float>
            getMemoryLimitMbList() {
          return ((bitField0_ & 0x00000001) != 0) ?
                   java.util.Collections.unmodifiableList(memoryLimitMb_) : memoryLimitMb_;
        }
        /**
         * <pre>
         * Per "virtual" device memory limit, in MB. The number of elements in
         * the list is the number of virtual devices to create on the
         * corresponding visible GPU (see "virtual_devices" below).
         * If empty, it will create single virtual device taking all available
         * memory from the device.
         * For the concept of "visible" and "virtual" GPU, see the comments for
         * "visible_device_list" above for more information.
         * </pre>
         *
         * <code>repeated float memory_limit_mb = 1;</code>
         */
        public int getMemoryLimitMbCount() {
          return memoryLimitMb_.size();
        }
        /**
         * <pre>
         * Per "virtual" device memory limit, in MB. The number of elements in
         * the list is the number of virtual devices to create on the
         * corresponding visible GPU (see "virtual_devices" below).
         * If empty, it will create single virtual device taking all available
         * memory from the device.
         * For the concept of "visible" and "virtual" GPU, see the comments for
         * "visible_device_list" above for more information.
         * </pre>
         *
         * <code>repeated float memory_limit_mb = 1;</code>
         */
        public float getMemoryLimitMb(int index) {
          return memoryLimitMb_.getFloat(index);
        }
        /**
         * <pre>
         * Per "virtual" device memory limit, in MB. The number of elements in
         * the list is the number of virtual devices to create on the
         * corresponding visible GPU (see "virtual_devices" below).
         * If empty, it will create single virtual device taking all available
         * memory from the device.
         * For the concept of "visible" and "virtual" GPU, see the comments for
         * "visible_device_list" above for more information.
         * </pre>
         *
         * <code>repeated float memory_limit_mb = 1;</code>
         */
        public Builder setMemoryLimitMb(
            int index, float value) {
          ensureMemoryLimitMbIsMutable();
          memoryLimitMb_.setFloat(index, value);
          onChanged();
          return this;
        }
        /**
         * <pre>
         * Per "virtual" device memory limit, in MB. The number of elements in
         * the list is the number of virtual devices to create on the
         * corresponding visible GPU (see "virtual_devices" below).
         * If empty, it will create single virtual device taking all available
         * memory from the device.
         * For the concept of "visible" and "virtual" GPU, see the comments for
         * "visible_device_list" above for more information.
         * </pre>
         *
         * <code>repeated float memory_limit_mb = 1;</code>
         */
        public Builder addMemoryLimitMb(float value) {
          ensureMemoryLimitMbIsMutable();
          memoryLimitMb_.addFloat(value);
          onChanged();
          return this;
        }
        /**
         * <pre>
         * Per "virtual" device memory limit, in MB. The number of elements in
         * the list is the number of virtual devices to create on the
         * corresponding visible GPU (see "virtual_devices" below).
         * If empty, it will create single virtual device taking all available
         * memory from the device.
         * For the concept of "visible" and "virtual" GPU, see the comments for
         * "visible_device_list" above for more information.
         * </pre>
         *
         * <code>repeated float memory_limit_mb = 1;</code>
         */
        public Builder addAllMemoryLimitMb(
            java.lang.Iterable<? extends java.lang.Float> values) {
          ensureMemoryLimitMbIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, memoryLimitMb_);
          onChanged();
          return this;
        }
        /**
         * <pre>
         * Per "virtual" device memory limit, in MB. The number of elements in
         * the list is the number of virtual devices to create on the
         * corresponding visible GPU (see "virtual_devices" below).
         * If empty, it will create single virtual device taking all available
         * memory from the device.
         * For the concept of "visible" and "virtual" GPU, see the comments for
         * "visible_device_list" above for more information.
         * </pre>
         *
         * <code>repeated float memory_limit_mb = 1;</code>
         */
        public Builder clearMemoryLimitMb() {
          memoryLimitMb_ = emptyFloatList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
          return this;
        }

        private com.google.protobuf.Internal.IntList priority_ = emptyIntList();
        private void ensurePriorityIsMutable() {
          if (!((bitField0_ & 0x00000002) != 0)) {
            priority_ = mutableCopy(priority_);
            bitField0_ |= 0x00000002;
           }
        }
        /**
         * <pre>
         * Priority values to use with the virtual devices. Use the cuda function
         * cudaDeviceGetStreamPriorityRange to query for valid range of values for
         * priority.
         * On a P4000 GPU with cuda 10.1, the priority range reported was 0 for
         * least priority and -1 for greatest priority.
         * If this field is not specified, then the virtual devices will be
         * created with the default. If this field has values set, then the size
         * of this must match with the above memory_limit_mb.
         * </pre>
         *
         * <code>repeated int32 priority = 2;</code>
         */
        public java.util.List<java.lang.Integer>
            getPriorityList() {
          return ((bitField0_ & 0x00000002) != 0) ?
                   java.util.Collections.unmodifiableList(priority_) : priority_;
        }
        /**
         * <pre>
         * Priority values to use with the virtual devices. Use the cuda function
         * cudaDeviceGetStreamPriorityRange to query for valid range of values for
         * priority.
         * On a P4000 GPU with cuda 10.1, the priority range reported was 0 for
         * least priority and -1 for greatest priority.
         * If this field is not specified, then the virtual devices will be
         * created with the default. If this field has values set, then the size
         * of this must match with the above memory_limit_mb.
         * </pre>
         *
         * <code>repeated int32 priority = 2;</code>
         */
        public int getPriorityCount() {
          return priority_.size();
        }
        /**
         * <pre>
         * Priority values to use with the virtual devices. Use the cuda function
         * cudaDeviceGetStreamPriorityRange to query for valid range of values for
         * priority.
         * On a P4000 GPU with cuda 10.1, the priority range reported was 0 for
         * least priority and -1 for greatest priority.
         * If this field is not specified, then the virtual devices will be
         * created with the default. If this field has values set, then the size
         * of this must match with the above memory_limit_mb.
         * </pre>
         *
         * <code>repeated int32 priority = 2;</code>
         */
        public int getPriority(int index) {
          return priority_.getInt(index);
        }
        /**
         * <pre>
         * Priority values to use with the virtual devices. Use the cuda function
         * cudaDeviceGetStreamPriorityRange to query for valid range of values for
         * priority.
         * On a P4000 GPU with cuda 10.1, the priority range reported was 0 for
         * least priority and -1 for greatest priority.
         * If this field is not specified, then the virtual devices will be
         * created with the default. If this field has values set, then the size
         * of this must match with the above memory_limit_mb.
         * </pre>
         *
         * <code>repeated int32 priority = 2;</code>
         */
        public Builder setPriority(
            int index, int value) {
          ensurePriorityIsMutable();
          priority_.setInt(index, value);
          onChanged();
          return this;
        }
        /**
         * <pre>
         * Priority values to use with the virtual devices. Use the cuda function
         * cudaDeviceGetStreamPriorityRange to query for valid range of values for
         * priority.
         * On a P4000 GPU with cuda 10.1, the priority range reported was 0 for
         * least priority and -1 for greatest priority.
         * If this field is not specified, then the virtual devices will be
         * created with the default. If this field has values set, then the size
         * of this must match with the above memory_limit_mb.
         * </pre>
         *
         * <code>repeated int32 priority = 2;</code>
         */
        public Builder addPriority(int value) {
          ensurePriorityIsMutable();
          priority_.addInt(value);
          onChanged();
          return this;
        }
        /**
         * <pre>
         * Priority values to use with the virtual devices. Use the cuda function
         * cudaDeviceGetStreamPriorityRange to query for valid range of values for
         * priority.
         * On a P4000 GPU with cuda 10.1, the priority range reported was 0 for
         * least priority and -1 for greatest priority.
         * If this field is not specified, then the virtual devices will be
         * created with the default. If this field has values set, then the size
         * of this must match with the above memory_limit_mb.
         * </pre>
         *
         * <code>repeated int32 priority = 2;</code>
         */
        public Builder addAllPriority(
            java.lang.Iterable<? extends java.lang.Integer> values) {
          ensurePriorityIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, priority_);
          onChanged();
          return this;
        }
        /**
         * <pre>
         * Priority values to use with the virtual devices. Use the cuda function
         * cudaDeviceGetStreamPriorityRange to query for valid range of values for
         * priority.
         * On a P4000 GPU with cuda 10.1, the priority range reported was 0 for
         * least priority and -1 for greatest priority.
         * If this field is not specified, then the virtual devices will be
         * created with the default. If this field has values set, then the size
         * of this must match with the above memory_limit_mb.
         * </pre>
         *
         * <code>repeated int32 priority = 2;</code>
         */
        public Builder clearPriority() {
          priority_ = emptyIntList();
          bitField0_ = (bitField0_ & ~0x00000002);
          onChanged();
          return this;
        }
        @java.lang.Override
        public final Builder setUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.setUnknownFields(unknownFields);
        }

        @java.lang.Override
        public final Builder mergeUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.mergeUnknownFields(unknownFields);
        }


        // @@protoc_insertion_point(builder_scope:tensorflow.GPUOptions.Experimental.VirtualDevices)
      }

      // @@protoc_insertion_point(class_scope:tensorflow.GPUOptions.Experimental.VirtualDevices)
      private static final org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices DEFAULT_INSTANCE;
      static {
        DEFAULT_INSTANCE = new org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices();
      }

      public static org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static final com.google.protobuf.Parser<VirtualDevices>
          PARSER = new com.google.protobuf.AbstractParser<VirtualDevices>() {
        @java.lang.Override
        public VirtualDevices parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new VirtualDevices(input, extensionRegistry);
        }
      };

      public static com.google.protobuf.Parser<VirtualDevices> parser() {
        return PARSER;
      }

      @java.lang.Override
      public com.google.protobuf.Parser<VirtualDevices> getParserForType() {
        return PARSER;
      }

      @java.lang.Override
      public org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices getDefaultInstanceForType() {
        return DEFAULT_INSTANCE;
      }

    }

    public static final int VIRTUAL_DEVICES_FIELD_NUMBER = 1;
    private java.util.List<org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices> virtualDevices_;
    /**
     * <pre>
     * The multi virtual device settings. If empty (not set), it will create
     * single virtual device on each visible GPU, according to the settings
     * in "visible_device_list" above. Otherwise, the number of elements in the
     * list must be the same as the number of visible GPUs (after
     * "visible_device_list" filtering if it is set), and the string represented
     * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
     * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
     * according to the order they appear in this list and the "memory_limit"
     * list inside each element. For example,
     *   visible_device_list = "1,0"
     *   virtual_devices { memory_limit: 1GB memory_limit: 2GB }
     *   virtual_devices {}
     * will create three virtual devices as:
     *   /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
     *   /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
     *   /device:GPU:2 -&gt; visible GPU 0 with all available memory
     * NOTE:
     * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
     *    at the same time.
     * 2. Currently this setting is per-process, not per-session. Using
     *    different settings in different sessions within same process will
     *    result in undefined behavior.
     * </pre>
     *
     * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
     */
    public java.util.List<org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices> getVirtualDevicesList() {
      return virtualDevices_;
    }
    /**
     * <pre>
     * The multi virtual device settings. If empty (not set), it will create
     * single virtual device on each visible GPU, according to the settings
     * in "visible_device_list" above. Otherwise, the number of elements in the
     * list must be the same as the number of visible GPUs (after
     * "visible_device_list" filtering if it is set), and the string represented
     * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
     * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
     * according to the order they appear in this list and the "memory_limit"
     * list inside each element. For example,
     *   visible_device_list = "1,0"
     *   virtual_devices { memory_limit: 1GB memory_limit: 2GB }
     *   virtual_devices {}
     * will create three virtual devices as:
     *   /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
     *   /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
     *   /device:GPU:2 -&gt; visible GPU 0 with all available memory
     * NOTE:
     * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
     *    at the same time.
     * 2. Currently this setting is per-process, not per-session. Using
     *    different settings in different sessions within same process will
     *    result in undefined behavior.
     * </pre>
     *
     * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
     */
    public java.util.List<? extends org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevicesOrBuilder> 
        getVirtualDevicesOrBuilderList() {
      return virtualDevices_;
    }
    /**
     * <pre>
     * The multi virtual device settings. If empty (not set), it will create
     * single virtual device on each visible GPU, according to the settings
     * in "visible_device_list" above. Otherwise, the number of elements in the
     * list must be the same as the number of visible GPUs (after
     * "visible_device_list" filtering if it is set), and the string represented
     * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
     * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
     * according to the order they appear in this list and the "memory_limit"
     * list inside each element. For example,
     *   visible_device_list = "1,0"
     *   virtual_devices { memory_limit: 1GB memory_limit: 2GB }
     *   virtual_devices {}
     * will create three virtual devices as:
     *   /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
     *   /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
     *   /device:GPU:2 -&gt; visible GPU 0 with all available memory
     * NOTE:
     * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
     *    at the same time.
     * 2. Currently this setting is per-process, not per-session. Using
     *    different settings in different sessions within same process will
     *    result in undefined behavior.
     * </pre>
     *
     * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
     */
    public int getVirtualDevicesCount() {
      return virtualDevices_.size();
    }
    /**
     * <pre>
     * The multi virtual device settings. If empty (not set), it will create
     * single virtual device on each visible GPU, according to the settings
     * in "visible_device_list" above. Otherwise, the number of elements in the
     * list must be the same as the number of visible GPUs (after
     * "visible_device_list" filtering if it is set), and the string represented
     * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
     * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
     * according to the order they appear in this list and the "memory_limit"
     * list inside each element. For example,
     *   visible_device_list = "1,0"
     *   virtual_devices { memory_limit: 1GB memory_limit: 2GB }
     *   virtual_devices {}
     * will create three virtual devices as:
     *   /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
     *   /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
     *   /device:GPU:2 -&gt; visible GPU 0 with all available memory
     * NOTE:
     * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
     *    at the same time.
     * 2. Currently this setting is per-process, not per-session. Using
     *    different settings in different sessions within same process will
     *    result in undefined behavior.
     * </pre>
     *
     * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
     */
    public org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices getVirtualDevices(int index) {
      return virtualDevices_.get(index);
    }
    /**
     * <pre>
     * The multi virtual device settings. If empty (not set), it will create
     * single virtual device on each visible GPU, according to the settings
     * in "visible_device_list" above. Otherwise, the number of elements in the
     * list must be the same as the number of visible GPUs (after
     * "visible_device_list" filtering if it is set), and the string represented
     * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
     * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
     * according to the order they appear in this list and the "memory_limit"
     * list inside each element. For example,
     *   visible_device_list = "1,0"
     *   virtual_devices { memory_limit: 1GB memory_limit: 2GB }
     *   virtual_devices {}
     * will create three virtual devices as:
     *   /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
     *   /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
     *   /device:GPU:2 -&gt; visible GPU 0 with all available memory
     * NOTE:
     * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
     *    at the same time.
     * 2. Currently this setting is per-process, not per-session. Using
     *    different settings in different sessions within same process will
     *    result in undefined behavior.
     * </pre>
     *
     * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
     */
    public org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevicesOrBuilder getVirtualDevicesOrBuilder(
        int index) {
      return virtualDevices_.get(index);
    }

    public static final int USE_UNIFIED_MEMORY_FIELD_NUMBER = 2;
    private boolean useUnifiedMemory_;
    /**
     * <pre>
     * If true, uses CUDA unified memory for memory allocations. If
     * per_process_gpu_memory_fraction option is greater than 1.0, then unified
     * memory is used regardless of the value for this field. See comments for
     * per_process_gpu_memory_fraction field for more details and requirements
     * of the unified memory. This option is useful to oversubscribe memory if
     * multiple processes are sharing a single GPU while individually using less
     * than 1.0 per process memory fraction.
     * </pre>
     *
     * <code>bool use_unified_memory = 2;</code>
     */
    public boolean getUseUnifiedMemory() {
      return useUnifiedMemory_;
    }

    public static final int NUM_DEV_TO_DEV_COPY_STREAMS_FIELD_NUMBER = 3;
    private int numDevToDevCopyStreams_;
    /**
     * <pre>
     * If &gt; 1, the number of device-to-device copy streams to create
     * for each GPUDevice.  Default value is 0, which is automatically
     * converted to 1.
     * </pre>
     *
     * <code>int32 num_dev_to_dev_copy_streams = 3;</code>
     */
    public int getNumDevToDevCopyStreams() {
      return numDevToDevCopyStreams_;
    }

    public static final int COLLECTIVE_RING_ORDER_FIELD_NUMBER = 4;
    private volatile java.lang.Object collectiveRingOrder_;
    /**
     * <pre>
     * If non-empty, defines a good GPU ring order on a single worker based on
     * device interconnect.  This assumes that all workers have the same GPU
     * topology.  Specify as a comma-separated string, e.g. "3,2,1,0,7,6,5,4".
     * This ring order is used by the RingReducer implementation of
     * CollectiveReduce, and serves as an override to automatic ring order
     * generation in OrderTaskDeviceMap() during CollectiveParam resolution.
     * </pre>
     *
     * <code>string collective_ring_order = 4;</code>
     */
    public java.lang.String getCollectiveRingOrder() {
      java.lang.Object ref = collectiveRingOrder_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        collectiveRingOrder_ = s;
        return s;
      }
    }
    /**
     * <pre>
     * If non-empty, defines a good GPU ring order on a single worker based on
     * device interconnect.  This assumes that all workers have the same GPU
     * topology.  Specify as a comma-separated string, e.g. "3,2,1,0,7,6,5,4".
     * This ring order is used by the RingReducer implementation of
     * CollectiveReduce, and serves as an override to automatic ring order
     * generation in OrderTaskDeviceMap() during CollectiveParam resolution.
     * </pre>
     *
     * <code>string collective_ring_order = 4;</code>
     */
    public com.google.protobuf.ByteString
        getCollectiveRingOrderBytes() {
      java.lang.Object ref = collectiveRingOrder_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        collectiveRingOrder_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int TIMESTAMPED_ALLOCATOR_FIELD_NUMBER = 5;
    private boolean timestampedAllocator_;
    /**
     * <pre>
     * If true then extra work is done by GPUDevice and GPUBFCAllocator to
     * keep track of when GPU memory is freed and when kernels actually
     * complete so that we can know when a nominally free memory chunk
     * is really not subject to pending use.
     * </pre>
     *
     * <code>bool timestamped_allocator = 5;</code>
     */
    public boolean getTimestampedAllocator() {
      return timestampedAllocator_;
    }

    public static final int KERNEL_TRACKER_MAX_INTERVAL_FIELD_NUMBER = 7;
    private int kernelTrackerMaxInterval_;
    /**
     * <pre>
     * Parameters for GPUKernelTracker.  By default no kernel tracking is done.
     * Note that timestamped_allocator is only effective if some tracking is
     * specified.
     * If kernel_tracker_max_interval = n &gt; 0, then a tracking event
     * is inserted after every n kernels without an event.
     * </pre>
     *
     * <code>int32 kernel_tracker_max_interval = 7;</code>
     */
    public int getKernelTrackerMaxInterval() {
      return kernelTrackerMaxInterval_;
    }

    public static final int KERNEL_TRACKER_MAX_BYTES_FIELD_NUMBER = 8;
    private int kernelTrackerMaxBytes_;
    /**
     * <pre>
     * If kernel_tracker_max_bytes = n &gt; 0, then a tracking event is
     * inserted after every series of kernels allocating a sum of
     * memory &gt;= n.  If one kernel allocates b * n bytes, then one
     * event will be inserted after it, but it will count as b against
     * the pending limit.
     * </pre>
     *
     * <code>int32 kernel_tracker_max_bytes = 8;</code>
     */
    public int getKernelTrackerMaxBytes() {
      return kernelTrackerMaxBytes_;
    }

    public static final int KERNEL_TRACKER_MAX_PENDING_FIELD_NUMBER = 9;
    private int kernelTrackerMaxPending_;
    /**
     * <pre>
     * If kernel_tracker_max_pending &gt; 0 then no more than this many
     * tracking events can be outstanding at a time.  An attempt to
     * launch an additional kernel will stall until an event
     * completes.
     * </pre>
     *
     * <code>int32 kernel_tracker_max_pending = 9;</code>
     */
    public int getKernelTrackerMaxPending() {
      return kernelTrackerMaxPending_;
    }

    public static final int INTERNAL_FRAGMENTATION_FRACTION_FIELD_NUMBER = 10;
    private double internalFragmentationFraction_;
    /**
     * <pre>
     * BFC Allocator can return an allocated chunk of memory upto 2x the
     * requested size. For virtual devices with tight memory constraints, and
     * proportionately large allocation requests, this can lead to a significant
     * reduction in available memory. The threshold below controls when a chunk
     * should be split if the chunk size exceeds requested memory size. It is
     * expressed as a fraction of total available memory for the tf device. For
     * example setting it to 0.05 would imply a chunk needs to be split if its
     * size exceeds the requested memory by 5% of the total virtual device/gpu
     * memory size.
     * </pre>
     *
     * <code>double internal_fragmentation_fraction = 10;</code>
     */
    public double getInternalFragmentationFraction() {
      return internalFragmentationFraction_;
    }

    public static final int USE_CUDA_MALLOC_ASYNC_FIELD_NUMBER = 11;
    private boolean useCudaMallocAsync_;
    /**
     * <pre>
     * When true, use CUDA cudaMallocAsync API instead of TF gpu allocator.
     * </pre>
     *
     * <code>bool use_cuda_malloc_async = 11;</code>
     */
    public boolean getUseCudaMallocAsync() {
      return useCudaMallocAsync_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      for (int i = 0; i < virtualDevices_.size(); i++) {
        output.writeMessage(1, virtualDevices_.get(i));
      }
      if (useUnifiedMemory_ != false) {
        output.writeBool(2, useUnifiedMemory_);
      }
      if (numDevToDevCopyStreams_ != 0) {
        output.writeInt32(3, numDevToDevCopyStreams_);
      }
      if (!getCollectiveRingOrderBytes().isEmpty()) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 4, collectiveRingOrder_);
      }
      if (timestampedAllocator_ != false) {
        output.writeBool(5, timestampedAllocator_);
      }
      if (kernelTrackerMaxInterval_ != 0) {
        output.writeInt32(7, kernelTrackerMaxInterval_);
      }
      if (kernelTrackerMaxBytes_ != 0) {
        output.writeInt32(8, kernelTrackerMaxBytes_);
      }
      if (kernelTrackerMaxPending_ != 0) {
        output.writeInt32(9, kernelTrackerMaxPending_);
      }
      if (internalFragmentationFraction_ != 0D) {
        output.writeDouble(10, internalFragmentationFraction_);
      }
      if (useCudaMallocAsync_ != false) {
        output.writeBool(11, useCudaMallocAsync_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < virtualDevices_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, virtualDevices_.get(i));
      }
      if (useUnifiedMemory_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(2, useUnifiedMemory_);
      }
      if (numDevToDevCopyStreams_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(3, numDevToDevCopyStreams_);
      }
      if (!getCollectiveRingOrderBytes().isEmpty()) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(4, collectiveRingOrder_);
      }
      if (timestampedAllocator_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(5, timestampedAllocator_);
      }
      if (kernelTrackerMaxInterval_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(7, kernelTrackerMaxInterval_);
      }
      if (kernelTrackerMaxBytes_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(8, kernelTrackerMaxBytes_);
      }
      if (kernelTrackerMaxPending_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(9, kernelTrackerMaxPending_);
      }
      if (internalFragmentationFraction_ != 0D) {
        size += com.google.protobuf.CodedOutputStream
          .computeDoubleSize(10, internalFragmentationFraction_);
      }
      if (useCudaMallocAsync_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(11, useCudaMallocAsync_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.tensorflow.proto.framework.GPUOptions.Experimental)) {
        return super.equals(obj);
      }
      org.tensorflow.proto.framework.GPUOptions.Experimental other = (org.tensorflow.proto.framework.GPUOptions.Experimental) obj;

      if (!getVirtualDevicesList()
          .equals(other.getVirtualDevicesList())) return false;
      if (getUseUnifiedMemory()
          != other.getUseUnifiedMemory()) return false;
      if (getNumDevToDevCopyStreams()
          != other.getNumDevToDevCopyStreams()) return false;
      if (!getCollectiveRingOrder()
          .equals(other.getCollectiveRingOrder())) return false;
      if (getTimestampedAllocator()
          != other.getTimestampedAllocator()) return false;
      if (getKernelTrackerMaxInterval()
          != other.getKernelTrackerMaxInterval()) return false;
      if (getKernelTrackerMaxBytes()
          != other.getKernelTrackerMaxBytes()) return false;
      if (getKernelTrackerMaxPending()
          != other.getKernelTrackerMaxPending()) return false;
      if (java.lang.Double.doubleToLongBits(getInternalFragmentationFraction())
          != java.lang.Double.doubleToLongBits(
              other.getInternalFragmentationFraction())) return false;
      if (getUseCudaMallocAsync()
          != other.getUseCudaMallocAsync()) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (getVirtualDevicesCount() > 0) {
        hash = (37 * hash) + VIRTUAL_DEVICES_FIELD_NUMBER;
        hash = (53 * hash) + getVirtualDevicesList().hashCode();
      }
      hash = (37 * hash) + USE_UNIFIED_MEMORY_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getUseUnifiedMemory());
      hash = (37 * hash) + NUM_DEV_TO_DEV_COPY_STREAMS_FIELD_NUMBER;
      hash = (53 * hash) + getNumDevToDevCopyStreams();
      hash = (37 * hash) + COLLECTIVE_RING_ORDER_FIELD_NUMBER;
      hash = (53 * hash) + getCollectiveRingOrder().hashCode();
      hash = (37 * hash) + TIMESTAMPED_ALLOCATOR_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getTimestampedAllocator());
      hash = (37 * hash) + KERNEL_TRACKER_MAX_INTERVAL_FIELD_NUMBER;
      hash = (53 * hash) + getKernelTrackerMaxInterval();
      hash = (37 * hash) + KERNEL_TRACKER_MAX_BYTES_FIELD_NUMBER;
      hash = (53 * hash) + getKernelTrackerMaxBytes();
      hash = (37 * hash) + KERNEL_TRACKER_MAX_PENDING_FIELD_NUMBER;
      hash = (53 * hash) + getKernelTrackerMaxPending();
      hash = (37 * hash) + INTERNAL_FRAGMENTATION_FRACTION_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          java.lang.Double.doubleToLongBits(getInternalFragmentationFraction()));
      hash = (37 * hash) + USE_CUDA_MALLOC_ASYNC_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getUseCudaMallocAsync());
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.tensorflow.proto.framework.GPUOptions.Experimental parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.tensorflow.proto.framework.GPUOptions.Experimental parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.tensorflow.proto.framework.GPUOptions.Experimental parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.tensorflow.proto.framework.GPUOptions.Experimental parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.tensorflow.proto.framework.GPUOptions.Experimental parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.tensorflow.proto.framework.GPUOptions.Experimental parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.tensorflow.proto.framework.GPUOptions.Experimental parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.tensorflow.proto.framework.GPUOptions.Experimental parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.tensorflow.proto.framework.GPUOptions.Experimental parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.tensorflow.proto.framework.GPUOptions.Experimental parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.tensorflow.proto.framework.GPUOptions.Experimental parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.tensorflow.proto.framework.GPUOptions.Experimental parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.tensorflow.proto.framework.GPUOptions.Experimental prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code tensorflow.GPUOptions.Experimental}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:tensorflow.GPUOptions.Experimental)
        org.tensorflow.proto.framework.GPUOptions.ExperimentalOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.tensorflow.proto.framework.ConfigProtos.internal_static_tensorflow_GPUOptions_Experimental_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.tensorflow.proto.framework.ConfigProtos.internal_static_tensorflow_GPUOptions_Experimental_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.tensorflow.proto.framework.GPUOptions.Experimental.class, org.tensorflow.proto.framework.GPUOptions.Experimental.Builder.class);
      }

      // Construct using org.tensorflow.proto.framework.GPUOptions.Experimental.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getVirtualDevicesFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (virtualDevicesBuilder_ == null) {
          virtualDevices_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          virtualDevicesBuilder_.clear();
        }
        useUnifiedMemory_ = false;

        numDevToDevCopyStreams_ = 0;

        collectiveRingOrder_ = "";

        timestampedAllocator_ = false;

        kernelTrackerMaxInterval_ = 0;

        kernelTrackerMaxBytes_ = 0;

        kernelTrackerMaxPending_ = 0;

        internalFragmentationFraction_ = 0D;

        useCudaMallocAsync_ = false;

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.tensorflow.proto.framework.ConfigProtos.internal_static_tensorflow_GPUOptions_Experimental_descriptor;
      }

      @java.lang.Override
      public org.tensorflow.proto.framework.GPUOptions.Experimental getDefaultInstanceForType() {
        return org.tensorflow.proto.framework.GPUOptions.Experimental.getDefaultInstance();
      }

      @java.lang.Override
      public org.tensorflow.proto.framework.GPUOptions.Experimental build() {
        org.tensorflow.proto.framework.GPUOptions.Experimental result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.tensorflow.proto.framework.GPUOptions.Experimental buildPartial() {
        org.tensorflow.proto.framework.GPUOptions.Experimental result = new org.tensorflow.proto.framework.GPUOptions.Experimental(this);
        int from_bitField0_ = bitField0_;
        if (virtualDevicesBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0)) {
            virtualDevices_ = java.util.Collections.unmodifiableList(virtualDevices_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.virtualDevices_ = virtualDevices_;
        } else {
          result.virtualDevices_ = virtualDevicesBuilder_.build();
        }
        result.useUnifiedMemory_ = useUnifiedMemory_;
        result.numDevToDevCopyStreams_ = numDevToDevCopyStreams_;
        result.collectiveRingOrder_ = collectiveRingOrder_;
        result.timestampedAllocator_ = timestampedAllocator_;
        result.kernelTrackerMaxInterval_ = kernelTrackerMaxInterval_;
        result.kernelTrackerMaxBytes_ = kernelTrackerMaxBytes_;
        result.kernelTrackerMaxPending_ = kernelTrackerMaxPending_;
        result.internalFragmentationFraction_ = internalFragmentationFraction_;
        result.useCudaMallocAsync_ = useCudaMallocAsync_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.tensorflow.proto.framework.GPUOptions.Experimental) {
          return mergeFrom((org.tensorflow.proto.framework.GPUOptions.Experimental)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.tensorflow.proto.framework.GPUOptions.Experimental other) {
        if (other == org.tensorflow.proto.framework.GPUOptions.Experimental.getDefaultInstance()) return this;
        if (virtualDevicesBuilder_ == null) {
          if (!other.virtualDevices_.isEmpty()) {
            if (virtualDevices_.isEmpty()) {
              virtualDevices_ = other.virtualDevices_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureVirtualDevicesIsMutable();
              virtualDevices_.addAll(other.virtualDevices_);
            }
            onChanged();
          }
        } else {
          if (!other.virtualDevices_.isEmpty()) {
            if (virtualDevicesBuilder_.isEmpty()) {
              virtualDevicesBuilder_.dispose();
              virtualDevicesBuilder_ = null;
              virtualDevices_ = other.virtualDevices_;
              bitField0_ = (bitField0_ & ~0x00000001);
              virtualDevicesBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getVirtualDevicesFieldBuilder() : null;
            } else {
              virtualDevicesBuilder_.addAllMessages(other.virtualDevices_);
            }
          }
        }
        if (other.getUseUnifiedMemory() != false) {
          setUseUnifiedMemory(other.getUseUnifiedMemory());
        }
        if (other.getNumDevToDevCopyStreams() != 0) {
          setNumDevToDevCopyStreams(other.getNumDevToDevCopyStreams());
        }
        if (!other.getCollectiveRingOrder().isEmpty()) {
          collectiveRingOrder_ = other.collectiveRingOrder_;
          onChanged();
        }
        if (other.getTimestampedAllocator() != false) {
          setTimestampedAllocator(other.getTimestampedAllocator());
        }
        if (other.getKernelTrackerMaxInterval() != 0) {
          setKernelTrackerMaxInterval(other.getKernelTrackerMaxInterval());
        }
        if (other.getKernelTrackerMaxBytes() != 0) {
          setKernelTrackerMaxBytes(other.getKernelTrackerMaxBytes());
        }
        if (other.getKernelTrackerMaxPending() != 0) {
          setKernelTrackerMaxPending(other.getKernelTrackerMaxPending());
        }
        if (other.getInternalFragmentationFraction() != 0D) {
          setInternalFragmentationFraction(other.getInternalFragmentationFraction());
        }
        if (other.getUseCudaMallocAsync() != false) {
          setUseCudaMallocAsync(other.getUseCudaMallocAsync());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.tensorflow.proto.framework.GPUOptions.Experimental parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.tensorflow.proto.framework.GPUOptions.Experimental) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.util.List<org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices> virtualDevices_ =
        java.util.Collections.emptyList();
      private void ensureVirtualDevicesIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          virtualDevices_ = new java.util.ArrayList<org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices>(virtualDevices_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices, org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices.Builder, org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevicesOrBuilder> virtualDevicesBuilder_;

      /**
       * <pre>
       * The multi virtual device settings. If empty (not set), it will create
       * single virtual device on each visible GPU, according to the settings
       * in "visible_device_list" above. Otherwise, the number of elements in the
       * list must be the same as the number of visible GPUs (after
       * "visible_device_list" filtering if it is set), and the string represented
       * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
       * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
       * according to the order they appear in this list and the "memory_limit"
       * list inside each element. For example,
       *   visible_device_list = "1,0"
       *   virtual_devices { memory_limit: 1GB memory_limit: 2GB }
       *   virtual_devices {}
       * will create three virtual devices as:
       *   /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
       *   /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
       *   /device:GPU:2 -&gt; visible GPU 0 with all available memory
       * NOTE:
       * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
       *    at the same time.
       * 2. Currently this setting is per-process, not per-session. Using
       *    different settings in different sessions within same process will
       *    result in undefined behavior.
       * </pre>
       *
       * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
       */
      public java.util.List<org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices> getVirtualDevicesList() {
        if (virtualDevicesBuilder_ == null) {
          return java.util.Collections.unmodifiableList(virtualDevices_);
        } else {
          return virtualDevicesBuilder_.getMessageList();
        }
      }
      /**
       * <pre>
       * The multi virtual device settings. If empty (not set), it will create
       * single virtual device on each visible GPU, according to the settings
       * in "visible_device_list" above. Otherwise, the number of elements in the
       * list must be the same as the number of visible GPUs (after
       * "visible_device_list" filtering if it is set), and the string represented
       * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
       * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
       * according to the order they appear in this list and the "memory_limit"
       * list inside each element. For example,
       *   visible_device_list = "1,0"
       *   virtual_devices { memory_limit: 1GB memory_limit: 2GB }
       *   virtual_devices {}
       * will create three virtual devices as:
       *   /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
       *   /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
       *   /device:GPU:2 -&gt; visible GPU 0 with all available memory
       * NOTE:
       * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
       *    at the same time.
       * 2. Currently this setting is per-process, not per-session. Using
       *    different settings in different sessions within same process will
       *    result in undefined behavior.
       * </pre>
       *
       * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
       */
      public int getVirtualDevicesCount() {
        if (virtualDevicesBuilder_ == null) {
          return virtualDevices_.size();
        } else {
          return virtualDevicesBuilder_.getCount();
        }
      }
      /**
       * <pre>
       * The multi virtual device settings. If empty (not set), it will create
       * single virtual device on each visible GPU, according to the settings
       * in "visible_device_list" above. Otherwise, the number of elements in the
       * list must be the same as the number of visible GPUs (after
       * "visible_device_list" filtering if it is set), and the string represented
       * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
       * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
       * according to the order they appear in this list and the "memory_limit"
       * list inside each element. For example,
       *   visible_device_list = "1,0"
       *   virtual_devices { memory_limit: 1GB memory_limit: 2GB }
       *   virtual_devices {}
       * will create three virtual devices as:
       *   /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
       *   /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
       *   /device:GPU:2 -&gt; visible GPU 0 with all available memory
       * NOTE:
       * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
       *    at the same time.
       * 2. Currently this setting is per-process, not per-session. Using
       *    different settings in different sessions within same process will
       *    result in undefined behavior.
       * </pre>
       *
       * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
       */
      public org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices getVirtualDevices(int index) {
        if (virtualDevicesBuilder_ == null) {
          return virtualDevices_.get(index);
        } else {
          return virtualDevicesBuilder_.getMessage(index);
        }
      }
      /**
       * <pre>
       * The multi virtual device settings. If empty (not set), it will create
       * single virtual device on each visible GPU, according to the settings
       * in "visible_device_list" above. Otherwise, the number of elements in the
       * list must be the same as the number of visible GPUs (after
       * "visible_device_list" filtering if it is set), and the string represented
       * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
       * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
       * according to the order they appear in this list and the "memory_limit"
       * list inside each element. For example,
       *   visible_device_list = "1,0"
       *   virtual_devices { memory_limit: 1GB memory_limit: 2GB }
       *   virtual_devices {}
       * will create three virtual devices as:
       *   /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
       *   /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
       *   /device:GPU:2 -&gt; visible GPU 0 with all available memory
       * NOTE:
       * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
       *    at the same time.
       * 2. Currently this setting is per-process, not per-session. Using
       *    different settings in different sessions within same process will
       *    result in undefined behavior.
       * </pre>
       *
       * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
       */
      public Builder setVirtualDevices(
          int index, org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices value) {
        if (virtualDevicesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureVirtualDevicesIsMutable();
          virtualDevices_.set(index, value);
          onChanged();
        } else {
          virtualDevicesBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       * The multi virtual device settings. If empty (not set), it will create
       * single virtual device on each visible GPU, according to the settings
       * in "visible_device_list" above. Otherwise, the number of elements in the
       * list must be the same as the number of visible GPUs (after
       * "visible_device_list" filtering if it is set), and the string represented
       * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
       * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
       * according to the order they appear in this list and the "memory_limit"
       * list inside each element. For example,
       *   visible_device_list = "1,0"
       *   virtual_devices { memory_limit: 1GB memory_limit: 2GB }
       *   virtual_devices {}
       * will create three virtual devices as:
       *   /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
       *   /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
       *   /device:GPU:2 -&gt; visible GPU 0 with all available memory
       * NOTE:
       * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
       *    at the same time.
       * 2. Currently this setting is per-process, not per-session. Using
       *    different settings in different sessions within same process will
       *    result in undefined behavior.
       * </pre>
       *
       * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
       */
      public Builder setVirtualDevices(
          int index, org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices.Builder builderForValue) {
        if (virtualDevicesBuilder_ == null) {
          ensureVirtualDevicesIsMutable();
          virtualDevices_.set(index, builderForValue.build());
          onChanged();
        } else {
          virtualDevicesBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       * The multi virtual device settings. If empty (not set), it will create
       * single virtual device on each visible GPU, according to the settings
       * in "visible_device_list" above. Otherwise, the number of elements in the
       * list must be the same as the number of visible GPUs (after
       * "visible_device_list" filtering if it is set), and the string represented
       * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
       * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
       * according to the order they appear in this list and the "memory_limit"
       * list inside each element. For example,
       *   visible_device_list = "1,0"
       *   virtual_devices { memory_limit: 1GB memory_limit: 2GB }
       *   virtual_devices {}
       * will create three virtual devices as:
       *   /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
       *   /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
       *   /device:GPU:2 -&gt; visible GPU 0 with all available memory
       * NOTE:
       * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
       *    at the same time.
       * 2. Currently this setting is per-process, not per-session. Using
       *    different settings in different sessions within same process will
       *    result in undefined behavior.
       * </pre>
       *
       * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
       */
      public Builder addVirtualDevices(org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices value) {
        if (virtualDevicesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureVirtualDevicesIsMutable();
          virtualDevices_.add(value);
          onChanged();
        } else {
          virtualDevicesBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <pre>
       * The multi virtual device settings. If empty (not set), it will create
       * single virtual device on each visible GPU, according to the settings
       * in "visible_device_list" above. Otherwise, the number of elements in the
       * list must be the same as the number of visible GPUs (after
       * "visible_device_list" filtering if it is set), and the string represented
       * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
       * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
       * according to the order they appear in this list and the "memory_limit"
       * list inside each element. For example,
       *   visible_device_list = "1,0"
       *   virtual_devices { memory_limit: 1GB memory_limit: 2GB }
       *   virtual_devices {}
       * will create three virtual devices as:
       *   /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
       *   /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
       *   /device:GPU:2 -&gt; visible GPU 0 with all available memory
       * NOTE:
       * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
       *    at the same time.
       * 2. Currently this setting is per-process, not per-session. Using
       *    different settings in different sessions within same process will
       *    result in undefined behavior.
       * </pre>
       *
       * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
       */
      public Builder addVirtualDevices(
          int index, org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices value) {
        if (virtualDevicesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureVirtualDevicesIsMutable();
          virtualDevices_.add(index, value);
          onChanged();
        } else {
          virtualDevicesBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       * The multi virtual device settings. If empty (not set), it will create
       * single virtual device on each visible GPU, according to the settings
       * in "visible_device_list" above. Otherwise, the number of elements in the
       * list must be the same as the number of visible GPUs (after
       * "visible_device_list" filtering if it is set), and the string represented
       * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
       * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
       * according to the order they appear in this list and the "memory_limit"
       * list inside each element. For example,
       *   visible_device_list = "1,0"
       *   virtual_devices { memory_limit: 1GB memory_limit: 2GB }
       *   virtual_devices {}
       * will create three virtual devices as:
       *   /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
       *   /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
       *   /device:GPU:2 -&gt; visible GPU 0 with all available memory
       * NOTE:
       * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
       *    at the same time.
       * 2. Currently this setting is per-process, not per-session. Using
       *    different settings in different sessions within same process will
       *    result in undefined behavior.
       * </pre>
       *
       * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
       */
      public Builder addVirtualDevices(
          org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices.Builder builderForValue) {
        if (virtualDevicesBuilder_ == null) {
          ensureVirtualDevicesIsMutable();
          virtualDevices_.add(builderForValue.build());
          onChanged();
        } else {
          virtualDevicesBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       * The multi virtual device settings. If empty (not set), it will create
       * single virtual device on each visible GPU, according to the settings
       * in "visible_device_list" above. Otherwise, the number of elements in the
       * list must be the same as the number of visible GPUs (after
       * "visible_device_list" filtering if it is set), and the string represented
       * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
       * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
       * according to the order they appear in this list and the "memory_limit"
       * list inside each element. For example,
       *   visible_device_list = "1,0"
       *   virtual_devices { memory_limit: 1GB memory_limit: 2GB }
       *   virtual_devices {}
       * will create three virtual devices as:
       *   /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
       *   /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
       *   /device:GPU:2 -&gt; visible GPU 0 with all available memory
       * NOTE:
       * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
       *    at the same time.
       * 2. Currently this setting is per-process, not per-session. Using
       *    different settings in different sessions within same process will
       *    result in undefined behavior.
       * </pre>
       *
       * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
       */
      public Builder addVirtualDevices(
          int index, org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices.Builder builderForValue) {
        if (virtualDevicesBuilder_ == null) {
          ensureVirtualDevicesIsMutable();
          virtualDevices_.add(index, builderForValue.build());
          onChanged();
        } else {
          virtualDevicesBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       * The multi virtual device settings. If empty (not set), it will create
       * single virtual device on each visible GPU, according to the settings
       * in "visible_device_list" above. Otherwise, the number of elements in the
       * list must be the same as the number of visible GPUs (after
       * "visible_device_list" filtering if it is set), and the string represented
       * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
       * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
       * according to the order they appear in this list and the "memory_limit"
       * list inside each element. For example,
       *   visible_device_list = "1,0"
       *   virtual_devices { memory_limit: 1GB memory_limit: 2GB }
       *   virtual_devices {}
       * will create three virtual devices as:
       *   /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
       *   /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
       *   /device:GPU:2 -&gt; visible GPU 0 with all available memory
       * NOTE:
       * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
       *    at the same time.
       * 2. Currently this setting is per-process, not per-session. Using
       *    different settings in different sessions within same process will
       *    result in undefined behavior.
       * </pre>
       *
       * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
       */
      public Builder addAllVirtualDevices(
          java.lang.Iterable<? extends org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices> values) {
        if (virtualDevicesBuilder_ == null) {
          ensureVirtualDevicesIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, virtualDevices_);
          onChanged();
        } else {
          virtualDevicesBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <pre>
       * The multi virtual device settings. If empty (not set), it will create
       * single virtual device on each visible GPU, according to the settings
       * in "visible_device_list" above. Otherwise, the number of elements in the
       * list must be the same as the number of visible GPUs (after
       * "visible_device_list" filtering if it is set), and the string represented
       * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
       * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
       * according to the order they appear in this list and the "memory_limit"
       * list inside each element. For example,
       *   visible_device_list = "1,0"
       *   virtual_devices { memory_limit: 1GB memory_limit: 2GB }
       *   virtual_devices {}
       * will create three virtual devices as:
       *   /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
       *   /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
       *   /device:GPU:2 -&gt; visible GPU 0 with all available memory
       * NOTE:
       * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
       *    at the same time.
       * 2. Currently this setting is per-process, not per-session. Using
       *    different settings in different sessions within same process will
       *    result in undefined behavior.
       * </pre>
       *
       * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
       */
      public Builder clearVirtualDevices() {
        if (virtualDevicesBuilder_ == null) {
          virtualDevices_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          virtualDevicesBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       * The multi virtual device settings. If empty (not set), it will create
       * single virtual device on each visible GPU, according to the settings
       * in "visible_device_list" above. Otherwise, the number of elements in the
       * list must be the same as the number of visible GPUs (after
       * "visible_device_list" filtering if it is set), and the string represented
       * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
       * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
       * according to the order they appear in this list and the "memory_limit"
       * list inside each element. For example,
       *   visible_device_list = "1,0"
       *   virtual_devices { memory_limit: 1GB memory_limit: 2GB }
       *   virtual_devices {}
       * will create three virtual devices as:
       *   /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
       *   /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
       *   /device:GPU:2 -&gt; visible GPU 0 with all available memory
       * NOTE:
       * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
       *    at the same time.
       * 2. Currently this setting is per-process, not per-session. Using
       *    different settings in different sessions within same process will
       *    result in undefined behavior.
       * </pre>
       *
       * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
       */
      public Builder removeVirtualDevices(int index) {
        if (virtualDevicesBuilder_ == null) {
          ensureVirtualDevicesIsMutable();
          virtualDevices_.remove(index);
          onChanged();
        } else {
          virtualDevicesBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <pre>
       * The multi virtual device settings. If empty (not set), it will create
       * single virtual device on each visible GPU, according to the settings
       * in "visible_device_list" above. Otherwise, the number of elements in the
       * list must be the same as the number of visible GPUs (after
       * "visible_device_list" filtering if it is set), and the string represented
       * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
       * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
       * according to the order they appear in this list and the "memory_limit"
       * list inside each element. For example,
       *   visible_device_list = "1,0"
       *   virtual_devices { memory_limit: 1GB memory_limit: 2GB }
       *   virtual_devices {}
       * will create three virtual devices as:
       *   /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
       *   /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
       *   /device:GPU:2 -&gt; visible GPU 0 with all available memory
       * NOTE:
       * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
       *    at the same time.
       * 2. Currently this setting is per-process, not per-session. Using
       *    different settings in different sessions within same process will
       *    result in undefined behavior.
       * </pre>
       *
       * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
       */
      public org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices.Builder getVirtualDevicesBuilder(
          int index) {
        return getVirtualDevicesFieldBuilder().getBuilder(index);
      }
      /**
       * <pre>
       * The multi virtual device settings. If empty (not set), it will create
       * single virtual device on each visible GPU, according to the settings
       * in "visible_device_list" above. Otherwise, the number of elements in the
       * list must be the same as the number of visible GPUs (after
       * "visible_device_list" filtering if it is set), and the string represented
       * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
       * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
       * according to the order they appear in this list and the "memory_limit"
       * list inside each element. For example,
       *   visible_device_list = "1,0"
       *   virtual_devices { memory_limit: 1GB memory_limit: 2GB }
       *   virtual_devices {}
       * will create three virtual devices as:
       *   /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
       *   /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
       *   /device:GPU:2 -&gt; visible GPU 0 with all available memory
       * NOTE:
       * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
       *    at the same time.
       * 2. Currently this setting is per-process, not per-session. Using
       *    different settings in different sessions within same process will
       *    result in undefined behavior.
       * </pre>
       *
       * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
       */
      public org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevicesOrBuilder getVirtualDevicesOrBuilder(
          int index) {
        if (virtualDevicesBuilder_ == null) {
          return virtualDevices_.get(index);  } else {
          return virtualDevicesBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <pre>
       * The multi virtual device settings. If empty (not set), it will create
       * single virtual device on each visible GPU, according to the settings
       * in "visible_device_list" above. Otherwise, the number of elements in the
       * list must be the same as the number of visible GPUs (after
       * "visible_device_list" filtering if it is set), and the string represented
       * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
       * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
       * according to the order they appear in this list and the "memory_limit"
       * list inside each element. For example,
       *   visible_device_list = "1,0"
       *   virtual_devices { memory_limit: 1GB memory_limit: 2GB }
       *   virtual_devices {}
       * will create three virtual devices as:
       *   /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
       *   /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
       *   /device:GPU:2 -&gt; visible GPU 0 with all available memory
       * NOTE:
       * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
       *    at the same time.
       * 2. Currently this setting is per-process, not per-session. Using
       *    different settings in different sessions within same process will
       *    result in undefined behavior.
       * </pre>
       *
       * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
       */
      public java.util.List<? extends org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevicesOrBuilder> 
           getVirtualDevicesOrBuilderList() {
        if (virtualDevicesBuilder_ != null) {
          return virtualDevicesBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(virtualDevices_);
        }
      }
      /**
       * <pre>
       * The multi virtual device settings. If empty (not set), it will create
       * single virtual device on each visible GPU, according to the settings
       * in "visible_device_list" above. Otherwise, the number of elements in the
       * list must be the same as the number of visible GPUs (after
       * "visible_device_list" filtering if it is set), and the string represented
       * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
       * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
       * according to the order they appear in this list and the "memory_limit"
       * list inside each element. For example,
       *   visible_device_list = "1,0"
       *   virtual_devices { memory_limit: 1GB memory_limit: 2GB }
       *   virtual_devices {}
       * will create three virtual devices as:
       *   /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
       *   /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
       *   /device:GPU:2 -&gt; visible GPU 0 with all available memory
       * NOTE:
       * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
       *    at the same time.
       * 2. Currently this setting is per-process, not per-session. Using
       *    different settings in different sessions within same process will
       *    result in undefined behavior.
       * </pre>
       *
       * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
       */
      public org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices.Builder addVirtualDevicesBuilder() {
        return getVirtualDevicesFieldBuilder().addBuilder(
            org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices.getDefaultInstance());
      }
      /**
       * <pre>
       * The multi virtual device settings. If empty (not set), it will create
       * single virtual device on each visible GPU, according to the settings
       * in "visible_device_list" above. Otherwise, the number of elements in the
       * list must be the same as the number of visible GPUs (after
       * "visible_device_list" filtering if it is set), and the string represented
       * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
       * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
       * according to the order they appear in this list and the "memory_limit"
       * list inside each element. For example,
       *   visible_device_list = "1,0"
       *   virtual_devices { memory_limit: 1GB memory_limit: 2GB }
       *   virtual_devices {}
       * will create three virtual devices as:
       *   /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
       *   /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
       *   /device:GPU:2 -&gt; visible GPU 0 with all available memory
       * NOTE:
       * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
       *    at the same time.
       * 2. Currently this setting is per-process, not per-session. Using
       *    different settings in different sessions within same process will
       *    result in undefined behavior.
       * </pre>
       *
       * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
       */
      public org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices.Builder addVirtualDevicesBuilder(
          int index) {
        return getVirtualDevicesFieldBuilder().addBuilder(
            index, org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices.getDefaultInstance());
      }
      /**
       * <pre>
       * The multi virtual device settings. If empty (not set), it will create
       * single virtual device on each visible GPU, according to the settings
       * in "visible_device_list" above. Otherwise, the number of elements in the
       * list must be the same as the number of visible GPUs (after
       * "visible_device_list" filtering if it is set), and the string represented
       * device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
       * devices and have the &lt;id&gt; field assigned sequentially starting from 0,
       * according to the order they appear in this list and the "memory_limit"
       * list inside each element. For example,
       *   visible_device_list = "1,0"
       *   virtual_devices { memory_limit: 1GB memory_limit: 2GB }
       *   virtual_devices {}
       * will create three virtual devices as:
       *   /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
       *   /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
       *   /device:GPU:2 -&gt; visible GPU 0 with all available memory
       * NOTE:
       * 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
       *    at the same time.
       * 2. Currently this setting is per-process, not per-session. Using
       *    different settings in different sessions within same process will
       *    result in undefined behavior.
       * </pre>
       *
       * <code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code>
       */
      public java.util.List<org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices.Builder> 
           getVirtualDevicesBuilderList() {
        return getVirtualDevicesFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices, org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices.Builder, org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevicesOrBuilder> 
          getVirtualDevicesFieldBuilder() {
        if (virtualDevicesBuilder_ == null) {
          virtualDevicesBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices, org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevices.Builder, org.tensorflow.proto.framework.GPUOptions.Experimental.VirtualDevicesOrBuilder>(
                  virtualDevices_,
                  ((bitField0_ & 0x00000001) != 0),
                  getParentForChildren(),
                  isClean());
          virtualDevices_ = null;
        }
        return virtualDevicesBuilder_;
      }

      private boolean useUnifiedMemory_ ;
      /**
       * <pre>
       * If true, uses CUDA unified memory for memory allocations. If
       * per_process_gpu_memory_fraction option is greater than 1.0, then unified
       * memory is used regardless of the value for this field. See comments for
       * per_process_gpu_memory_fraction field for more details and requirements
       * of the unified memory. This option is useful to oversubscribe memory if
       * multiple processes are sharing a single GPU while individually using less
       * than 1.0 per process memory fraction.
       * </pre>
       *
       * <code>bool use_unified_memory = 2;</code>
       */
      public boolean getUseUnifiedMemory() {
        return useUnifiedMemory_;
      }
      /**
       * <pre>
       * If true, uses CUDA unified memory for memory allocations. If
       * per_process_gpu_memory_fraction option is greater than 1.0, then unified
       * memory is used regardless of the value for this field. See comments for
       * per_process_gpu_memory_fraction field for more details and requirements
       * of the unified memory. This option is useful to oversubscribe memory if
       * multiple processes are sharing a single GPU while individually using less
       * than 1.0 per process memory fraction.
       * </pre>
       *
       * <code>bool use_unified_memory = 2;</code>
       */
      public Builder setUseUnifiedMemory(boolean value) {
        
        useUnifiedMemory_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * If true, uses CUDA unified memory for memory allocations. If
       * per_process_gpu_memory_fraction option is greater than 1.0, then unified
       * memory is used regardless of the value for this field. See comments for
       * per_process_gpu_memory_fraction field for more details and requirements
       * of the unified memory. This option is useful to oversubscribe memory if
       * multiple processes are sharing a single GPU while individually using less
       * than 1.0 per process memory fraction.
       * </pre>
       *
       * <code>bool use_unified_memory = 2;</code>
       */
      public Builder clearUseUnifiedMemory() {
        
        useUnifiedMemory_ = false;
        onChanged();
        return this;
      }

      private int numDevToDevCopyStreams_ ;
      /**
       * <pre>
       * If &gt; 1, the number of device-to-device copy streams to create
       * for each GPUDevice.  Default value is 0, which is automatically
       * converted to 1.
       * </pre>
       *
       * <code>int32 num_dev_to_dev_copy_streams = 3;</code>
       */
      public int getNumDevToDevCopyStreams() {
        return numDevToDevCopyStreams_;
      }
      /**
       * <pre>
       * If &gt; 1, the number of device-to-device copy streams to create
       * for each GPUDevice.  Default value is 0, which is automatically
       * converted to 1.
       * </pre>
       *
       * <code>int32 num_dev_to_dev_copy_streams = 3;</code>
       */
      public Builder setNumDevToDevCopyStreams(int value) {
        
        numDevToDevCopyStreams_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * If &gt; 1, the number of device-to-device copy streams to create
       * for each GPUDevice.  Default value is 0, which is automatically
       * converted to 1.
       * </pre>
       *
       * <code>int32 num_dev_to_dev_copy_streams = 3;</code>
       */
      public Builder clearNumDevToDevCopyStreams() {
        
        numDevToDevCopyStreams_ = 0;
        onChanged();
        return this;
      }

      private java.lang.Object collectiveRingOrder_ = "";
      /**
       * <pre>
       * If non-empty, defines a good GPU ring order on a single worker based on
       * device interconnect.  This assumes that all workers have the same GPU
       * topology.  Specify as a comma-separated string, e.g. "3,2,1,0,7,6,5,4".
       * This ring order is used by the RingReducer implementation of
       * CollectiveReduce, and serves as an override to automatic ring order
       * generation in OrderTaskDeviceMap() during CollectiveParam resolution.
       * </pre>
       *
       * <code>string collective_ring_order = 4;</code>
       */
      public java.lang.String getCollectiveRingOrder() {
        java.lang.Object ref = collectiveRingOrder_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          collectiveRingOrder_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <pre>
       * If non-empty, defines a good GPU ring order on a single worker based on
       * device interconnect.  This assumes that all workers have the same GPU
       * topology.  Specify as a comma-separated string, e.g. "3,2,1,0,7,6,5,4".
       * This ring order is used by the RingReducer implementation of
       * CollectiveReduce, and serves as an override to automatic ring order
       * generation in OrderTaskDeviceMap() during CollectiveParam resolution.
       * </pre>
       *
       * <code>string collective_ring_order = 4;</code>
       */
      public com.google.protobuf.ByteString
          getCollectiveRingOrderBytes() {
        java.lang.Object ref = collectiveRingOrder_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          collectiveRingOrder_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <pre>
       * If non-empty, defines a good GPU ring order on a single worker based on
       * device interconnect.  This assumes that all workers have the same GPU
       * topology.  Specify as a comma-separated string, e.g. "3,2,1,0,7,6,5,4".
       * This ring order is used by the RingReducer implementation of
       * CollectiveReduce, and serves as an override to automatic ring order
       * generation in OrderTaskDeviceMap() during CollectiveParam resolution.
       * </pre>
       *
       * <code>string collective_ring_order = 4;</code>
       */
      public Builder setCollectiveRingOrder(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        collectiveRingOrder_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * If non-empty, defines a good GPU ring order on a single worker based on
       * device interconnect.  This assumes that all workers have the same GPU
       * topology.  Specify as a comma-separated string, e.g. "3,2,1,0,7,6,5,4".
       * This ring order is used by the RingReducer implementation of
       * CollectiveReduce, and serves as an override to automatic ring order
       * generation in OrderTaskDeviceMap() during CollectiveParam resolution.
       * </pre>
       *
       * <code>string collective_ring_order = 4;</code>
       */
      public Builder clearCollectiveRingOrder() {
        
        collectiveRingOrder_ = getDefaultInstance().getCollectiveRingOrder();
        onChanged();
        return this;
      }
      /**
       * <pre>
       * If non-empty, defines a good GPU ring order on a single worker based on
       * device interconnect.  This assumes that all workers have the same GPU
       * topology.  Specify as a comma-separated string, e.g. "3,2,1,0,7,6,5,4".
       * This ring order is used by the RingReducer implementation of
       * CollectiveReduce, and serves as an override to automatic ring order
       * generation in OrderTaskDeviceMap() during CollectiveParam resolution.
       * </pre>
       *
       * <code>string collective_ring_order = 4;</code>
       */
      public Builder setCollectiveRingOrderBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        collectiveRingOrder_ = value;
        onChanged();
        return this;
      }

      private boolean timestampedAllocator_ ;
      /**
       * <pre>
       * If true then extra work is done by GPUDevice and GPUBFCAllocator to
       * keep track of when GPU memory is freed and when kernels actually
       * complete so that we can know when a nominally free memory chunk
       * is really not subject to pending use.
       * </pre>
       *
       * <code>bool timestamped_allocator = 5;</code>
       */
      public boolean getTimestampedAllocator() {
        return timestampedAllocator_;
      }
      /**
       * <pre>
       * If true then extra work is done by GPUDevice and GPUBFCAllocator to
       * keep track of when GPU memory is freed and when kernels actually
       * complete so that we can know when a nominally free memory chunk
       * is really not subject to pending use.
       * </pre>
       *
       * <code>bool timestamped_allocator = 5;</code>
       */
      public Builder setTimestampedAllocator(boolean value) {
        
        timestampedAllocator_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * If true then extra work is done by GPUDevice and GPUBFCAllocator to
       * keep track of when GPU memory is freed and when kernels actually
       * complete so that we can know when a nominally free memory chunk
       * is really not subject to pending use.
       * </pre>
       *
       * <code>bool timestamped_allocator = 5;</code>
       */
      public Builder clearTimestampedAllocator() {
        
        timestampedAllocator_ = false;
        onChanged();
        return this;
      }

      private int kernelTrackerMaxInterval_ ;
      /**
       * <pre>
       * Parameters for GPUKernelTracker.  By default no kernel tracking is done.
       * Note that timestamped_allocator is only effective if some tracking is
       * specified.
       * If kernel_tracker_max_interval = n &gt; 0, then a tracking event
       * is inserted after every n kernels without an event.
       * </pre>
       *
       * <code>int32 kernel_tracker_max_interval = 7;</code>
       */
      public int getKernelTrackerMaxInterval() {
        return kernelTrackerMaxInterval_;
      }
      /**
       * <pre>
       * Parameters for GPUKernelTracker.  By default no kernel tracking is done.
       * Note that timestamped_allocator is only effective if some tracking is
       * specified.
       * If kernel_tracker_max_interval = n &gt; 0, then a tracking event
       * is inserted after every n kernels without an event.
       * </pre>
       *
       * <code>int32 kernel_tracker_max_interval = 7;</code>
       */
      public Builder setKernelTrackerMaxInterval(int value) {
        
        kernelTrackerMaxInterval_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * Parameters for GPUKernelTracker.  By default no kernel tracking is done.
       * Note that timestamped_allocator is only effective if some tracking is
       * specified.
       * If kernel_tracker_max_interval = n &gt; 0, then a tracking event
       * is inserted after every n kernels without an event.
       * </pre>
       *
       * <code>int32 kernel_tracker_max_interval = 7;</code>
       */
      public Builder clearKernelTrackerMaxInterval() {
        
        kernelTrackerMaxInterval_ = 0;
        onChanged();
        return this;
      }

      private int kernelTrackerMaxBytes_ ;
      /**
       * <pre>
       * If kernel_tracker_max_bytes = n &gt; 0, then a tracking event is
       * inserted after every series of kernels allocating a sum of
       * memory &gt;= n.  If one kernel allocates b * n bytes, then one
       * event will be inserted after it, but it will count as b against
       * the pending limit.
       * </pre>
       *
       * <code>int32 kernel_tracker_max_bytes = 8;</code>
       */
      public int getKernelTrackerMaxBytes() {
        return kernelTrackerMaxBytes_;
      }
      /**
       * <pre>
       * If kernel_tracker_max_bytes = n &gt; 0, then a tracking event is
       * inserted after every series of kernels allocating a sum of
       * memory &gt;= n.  If one kernel allocates b * n bytes, then one
       * event will be inserted after it, but it will count as b against
       * the pending limit.
       * </pre>
       *
       * <code>int32 kernel_tracker_max_bytes = 8;</code>
       */
      public Builder setKernelTrackerMaxBytes(int value) {
        
        kernelTrackerMaxBytes_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * If kernel_tracker_max_bytes = n &gt; 0, then a tracking event is
       * inserted after every series of kernels allocating a sum of
       * memory &gt;= n.  If one kernel allocates b * n bytes, then one
       * event will be inserted after it, but it will count as b against
       * the pending limit.
       * </pre>
       *
       * <code>int32 kernel_tracker_max_bytes = 8;</code>
       */
      public Builder clearKernelTrackerMaxBytes() {
        
        kernelTrackerMaxBytes_ = 0;
        onChanged();
        return this;
      }

      private int kernelTrackerMaxPending_ ;
      /**
       * <pre>
       * If kernel_tracker_max_pending &gt; 0 then no more than this many
       * tracking events can be outstanding at a time.  An attempt to
       * launch an additional kernel will stall until an event
       * completes.
       * </pre>
       *
       * <code>int32 kernel_tracker_max_pending = 9;</code>
       */
      public int getKernelTrackerMaxPending() {
        return kernelTrackerMaxPending_;
      }
      /**
       * <pre>
       * If kernel_tracker_max_pending &gt; 0 then no more than this many
       * tracking events can be outstanding at a time.  An attempt to
       * launch an additional kernel will stall until an event
       * completes.
       * </pre>
       *
       * <code>int32 kernel_tracker_max_pending = 9;</code>
       */
      public Builder setKernelTrackerMaxPending(int value) {
        
        kernelTrackerMaxPending_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * If kernel_tracker_max_pending &gt; 0 then no more than this many
       * tracking events can be outstanding at a time.  An attempt to
       * launch an additional kernel will stall until an event
       * completes.
       * </pre>
       *
       * <code>int32 kernel_tracker_max_pending = 9;</code>
       */
      public Builder clearKernelTrackerMaxPending() {
        
        kernelTrackerMaxPending_ = 0;
        onChanged();
        return this;
      }

      private double internalFragmentationFraction_ ;
      /**
       * <pre>
       * BFC Allocator can return an allocated chunk of memory upto 2x the
       * requested size. For virtual devices with tight memory constraints, and
       * proportionately large allocation requests, this can lead to a significant
       * reduction in available memory. The threshold below controls when a chunk
       * should be split if the chunk size exceeds requested memory size. It is
       * expressed as a fraction of total available memory for the tf device. For
       * example setting it to 0.05 would imply a chunk needs to be split if its
       * size exceeds the requested memory by 5% of the total virtual device/gpu
       * memory size.
       * </pre>
       *
       * <code>double internal_fragmentation_fraction = 10;</code>
       */
      public double getInternalFragmentationFraction() {
        return internalFragmentationFraction_;
      }
      /**
       * <pre>
       * BFC Allocator can return an allocated chunk of memory upto 2x the
       * requested size. For virtual devices with tight memory constraints, and
       * proportionately large allocation requests, this can lead to a significant
       * reduction in available memory. The threshold below controls when a chunk
       * should be split if the chunk size exceeds requested memory size. It is
       * expressed as a fraction of total available memory for the tf device. For
       * example setting it to 0.05 would imply a chunk needs to be split if its
       * size exceeds the requested memory by 5% of the total virtual device/gpu
       * memory size.
       * </pre>
       *
       * <code>double internal_fragmentation_fraction = 10;</code>
       */
      public Builder setInternalFragmentationFraction(double value) {
        
        internalFragmentationFraction_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * BFC Allocator can return an allocated chunk of memory upto 2x the
       * requested size. For virtual devices with tight memory constraints, and
       * proportionately large allocation requests, this can lead to a significant
       * reduction in available memory. The threshold below controls when a chunk
       * should be split if the chunk size exceeds requested memory size. It is
       * expressed as a fraction of total available memory for the tf device. For
       * example setting it to 0.05 would imply a chunk needs to be split if its
       * size exceeds the requested memory by 5% of the total virtual device/gpu
       * memory size.
       * </pre>
       *
       * <code>double internal_fragmentation_fraction = 10;</code>
       */
      public Builder clearInternalFragmentationFraction() {
        
        internalFragmentationFraction_ = 0D;
        onChanged();
        return this;
      }

      private boolean useCudaMallocAsync_ ;
      /**
       * <pre>
       * When true, use CUDA cudaMallocAsync API instead of TF gpu allocator.
       * </pre>
       *
       * <code>bool use_cuda_malloc_async = 11;</code>
       */
      public boolean getUseCudaMallocAsync() {
        return useCudaMallocAsync_;
      }
      /**
       * <pre>
       * When true, use CUDA cudaMallocAsync API instead of TF gpu allocator.
       * </pre>
       *
       * <code>bool use_cuda_malloc_async = 11;</code>
       */
      public Builder setUseCudaMallocAsync(boolean value) {
        
        useCudaMallocAsync_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * When true, use CUDA cudaMallocAsync API instead of TF gpu allocator.
       * </pre>
       *
       * <code>bool use_cuda_malloc_async = 11;</code>
       */
      public Builder clearUseCudaMallocAsync() {
        
        useCudaMallocAsync_ = false;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:tensorflow.GPUOptions.Experimental)
    }

    // @@protoc_insertion_point(class_scope:tensorflow.GPUOptions.Experimental)
    private static final org.tensorflow.proto.framework.GPUOptions.Experimental DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.tensorflow.proto.framework.GPUOptions.Experimental();
    }

    public static org.tensorflow.proto.framework.GPUOptions.Experimental getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<Experimental>
        PARSER = new com.google.protobuf.AbstractParser<Experimental>() {
      @java.lang.Override
      public Experimental parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new Experimental(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<Experimental> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<Experimental> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.tensorflow.proto.framework.GPUOptions.Experimental getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public static final int PER_PROCESS_GPU_MEMORY_FRACTION_FIELD_NUMBER = 1;
  private double perProcessGpuMemoryFraction_;
  /**
   * <pre>
   * Fraction of the available GPU memory to allocate for each process.
   * 1 means to allocate all of the GPU memory, 0.5 means the process
   * allocates up to ~50% of the available GPU memory.
   * GPU memory is pre-allocated unless the allow_growth option is enabled.
   * If greater than 1.0, uses CUDA unified memory to potentially oversubscribe
   * the amount of memory available on the GPU device by using host memory as a
   * swap space. Accessing memory not available on the device will be
   * significantly slower as that would require memory transfer between the host
   * and the device. Options to reduce the memory requirement should be
   * considered before enabling this option as this may come with a negative
   * performance impact. Oversubscription using the unified memory requires
   * Pascal class or newer GPUs and it is currently only supported on the Linux
   * operating system. See
   * https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-requirements
   * for the detailed requirements.
   * </pre>
   *
   * <code>double per_process_gpu_memory_fraction = 1;</code>
   */
  public double getPerProcessGpuMemoryFraction() {
    return perProcessGpuMemoryFraction_;
  }

  public static final int ALLOW_GROWTH_FIELD_NUMBER = 4;
  private boolean allowGrowth_;
  /**
   * <pre>
   * If true, the allocator does not pre-allocate the entire specified
   * GPU memory region, instead starting small and growing as needed.
   * </pre>
   *
   * <code>bool allow_growth = 4;</code>
   */
  public boolean getAllowGrowth() {
    return allowGrowth_;
  }

  public static final int ALLOCATOR_TYPE_FIELD_NUMBER = 2;
  private volatile java.lang.Object allocatorType_;
  /**
   * <pre>
   * The type of GPU allocation strategy to use.
   * Allowed values:
   * "": The empty string (default) uses a system-chosen default
   *     which may change over time.
   * "BFC": A "Best-fit with coalescing" algorithm, simplified from a
   *        version of dlmalloc.
   * </pre>
   *
   * <code>string allocator_type = 2;</code>
   */
  public java.lang.String getAllocatorType() {
    java.lang.Object ref = allocatorType_;
    if (ref instanceof java.lang.String) {
      return (java.lang.String) ref;
    } else {
      com.google.protobuf.ByteString bs = 
          (com.google.protobuf.ByteString) ref;
      java.lang.String s = bs.toStringUtf8();
      allocatorType_ = s;
      return s;
    }
  }
  /**
   * <pre>
   * The type of GPU allocation strategy to use.
   * Allowed values:
   * "": The empty string (default) uses a system-chosen default
   *     which may change over time.
   * "BFC": A "Best-fit with coalescing" algorithm, simplified from a
   *        version of dlmalloc.
   * </pre>
   *
   * <code>string allocator_type = 2;</code>
   */
  public com.google.protobuf.ByteString
      getAllocatorTypeBytes() {
    java.lang.Object ref = allocatorType_;
    if (ref instanceof java.lang.String) {
      com.google.protobuf.ByteString b = 
          com.google.protobuf.ByteString.copyFromUtf8(
              (java.lang.String) ref);
      allocatorType_ = b;
      return b;
    } else {
      return (com.google.protobuf.ByteString) ref;
    }
  }

  public static final int DEFERRED_DELETION_BYTES_FIELD_NUMBER = 3;
  private long deferredDeletionBytes_;
  /**
   * <pre>
   * Delay deletion of up to this many bytes to reduce the number of
   * interactions with gpu driver code.  If 0, the system chooses
   * a reasonable default (several MBs).
   * </pre>
   *
   * <code>int64 deferred_deletion_bytes = 3;</code>
   */
  public long getDeferredDeletionBytes() {
    return deferredDeletionBytes_;
  }

  public static final int VISIBLE_DEVICE_LIST_FIELD_NUMBER = 5;
  private volatile java.lang.Object visibleDeviceList_;
  /**
   * <pre>
   * A comma-separated list of GPU ids that determines the 'visible'
   * to 'virtual' mapping of GPU devices.  For example, if TensorFlow
   * can see 8 GPU devices in the process, and one wanted to map
   * visible GPU devices 5 and 3 as "/device:GPU:0", and "/device:GPU:1",
   * then one would specify this field as "5,3".  This field is similar in
   * spirit to the CUDA_VISIBLE_DEVICES environment variable, except
   * it applies to the visible GPU devices in the process.
   * NOTE:
   * 1. The GPU driver provides the process with the visible GPUs
   *    in an order which is not guaranteed to have any correlation to
   *    the *physical* GPU id in the machine.  This field is used for
   *    remapping "visible" to "virtual", which means this operates only
   *    after the process starts.  Users are required to use vendor
   *    specific mechanisms (e.g., CUDA_VISIBLE_DEVICES) to control the
   *    physical to visible device mapping prior to invoking TensorFlow.
   * 2. In the code, the ids in this list are also called "platform GPU id"s,
   *    and the 'virtual' ids of GPU devices (i.e. the ids in the device
   *    name "/device:GPU:&lt;id&gt;") are also called "TF GPU id"s. Please
   *    refer to third_party/tensorflow/core/common_runtime/gpu/gpu_id.h
   *    for more information.
   * </pre>
   *
   * <code>string visible_device_list = 5;</code>
   */
  public java.lang.String getVisibleDeviceList() {
    java.lang.Object ref = visibleDeviceList_;
    if (ref instanceof java.lang.String) {
      return (java.lang.String) ref;
    } else {
      com.google.protobuf.ByteString bs = 
          (com.google.protobuf.ByteString) ref;
      java.lang.String s = bs.toStringUtf8();
      visibleDeviceList_ = s;
      return s;
    }
  }
  /**
   * <pre>
   * A comma-separated list of GPU ids that determines the 'visible'
   * to 'virtual' mapping of GPU devices.  For example, if TensorFlow
   * can see 8 GPU devices in the process, and one wanted to map
   * visible GPU devices 5 and 3 as "/device:GPU:0", and "/device:GPU:1",
   * then one would specify this field as "5,3".  This field is similar in
   * spirit to the CUDA_VISIBLE_DEVICES environment variable, except
   * it applies to the visible GPU devices in the process.
   * NOTE:
   * 1. The GPU driver provides the process with the visible GPUs
   *    in an order which is not guaranteed to have any correlation to
   *    the *physical* GPU id in the machine.  This field is used for
   *    remapping "visible" to "virtual", which means this operates only
   *    after the process starts.  Users are required to use vendor
   *    specific mechanisms (e.g., CUDA_VISIBLE_DEVICES) to control the
   *    physical to visible device mapping prior to invoking TensorFlow.
   * 2. In the code, the ids in this list are also called "platform GPU id"s,
   *    and the 'virtual' ids of GPU devices (i.e. the ids in the device
   *    name "/device:GPU:&lt;id&gt;") are also called "TF GPU id"s. Please
   *    refer to third_party/tensorflow/core/common_runtime/gpu/gpu_id.h
   *    for more information.
   * </pre>
   *
   * <code>string visible_device_list = 5;</code>
   */
  public com.google.protobuf.ByteString
      getVisibleDeviceListBytes() {
    java.lang.Object ref = visibleDeviceList_;
    if (ref instanceof java.lang.String) {
      com.google.protobuf.ByteString b = 
          com.google.protobuf.ByteString.copyFromUtf8(
              (java.lang.String) ref);
      visibleDeviceList_ = b;
      return b;
    } else {
      return (com.google.protobuf.ByteString) ref;
    }
  }

  public static final int POLLING_ACTIVE_DELAY_USECS_FIELD_NUMBER = 6;
  private int pollingActiveDelayUsecs_;
  /**
   * <pre>
   * In the event polling loop sleep this many microseconds between
   * PollEvents calls, when the queue is not empty.  If value is not
   * set or set to 0, gets set to a non-zero default.
   * </pre>
   *
   * <code>int32 polling_active_delay_usecs = 6;</code>
   */
  public int getPollingActiveDelayUsecs() {
    return pollingActiveDelayUsecs_;
  }

  public static final int POLLING_INACTIVE_DELAY_MSECS_FIELD_NUMBER = 7;
  private int pollingInactiveDelayMsecs_;
  /**
   * <pre>
   * This field is deprecated and ignored.
   * </pre>
   *
   * <code>int32 polling_inactive_delay_msecs = 7;</code>
   */
  public int getPollingInactiveDelayMsecs() {
    return pollingInactiveDelayMsecs_;
  }

  public static final int FORCE_GPU_COMPATIBLE_FIELD_NUMBER = 8;
  private boolean forceGpuCompatible_;
  /**
   * <pre>
   * Force all tensors to be gpu_compatible. On a GPU-enabled TensorFlow,
   * enabling this option forces all CPU tensors to be allocated with Cuda
   * pinned memory. Normally, TensorFlow will infer which tensors should be
   * allocated as the pinned memory. But in case where the inference is
   * incomplete, this option can significantly speed up the cross-device memory
   * copy performance as long as it fits the memory.
   * Note that this option is not something that should be
   * enabled by default for unknown or very large models, since all Cuda pinned
   * memory is unpageable, having too much pinned memory might negatively impact
   * the overall host system performance.
   * </pre>
   *
   * <code>bool force_gpu_compatible = 8;</code>
   */
  public boolean getForceGpuCompatible() {
    return forceGpuCompatible_;
  }

  public static final int EXPERIMENTAL_FIELD_NUMBER = 9;
  private org.tensorflow.proto.framework.GPUOptions.Experimental experimental_;
  /**
   * <pre>
   * Everything inside experimental is subject to change and is not subject
   * to API stability guarantees in
   * https://www.tensorflow.org/guide/version_compat.
   * </pre>
   *
   * <code>.tensorflow.GPUOptions.Experimental experimental = 9;</code>
   */
  public boolean hasExperimental() {
    return experimental_ != null;
  }
  /**
   * <pre>
   * Everything inside experimental is subject to change and is not subject
   * to API stability guarantees in
   * https://www.tensorflow.org/guide/version_compat.
   * </pre>
   *
   * <code>.tensorflow.GPUOptions.Experimental experimental = 9;</code>
   */
  public org.tensorflow.proto.framework.GPUOptions.Experimental getExperimental() {
    return experimental_ == null ? org.tensorflow.proto.framework.GPUOptions.Experimental.getDefaultInstance() : experimental_;
  }
  /**
   * <pre>
   * Everything inside experimental is subject to change and is not subject
   * to API stability guarantees in
   * https://www.tensorflow.org/guide/version_compat.
   * </pre>
   *
   * <code>.tensorflow.GPUOptions.Experimental experimental = 9;</code>
   */
  public org.tensorflow.proto.framework.GPUOptions.ExperimentalOrBuilder getExperimentalOrBuilder() {
    return getExperimental();
  }

  private byte memoizedIsInitialized = -1;
  @java.lang.Override
  public final boolean isInitialized() {
    byte isInitialized = memoizedIsInitialized;
    if (isInitialized == 1) return true;
    if (isInitialized == 0) return false;

    memoizedIsInitialized = 1;
    return true;
  }

  @java.lang.Override
  public void writeTo(com.google.protobuf.CodedOutputStream output)
                      throws java.io.IOException {
    if (perProcessGpuMemoryFraction_ != 0D) {
      output.writeDouble(1, perProcessGpuMemoryFraction_);
    }
    if (!getAllocatorTypeBytes().isEmpty()) {
      com.google.protobuf.GeneratedMessageV3.writeString(output, 2, allocatorType_);
    }
    if (deferredDeletionBytes_ != 0L) {
      output.writeInt64(3, deferredDeletionBytes_);
    }
    if (allowGrowth_ != false) {
      output.writeBool(4, allowGrowth_);
    }
    if (!getVisibleDeviceListBytes().isEmpty()) {
      com.google.protobuf.GeneratedMessageV3.writeString(output, 5, visibleDeviceList_);
    }
    if (pollingActiveDelayUsecs_ != 0) {
      output.writeInt32(6, pollingActiveDelayUsecs_);
    }
    if (pollingInactiveDelayMsecs_ != 0) {
      output.writeInt32(7, pollingInactiveDelayMsecs_);
    }
    if (forceGpuCompatible_ != false) {
      output.writeBool(8, forceGpuCompatible_);
    }
    if (experimental_ != null) {
      output.writeMessage(9, getExperimental());
    }
    unknownFields.writeTo(output);
  }

  @java.lang.Override
  public int getSerializedSize() {
    int size = memoizedSize;
    if (size != -1) return size;

    size = 0;
    if (perProcessGpuMemoryFraction_ != 0D) {
      size += com.google.protobuf.CodedOutputStream
        .computeDoubleSize(1, perProcessGpuMemoryFraction_);
    }
    if (!getAllocatorTypeBytes().isEmpty()) {
      size += com.google.protobuf.GeneratedMessageV3.computeStringSize(2, allocatorType_);
    }
    if (deferredDeletionBytes_ != 0L) {
      size += com.google.protobuf.CodedOutputStream
        .computeInt64Size(3, deferredDeletionBytes_);
    }
    if (allowGrowth_ != false) {
      size += com.google.protobuf.CodedOutputStream
        .computeBoolSize(4, allowGrowth_);
    }
    if (!getVisibleDeviceListBytes().isEmpty()) {
      size += com.google.protobuf.GeneratedMessageV3.computeStringSize(5, visibleDeviceList_);
    }
    if (pollingActiveDelayUsecs_ != 0) {
      size += com.google.protobuf.CodedOutputStream
        .computeInt32Size(6, pollingActiveDelayUsecs_);
    }
    if (pollingInactiveDelayMsecs_ != 0) {
      size += com.google.protobuf.CodedOutputStream
        .computeInt32Size(7, pollingInactiveDelayMsecs_);
    }
    if (forceGpuCompatible_ != false) {
      size += com.google.protobuf.CodedOutputStream
        .computeBoolSize(8, forceGpuCompatible_);
    }
    if (experimental_ != null) {
      size += com.google.protobuf.CodedOutputStream
        .computeMessageSize(9, getExperimental());
    }
    size += unknownFields.getSerializedSize();
    memoizedSize = size;
    return size;
  }

  @java.lang.Override
  public boolean equals(final java.lang.Object obj) {
    if (obj == this) {
     return true;
    }
    if (!(obj instanceof org.tensorflow.proto.framework.GPUOptions)) {
      return super.equals(obj);
    }
    org.tensorflow.proto.framework.GPUOptions other = (org.tensorflow.proto.framework.GPUOptions) obj;

    if (java.lang.Double.doubleToLongBits(getPerProcessGpuMemoryFraction())
        != java.lang.Double.doubleToLongBits(
            other.getPerProcessGpuMemoryFraction())) return false;
    if (getAllowGrowth()
        != other.getAllowGrowth()) return false;
    if (!getAllocatorType()
        .equals(other.getAllocatorType())) return false;
    if (getDeferredDeletionBytes()
        != other.getDeferredDeletionBytes()) return false;
    if (!getVisibleDeviceList()
        .equals(other.getVisibleDeviceList())) return false;
    if (getPollingActiveDelayUsecs()
        != other.getPollingActiveDelayUsecs()) return false;
    if (getPollingInactiveDelayMsecs()
        != other.getPollingInactiveDelayMsecs()) return false;
    if (getForceGpuCompatible()
        != other.getForceGpuCompatible()) return false;
    if (hasExperimental() != other.hasExperimental()) return false;
    if (hasExperimental()) {
      if (!getExperimental()
          .equals(other.getExperimental())) return false;
    }
    if (!unknownFields.equals(other.unknownFields)) return false;
    return true;
  }

  @java.lang.Override
  public int hashCode() {
    if (memoizedHashCode != 0) {
      return memoizedHashCode;
    }
    int hash = 41;
    hash = (19 * hash) + getDescriptor().hashCode();
    hash = (37 * hash) + PER_PROCESS_GPU_MEMORY_FRACTION_FIELD_NUMBER;
    hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
        java.lang.Double.doubleToLongBits(getPerProcessGpuMemoryFraction()));
    hash = (37 * hash) + ALLOW_GROWTH_FIELD_NUMBER;
    hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
        getAllowGrowth());
    hash = (37 * hash) + ALLOCATOR_TYPE_FIELD_NUMBER;
    hash = (53 * hash) + getAllocatorType().hashCode();
    hash = (37 * hash) + DEFERRED_DELETION_BYTES_FIELD_NUMBER;
    hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
        getDeferredDeletionBytes());
    hash = (37 * hash) + VISIBLE_DEVICE_LIST_FIELD_NUMBER;
    hash = (53 * hash) + getVisibleDeviceList().hashCode();
    hash = (37 * hash) + POLLING_ACTIVE_DELAY_USECS_FIELD_NUMBER;
    hash = (53 * hash) + getPollingActiveDelayUsecs();
    hash = (37 * hash) + POLLING_INACTIVE_DELAY_MSECS_FIELD_NUMBER;
    hash = (53 * hash) + getPollingInactiveDelayMsecs();
    hash = (37 * hash) + FORCE_GPU_COMPATIBLE_FIELD_NUMBER;
    hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
        getForceGpuCompatible());
    if (hasExperimental()) {
      hash = (37 * hash) + EXPERIMENTAL_FIELD_NUMBER;
      hash = (53 * hash) + getExperimental().hashCode();
    }
    hash = (29 * hash) + unknownFields.hashCode();
    memoizedHashCode = hash;
    return hash;
  }

  public static org.tensorflow.proto.framework.GPUOptions parseFrom(
      java.nio.ByteBuffer data)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data);
  }
  public static org.tensorflow.proto.framework.GPUOptions parseFrom(
      java.nio.ByteBuffer data,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data, extensionRegistry);
  }
  public static org.tensorflow.proto.framework.GPUOptions parseFrom(
      com.google.protobuf.ByteString data)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data);
  }
  public static org.tensorflow.proto.framework.GPUOptions parseFrom(
      com.google.protobuf.ByteString data,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data, extensionRegistry);
  }
  public static org.tensorflow.proto.framework.GPUOptions parseFrom(byte[] data)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data);
  }
  public static org.tensorflow.proto.framework.GPUOptions parseFrom(
      byte[] data,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data, extensionRegistry);
  }
  public static org.tensorflow.proto.framework.GPUOptions parseFrom(java.io.InputStream input)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageV3
        .parseWithIOException(PARSER, input);
  }
  public static org.tensorflow.proto.framework.GPUOptions parseFrom(
      java.io.InputStream input,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageV3
        .parseWithIOException(PARSER, input, extensionRegistry);
  }
  public static org.tensorflow.proto.framework.GPUOptions parseDelimitedFrom(java.io.InputStream input)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageV3
        .parseDelimitedWithIOException(PARSER, input);
  }
  public static org.tensorflow.proto.framework.GPUOptions parseDelimitedFrom(
      java.io.InputStream input,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageV3
        .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
  }
  public static org.tensorflow.proto.framework.GPUOptions parseFrom(
      com.google.protobuf.CodedInputStream input)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageV3
        .parseWithIOException(PARSER, input);
  }
  public static org.tensorflow.proto.framework.GPUOptions parseFrom(
      com.google.protobuf.CodedInputStream input,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageV3
        .parseWithIOException(PARSER, input, extensionRegistry);
  }

  @java.lang.Override
  public Builder newBuilderForType() { return newBuilder(); }
  public static Builder newBuilder() {
    return DEFAULT_INSTANCE.toBuilder();
  }
  public static Builder newBuilder(org.tensorflow.proto.framework.GPUOptions prototype) {
    return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
  }
  @java.lang.Override
  public Builder toBuilder() {
    return this == DEFAULT_INSTANCE
        ? new Builder() : new Builder().mergeFrom(this);
  }

  @java.lang.Override
  protected Builder newBuilderForType(
      com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
    Builder builder = new Builder(parent);
    return builder;
  }
  /**
   * Protobuf type {@code tensorflow.GPUOptions}
   */
  public static final class Builder extends
      com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
      // @@protoc_insertion_point(builder_implements:tensorflow.GPUOptions)
      org.tensorflow.proto.framework.GPUOptionsOrBuilder {
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.tensorflow.proto.framework.ConfigProtos.internal_static_tensorflow_GPUOptions_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.tensorflow.proto.framework.ConfigProtos.internal_static_tensorflow_GPUOptions_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.tensorflow.proto.framework.GPUOptions.class, org.tensorflow.proto.framework.GPUOptions.Builder.class);
    }

    // Construct using org.tensorflow.proto.framework.GPUOptions.newBuilder()
    private Builder() {
      maybeForceBuilderInitialization();
    }

    private Builder(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      super(parent);
      maybeForceBuilderInitialization();
    }
    private void maybeForceBuilderInitialization() {
      if (com.google.protobuf.GeneratedMessageV3
              .alwaysUseFieldBuilders) {
      }
    }
    @java.lang.Override
    public Builder clear() {
      super.clear();
      perProcessGpuMemoryFraction_ = 0D;

      allowGrowth_ = false;

      allocatorType_ = "";

      deferredDeletionBytes_ = 0L;

      visibleDeviceList_ = "";

      pollingActiveDelayUsecs_ = 0;

      pollingInactiveDelayMsecs_ = 0;

      forceGpuCompatible_ = false;

      if (experimentalBuilder_ == null) {
        experimental_ = null;
      } else {
        experimental_ = null;
        experimentalBuilder_ = null;
      }
      return this;
    }

    @java.lang.Override
    public com.google.protobuf.Descriptors.Descriptor
        getDescriptorForType() {
      return org.tensorflow.proto.framework.ConfigProtos.internal_static_tensorflow_GPUOptions_descriptor;
    }

    @java.lang.Override
    public org.tensorflow.proto.framework.GPUOptions getDefaultInstanceForType() {
      return org.tensorflow.proto.framework.GPUOptions.getDefaultInstance();
    }

    @java.lang.Override
    public org.tensorflow.proto.framework.GPUOptions build() {
      org.tensorflow.proto.framework.GPUOptions result = buildPartial();
      if (!result.isInitialized()) {
        throw newUninitializedMessageException(result);
      }
      return result;
    }

    @java.lang.Override
    public org.tensorflow.proto.framework.GPUOptions buildPartial() {
      org.tensorflow.proto.framework.GPUOptions result = new org.tensorflow.proto.framework.GPUOptions(this);
      result.perProcessGpuMemoryFraction_ = perProcessGpuMemoryFraction_;
      result.allowGrowth_ = allowGrowth_;
      result.allocatorType_ = allocatorType_;
      result.deferredDeletionBytes_ = deferredDeletionBytes_;
      result.visibleDeviceList_ = visibleDeviceList_;
      result.pollingActiveDelayUsecs_ = pollingActiveDelayUsecs_;
      result.pollingInactiveDelayMsecs_ = pollingInactiveDelayMsecs_;
      result.forceGpuCompatible_ = forceGpuCompatible_;
      if (experimentalBuilder_ == null) {
        result.experimental_ = experimental_;
      } else {
        result.experimental_ = experimentalBuilder_.build();
      }
      onBuilt();
      return result;
    }

    @java.lang.Override
    public Builder clone() {
      return super.clone();
    }
    @java.lang.Override
    public Builder setField(
        com.google.protobuf.Descriptors.FieldDescriptor field,
        java.lang.Object value) {
      return super.setField(field, value);
    }
    @java.lang.Override
    public Builder clearField(
        com.google.protobuf.Descriptors.FieldDescriptor field) {
      return super.clearField(field);
    }
    @java.lang.Override
    public Builder clearOneof(
        com.google.protobuf.Descriptors.OneofDescriptor oneof) {
      return super.clearOneof(oneof);
    }
    @java.lang.Override
    public Builder setRepeatedField(
        com.google.protobuf.Descriptors.FieldDescriptor field,
        int index, java.lang.Object value) {
      return super.setRepeatedField(field, index, value);
    }
    @java.lang.Override
    public Builder addRepeatedField(
        com.google.protobuf.Descriptors.FieldDescriptor field,
        java.lang.Object value) {
      return super.addRepeatedField(field, value);
    }
    @java.lang.Override
    public Builder mergeFrom(com.google.protobuf.Message other) {
      if (other instanceof org.tensorflow.proto.framework.GPUOptions) {
        return mergeFrom((org.tensorflow.proto.framework.GPUOptions)other);
      } else {
        super.mergeFrom(other);
        return this;
      }
    }

    public Builder mergeFrom(org.tensorflow.proto.framework.GPUOptions other) {
      if (other == org.tensorflow.proto.framework.GPUOptions.getDefaultInstance()) return this;
      if (other.getPerProcessGpuMemoryFraction() != 0D) {
        setPerProcessGpuMemoryFraction(other.getPerProcessGpuMemoryFraction());
      }
      if (other.getAllowGrowth() != false) {
        setAllowGrowth(other.getAllowGrowth());
      }
      if (!other.getAllocatorType().isEmpty()) {
        allocatorType_ = other.allocatorType_;
        onChanged();
      }
      if (other.getDeferredDeletionBytes() != 0L) {
        setDeferredDeletionBytes(other.getDeferredDeletionBytes());
      }
      if (!other.getVisibleDeviceList().isEmpty()) {
        visibleDeviceList_ = other.visibleDeviceList_;
        onChanged();
      }
      if (other.getPollingActiveDelayUsecs() != 0) {
        setPollingActiveDelayUsecs(other.getPollingActiveDelayUsecs());
      }
      if (other.getPollingInactiveDelayMsecs() != 0) {
        setPollingInactiveDelayMsecs(other.getPollingInactiveDelayMsecs());
      }
      if (other.getForceGpuCompatible() != false) {
        setForceGpuCompatible(other.getForceGpuCompatible());
      }
      if (other.hasExperimental()) {
        mergeExperimental(other.getExperimental());
      }
      this.mergeUnknownFields(other.unknownFields);
      onChanged();
      return this;
    }

    @java.lang.Override
    public final boolean isInitialized() {
      return true;
    }

    @java.lang.Override
    public Builder mergeFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      org.tensorflow.proto.framework.GPUOptions parsedMessage = null;
      try {
        parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        parsedMessage = (org.tensorflow.proto.framework.GPUOptions) e.getUnfinishedMessage();
        throw e.unwrapIOException();
      } finally {
        if (parsedMessage != null) {
          mergeFrom(parsedMessage);
        }
      }
      return this;
    }

    private double perProcessGpuMemoryFraction_ ;
    /**
     * <pre>
     * Fraction of the available GPU memory to allocate for each process.
     * 1 means to allocate all of the GPU memory, 0.5 means the process
     * allocates up to ~50% of the available GPU memory.
     * GPU memory is pre-allocated unless the allow_growth option is enabled.
     * If greater than 1.0, uses CUDA unified memory to potentially oversubscribe
     * the amount of memory available on the GPU device by using host memory as a
     * swap space. Accessing memory not available on the device will be
     * significantly slower as that would require memory transfer between the host
     * and the device. Options to reduce the memory requirement should be
     * considered before enabling this option as this may come with a negative
     * performance impact. Oversubscription using the unified memory requires
     * Pascal class or newer GPUs and it is currently only supported on the Linux
     * operating system. See
     * https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-requirements
     * for the detailed requirements.
     * </pre>
     *
     * <code>double per_process_gpu_memory_fraction = 1;</code>
     */
    public double getPerProcessGpuMemoryFraction() {
      return perProcessGpuMemoryFraction_;
    }
    /**
     * <pre>
     * Fraction of the available GPU memory to allocate for each process.
     * 1 means to allocate all of the GPU memory, 0.5 means the process
     * allocates up to ~50% of the available GPU memory.
     * GPU memory is pre-allocated unless the allow_growth option is enabled.
     * If greater than 1.0, uses CUDA unified memory to potentially oversubscribe
     * the amount of memory available on the GPU device by using host memory as a
     * swap space. Accessing memory not available on the device will be
     * significantly slower as that would require memory transfer between the host
     * and the device. Options to reduce the memory requirement should be
     * considered before enabling this option as this may come with a negative
     * performance impact. Oversubscription using the unified memory requires
     * Pascal class or newer GPUs and it is currently only supported on the Linux
     * operating system. See
     * https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-requirements
     * for the detailed requirements.
     * </pre>
     *
     * <code>double per_process_gpu_memory_fraction = 1;</code>
     */
    public Builder setPerProcessGpuMemoryFraction(double value) {
      
      perProcessGpuMemoryFraction_ = value;
      onChanged();
      return this;
    }
    /**
     * <pre>
     * Fraction of the available GPU memory to allocate for each process.
     * 1 means to allocate all of the GPU memory, 0.5 means the process
     * allocates up to ~50% of the available GPU memory.
     * GPU memory is pre-allocated unless the allow_growth option is enabled.
     * If greater than 1.0, uses CUDA unified memory to potentially oversubscribe
     * the amount of memory available on the GPU device by using host memory as a
     * swap space. Accessing memory not available on the device will be
     * significantly slower as that would require memory transfer between the host
     * and the device. Options to reduce the memory requirement should be
     * considered before enabling this option as this may come with a negative
     * performance impact. Oversubscription using the unified memory requires
     * Pascal class or newer GPUs and it is currently only supported on the Linux
     * operating system. See
     * https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-requirements
     * for the detailed requirements.
     * </pre>
     *
     * <code>double per_process_gpu_memory_fraction = 1;</code>
     */
    public Builder clearPerProcessGpuMemoryFraction() {
      
      perProcessGpuMemoryFraction_ = 0D;
      onChanged();
      return this;
    }

    private boolean allowGrowth_ ;
    /**
     * <pre>
     * If true, the allocator does not pre-allocate the entire specified
     * GPU memory region, instead starting small and growing as needed.
     * </pre>
     *
     * <code>bool allow_growth = 4;</code>
     */
    public boolean getAllowGrowth() {
      return allowGrowth_;
    }
    /**
     * <pre>
     * If true, the allocator does not pre-allocate the entire specified
     * GPU memory region, instead starting small and growing as needed.
     * </pre>
     *
     * <code>bool allow_growth = 4;</code>
     */
    public Builder setAllowGrowth(boolean value) {
      
      allowGrowth_ = value;
      onChanged();
      return this;
    }
    /**
     * <pre>
     * If true, the allocator does not pre-allocate the entire specified
     * GPU memory region, instead starting small and growing as needed.
     * </pre>
     *
     * <code>bool allow_growth = 4;</code>
     */
    public Builder clearAllowGrowth() {
      
      allowGrowth_ = false;
      onChanged();
      return this;
    }

    private java.lang.Object allocatorType_ = "";
    /**
     * <pre>
     * The type of GPU allocation strategy to use.
     * Allowed values:
     * "": The empty string (default) uses a system-chosen default
     *     which may change over time.
     * "BFC": A "Best-fit with coalescing" algorithm, simplified from a
     *        version of dlmalloc.
     * </pre>
     *
     * <code>string allocator_type = 2;</code>
     */
    public java.lang.String getAllocatorType() {
      java.lang.Object ref = allocatorType_;
      if (!(ref instanceof java.lang.String)) {
        com.google.protobuf.ByteString bs =
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        allocatorType_ = s;
        return s;
      } else {
        return (java.lang.String) ref;
      }
    }
    /**
     * <pre>
     * The type of GPU allocation strategy to use.
     * Allowed values:
     * "": The empty string (default) uses a system-chosen default
     *     which may change over time.
     * "BFC": A "Best-fit with coalescing" algorithm, simplified from a
     *        version of dlmalloc.
     * </pre>
     *
     * <code>string allocator_type = 2;</code>
     */
    public com.google.protobuf.ByteString
        getAllocatorTypeBytes() {
      java.lang.Object ref = allocatorType_;
      if (ref instanceof String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        allocatorType_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }
    /**
     * <pre>
     * The type of GPU allocation strategy to use.
     * Allowed values:
     * "": The empty string (default) uses a system-chosen default
     *     which may change over time.
     * "BFC": A "Best-fit with coalescing" algorithm, simplified from a
     *        version of dlmalloc.
     * </pre>
     *
     * <code>string allocator_type = 2;</code>
     */
    public Builder setAllocatorType(
        java.lang.String value) {
      if (value == null) {
    throw new NullPointerException();
  }
  
      allocatorType_ = value;
      onChanged();
      return this;
    }
    /**
     * <pre>
     * The type of GPU allocation strategy to use.
     * Allowed values:
     * "": The empty string (default) uses a system-chosen default
     *     which may change over time.
     * "BFC": A "Best-fit with coalescing" algorithm, simplified from a
     *        version of dlmalloc.
     * </pre>
     *
     * <code>string allocator_type = 2;</code>
     */
    public Builder clearAllocatorType() {
      
      allocatorType_ = getDefaultInstance().getAllocatorType();
      onChanged();
      return this;
    }
    /**
     * <pre>
     * The type of GPU allocation strategy to use.
     * Allowed values:
     * "": The empty string (default) uses a system-chosen default
     *     which may change over time.
     * "BFC": A "Best-fit with coalescing" algorithm, simplified from a
     *        version of dlmalloc.
     * </pre>
     *
     * <code>string allocator_type = 2;</code>
     */
    public Builder setAllocatorTypeBytes(
        com.google.protobuf.ByteString value) {
      if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
      
      allocatorType_ = value;
      onChanged();
      return this;
    }

    private long deferredDeletionBytes_ ;
    /**
     * <pre>
     * Delay deletion of up to this many bytes to reduce the number of
     * interactions with gpu driver code.  If 0, the system chooses
     * a reasonable default (several MBs).
     * </pre>
     *
     * <code>int64 deferred_deletion_bytes = 3;</code>
     */
    public long getDeferredDeletionBytes() {
      return deferredDeletionBytes_;
    }
    /**
     * <pre>
     * Delay deletion of up to this many bytes to reduce the number of
     * interactions with gpu driver code.  If 0, the system chooses
     * a reasonable default (several MBs).
     * </pre>
     *
     * <code>int64 deferred_deletion_bytes = 3;</code>
     */
    public Builder setDeferredDeletionBytes(long value) {
      
      deferredDeletionBytes_ = value;
      onChanged();
      return this;
    }
    /**
     * <pre>
     * Delay deletion of up to this many bytes to reduce the number of
     * interactions with gpu driver code.  If 0, the system chooses
     * a reasonable default (several MBs).
     * </pre>
     *
     * <code>int64 deferred_deletion_bytes = 3;</code>
     */
    public Builder clearDeferredDeletionBytes() {
      
      deferredDeletionBytes_ = 0L;
      onChanged();
      return this;
    }

    private java.lang.Object visibleDeviceList_ = "";
    /**
     * <pre>
     * A comma-separated list of GPU ids that determines the 'visible'
     * to 'virtual' mapping of GPU devices.  For example, if TensorFlow
     * can see 8 GPU devices in the process, and one wanted to map
     * visible GPU devices 5 and 3 as "/device:GPU:0", and "/device:GPU:1",
     * then one would specify this field as "5,3".  This field is similar in
     * spirit to the CUDA_VISIBLE_DEVICES environment variable, except
     * it applies to the visible GPU devices in the process.
     * NOTE:
     * 1. The GPU driver provides the process with the visible GPUs
     *    in an order which is not guaranteed to have any correlation to
     *    the *physical* GPU id in the machine.  This field is used for
     *    remapping "visible" to "virtual", which means this operates only
     *    after the process starts.  Users are required to use vendor
     *    specific mechanisms (e.g., CUDA_VISIBLE_DEVICES) to control the
     *    physical to visible device mapping prior to invoking TensorFlow.
     * 2. In the code, the ids in this list are also called "platform GPU id"s,
     *    and the 'virtual' ids of GPU devices (i.e. the ids in the device
     *    name "/device:GPU:&lt;id&gt;") are also called "TF GPU id"s. Please
     *    refer to third_party/tensorflow/core/common_runtime/gpu/gpu_id.h
     *    for more information.
     * </pre>
     *
     * <code>string visible_device_list = 5;</code>
     */
    public java.lang.String getVisibleDeviceList() {
      java.lang.Object ref = visibleDeviceList_;
      if (!(ref instanceof java.lang.String)) {
        com.google.protobuf.ByteString bs =
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        visibleDeviceList_ = s;
        return s;
      } else {
        return (java.lang.String) ref;
      }
    }
    /**
     * <pre>
     * A comma-separated list of GPU ids that determines the 'visible'
     * to 'virtual' mapping of GPU devices.  For example, if TensorFlow
     * can see 8 GPU devices in the process, and one wanted to map
     * visible GPU devices 5 and 3 as "/device:GPU:0", and "/device:GPU:1",
     * then one would specify this field as "5,3".  This field is similar in
     * spirit to the CUDA_VISIBLE_DEVICES environment variable, except
     * it applies to the visible GPU devices in the process.
     * NOTE:
     * 1. The GPU driver provides the process with the visible GPUs
     *    in an order which is not guaranteed to have any correlation to
     *    the *physical* GPU id in the machine.  This field is used for
     *    remapping "visible" to "virtual", which means this operates only
     *    after the process starts.  Users are required to use vendor
     *    specific mechanisms (e.g., CUDA_VISIBLE_DEVICES) to control the
     *    physical to visible device mapping prior to invoking TensorFlow.
     * 2. In the code, the ids in this list are also called "platform GPU id"s,
     *    and the 'virtual' ids of GPU devices (i.e. the ids in the device
     *    name "/device:GPU:&lt;id&gt;") are also called "TF GPU id"s. Please
     *    refer to third_party/tensorflow/core/common_runtime/gpu/gpu_id.h
     *    for more information.
     * </pre>
     *
     * <code>string visible_device_list = 5;</code>
     */
    public com.google.protobuf.ByteString
        getVisibleDeviceListBytes() {
      java.lang.Object ref = visibleDeviceList_;
      if (ref instanceof String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        visibleDeviceList_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }
    /**
     * <pre>
     * A comma-separated list of GPU ids that determines the 'visible'
     * to 'virtual' mapping of GPU devices.  For example, if TensorFlow
     * can see 8 GPU devices in the process, and one wanted to map
     * visible GPU devices 5 and 3 as "/device:GPU:0", and "/device:GPU:1",
     * then one would specify this field as "5,3".  This field is similar in
     * spirit to the CUDA_VISIBLE_DEVICES environment variable, except
     * it applies to the visible GPU devices in the process.
     * NOTE:
     * 1. The GPU driver provides the process with the visible GPUs
     *    in an order which is not guaranteed to have any correlation to
     *    the *physical* GPU id in the machine.  This field is used for
     *    remapping "visible" to "virtual", which means this operates only
     *    after the process starts.  Users are required to use vendor
     *    specific mechanisms (e.g., CUDA_VISIBLE_DEVICES) to control the
     *    physical to visible device mapping prior to invoking TensorFlow.
     * 2. In the code, the ids in this list are also called "platform GPU id"s,
     *    and the 'virtual' ids of GPU devices (i.e. the ids in the device
     *    name "/device:GPU:&lt;id&gt;") are also called "TF GPU id"s. Please
     *    refer to third_party/tensorflow/core/common_runtime/gpu/gpu_id.h
     *    for more information.
     * </pre>
     *
     * <code>string visible_device_list = 5;</code>
     */
    public Builder setVisibleDeviceList(
        java.lang.String value) {
      if (value == null) {
    throw new NullPointerException();
  }
  
      visibleDeviceList_ = value;
      onChanged();
      return this;
    }
    /**
     * <pre>
     * A comma-separated list of GPU ids that determines the 'visible'
     * to 'virtual' mapping of GPU devices.  For example, if TensorFlow
     * can see 8 GPU devices in the process, and one wanted to map
     * visible GPU devices 5 and 3 as "/device:GPU:0", and "/device:GPU:1",
     * then one would specify this field as "5,3".  This field is similar in
     * spirit to the CUDA_VISIBLE_DEVICES environment variable, except
     * it applies to the visible GPU devices in the process.
     * NOTE:
     * 1. The GPU driver provides the process with the visible GPUs
     *    in an order which is not guaranteed to have any correlation to
     *    the *physical* GPU id in the machine.  This field is used for
     *    remapping "visible" to "virtual", which means this operates only
     *    after the process starts.  Users are required to use vendor
     *    specific mechanisms (e.g., CUDA_VISIBLE_DEVICES) to control the
     *    physical to visible device mapping prior to invoking TensorFlow.
     * 2. In the code, the ids in this list are also called "platform GPU id"s,
     *    and the 'virtual' ids of GPU devices (i.e. the ids in the device
     *    name "/device:GPU:&lt;id&gt;") are also called "TF GPU id"s. Please
     *    refer to third_party/tensorflow/core/common_runtime/gpu/gpu_id.h
     *    for more information.
     * </pre>
     *
     * <code>string visible_device_list = 5;</code>
     */
    public Builder clearVisibleDeviceList() {
      
      visibleDeviceList_ = getDefaultInstance().getVisibleDeviceList();
      onChanged();
      return this;
    }
    /**
     * <pre>
     * A comma-separated list of GPU ids that determines the 'visible'
     * to 'virtual' mapping of GPU devices.  For example, if TensorFlow
     * can see 8 GPU devices in the process, and one wanted to map
     * visible GPU devices 5 and 3 as "/device:GPU:0", and "/device:GPU:1",
     * then one would specify this field as "5,3".  This field is similar in
     * spirit to the CUDA_VISIBLE_DEVICES environment variable, except
     * it applies to the visible GPU devices in the process.
     * NOTE:
     * 1. The GPU driver provides the process with the visible GPUs
     *    in an order which is not guaranteed to have any correlation to
     *    the *physical* GPU id in the machine.  This field is used for
     *    remapping "visible" to "virtual", which means this operates only
     *    after the process starts.  Users are required to use vendor
     *    specific mechanisms (e.g., CUDA_VISIBLE_DEVICES) to control the
     *    physical to visible device mapping prior to invoking TensorFlow.
     * 2. In the code, the ids in this list are also called "platform GPU id"s,
     *    and the 'virtual' ids of GPU devices (i.e. the ids in the device
     *    name "/device:GPU:&lt;id&gt;") are also called "TF GPU id"s. Please
     *    refer to third_party/tensorflow/core/common_runtime/gpu/gpu_id.h
     *    for more information.
     * </pre>
     *
     * <code>string visible_device_list = 5;</code>
     */
    public Builder setVisibleDeviceListBytes(
        com.google.protobuf.ByteString value) {
      if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
      
      visibleDeviceList_ = value;
      onChanged();
      return this;
    }

    private int pollingActiveDelayUsecs_ ;
    /**
     * <pre>
     * In the event polling loop sleep this many microseconds between
     * PollEvents calls, when the queue is not empty.  If value is not
     * set or set to 0, gets set to a non-zero default.
     * </pre>
     *
     * <code>int32 polling_active_delay_usecs = 6;</code>
     */
    public int getPollingActiveDelayUsecs() {
      return pollingActiveDelayUsecs_;
    }
    /**
     * <pre>
     * In the event polling loop sleep this many microseconds between
     * PollEvents calls, when the queue is not empty.  If value is not
     * set or set to 0, gets set to a non-zero default.
     * </pre>
     *
     * <code>int32 polling_active_delay_usecs = 6;</code>
     */
    public Builder setPollingActiveDelayUsecs(int value) {
      
      pollingActiveDelayUsecs_ = value;
      onChanged();
      return this;
    }
    /**
     * <pre>
     * In the event polling loop sleep this many microseconds between
     * PollEvents calls, when the queue is not empty.  If value is not
     * set or set to 0, gets set to a non-zero default.
     * </pre>
     *
     * <code>int32 polling_active_delay_usecs = 6;</code>
     */
    public Builder clearPollingActiveDelayUsecs() {
      
      pollingActiveDelayUsecs_ = 0;
      onChanged();
      return this;
    }

    private int pollingInactiveDelayMsecs_ ;
    /**
     * <pre>
     * This field is deprecated and ignored.
     * </pre>
     *
     * <code>int32 polling_inactive_delay_msecs = 7;</code>
     */
    public int getPollingInactiveDelayMsecs() {
      return pollingInactiveDelayMsecs_;
    }
    /**
     * <pre>
     * This field is deprecated and ignored.
     * </pre>
     *
     * <code>int32 polling_inactive_delay_msecs = 7;</code>
     */
    public Builder setPollingInactiveDelayMsecs(int value) {
      
      pollingInactiveDelayMsecs_ = value;
      onChanged();
      return this;
    }
    /**
     * <pre>
     * This field is deprecated and ignored.
     * </pre>
     *
     * <code>int32 polling_inactive_delay_msecs = 7;</code>
     */
    public Builder clearPollingInactiveDelayMsecs() {
      
      pollingInactiveDelayMsecs_ = 0;
      onChanged();
      return this;
    }

    private boolean forceGpuCompatible_ ;
    /**
     * <pre>
     * Force all tensors to be gpu_compatible. On a GPU-enabled TensorFlow,
     * enabling this option forces all CPU tensors to be allocated with Cuda
     * pinned memory. Normally, TensorFlow will infer which tensors should be
     * allocated as the pinned memory. But in case where the inference is
     * incomplete, this option can significantly speed up the cross-device memory
     * copy performance as long as it fits the memory.
     * Note that this option is not something that should be
     * enabled by default for unknown or very large models, since all Cuda pinned
     * memory is unpageable, having too much pinned memory might negatively impact
     * the overall host system performance.
     * </pre>
     *
     * <code>bool force_gpu_compatible = 8;</code>
     */
    public boolean getForceGpuCompatible() {
      return forceGpuCompatible_;
    }
    /**
     * <pre>
     * Force all tensors to be gpu_compatible. On a GPU-enabled TensorFlow,
     * enabling this option forces all CPU tensors to be allocated with Cuda
     * pinned memory. Normally, TensorFlow will infer which tensors should be
     * allocated as the pinned memory. But in case where the inference is
     * incomplete, this option can significantly speed up the cross-device memory
     * copy performance as long as it fits the memory.
     * Note that this option is not something that should be
     * enabled by default for unknown or very large models, since all Cuda pinned
     * memory is unpageable, having too much pinned memory might negatively impact
     * the overall host system performance.
     * </pre>
     *
     * <code>bool force_gpu_compatible = 8;</code>
     */
    public Builder setForceGpuCompatible(boolean value) {
      
      forceGpuCompatible_ = value;
      onChanged();
      return this;
    }
    /**
     * <pre>
     * Force all tensors to be gpu_compatible. On a GPU-enabled TensorFlow,
     * enabling this option forces all CPU tensors to be allocated with Cuda
     * pinned memory. Normally, TensorFlow will infer which tensors should be
     * allocated as the pinned memory. But in case where the inference is
     * incomplete, this option can significantly speed up the cross-device memory
     * copy performance as long as it fits the memory.
     * Note that this option is not something that should be
     * enabled by default for unknown or very large models, since all Cuda pinned
     * memory is unpageable, having too much pinned memory might negatively impact
     * the overall host system performance.
     * </pre>
     *
     * <code>bool force_gpu_compatible = 8;</code>
     */
    public Builder clearForceGpuCompatible() {
      
      forceGpuCompatible_ = false;
      onChanged();
      return this;
    }

    private org.tensorflow.proto.framework.GPUOptions.Experimental experimental_;
    private com.google.protobuf.SingleFieldBuilderV3<
        org.tensorflow.proto.framework.GPUOptions.Experimental, org.tensorflow.proto.framework.GPUOptions.Experimental.Builder, org.tensorflow.proto.framework.GPUOptions.ExperimentalOrBuilder> experimentalBuilder_;
    /**
     * <pre>
     * Everything inside experimental is subject to change and is not subject
     * to API stability guarantees in
     * https://www.tensorflow.org/guide/version_compat.
     * </pre>
     *
     * <code>.tensorflow.GPUOptions.Experimental experimental = 9;</code>
     */
    public boolean hasExperimental() {
      return experimentalBuilder_ != null || experimental_ != null;
    }
    /**
     * <pre>
     * Everything inside experimental is subject to change and is not subject
     * to API stability guarantees in
     * https://www.tensorflow.org/guide/version_compat.
     * </pre>
     *
     * <code>.tensorflow.GPUOptions.Experimental experimental = 9;</code>
     */
    public org.tensorflow.proto.framework.GPUOptions.Experimental getExperimental() {
      if (experimentalBuilder_ == null) {
        return experimental_ == null ? org.tensorflow.proto.framework.GPUOptions.Experimental.getDefaultInstance() : experimental_;
      } else {
        return experimentalBuilder_.getMessage();
      }
    }
    /**
     * <pre>
     * Everything inside experimental is subject to change and is not subject
     * to API stability guarantees in
     * https://www.tensorflow.org/guide/version_compat.
     * </pre>
     *
     * <code>.tensorflow.GPUOptions.Experimental experimental = 9;</code>
     */
    public Builder setExperimental(org.tensorflow.proto.framework.GPUOptions.Experimental value) {
      if (experimentalBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        experimental_ = value;
        onChanged();
      } else {
        experimentalBuilder_.setMessage(value);
      }

      return this;
    }
    /**
     * <pre>
     * Everything inside experimental is subject to change and is not subject
     * to API stability guarantees in
     * https://www.tensorflow.org/guide/version_compat.
     * </pre>
     *
     * <code>.tensorflow.GPUOptions.Experimental experimental = 9;</code>
     */
    public Builder setExperimental(
        org.tensorflow.proto.framework.GPUOptions.Experimental.Builder builderForValue) {
      if (experimentalBuilder_ == null) {
        experimental_ = builderForValue.build();
        onChanged();
      } else {
        experimentalBuilder_.setMessage(builderForValue.build());
      }

      return this;
    }
    /**
     * <pre>
     * Everything inside experimental is subject to change and is not subject
     * to API stability guarantees in
     * https://www.tensorflow.org/guide/version_compat.
     * </pre>
     *
     * <code>.tensorflow.GPUOptions.Experimental experimental = 9;</code>
     */
    public Builder mergeExperimental(org.tensorflow.proto.framework.GPUOptions.Experimental value) {
      if (experimentalBuilder_ == null) {
        if (experimental_ != null) {
          experimental_ =
            org.tensorflow.proto.framework.GPUOptions.Experimental.newBuilder(experimental_).mergeFrom(value).buildPartial();
        } else {
          experimental_ = value;
        }
        onChanged();
      } else {
        experimentalBuilder_.mergeFrom(value);
      }

      return this;
    }
    /**
     * <pre>
     * Everything inside experimental is subject to change and is not subject
     * to API stability guarantees in
     * https://www.tensorflow.org/guide/version_compat.
     * </pre>
     *
     * <code>.tensorflow.GPUOptions.Experimental experimental = 9;</code>
     */
    public Builder clearExperimental() {
      if (experimentalBuilder_ == null) {
        experimental_ = null;
        onChanged();
      } else {
        experimental_ = null;
        experimentalBuilder_ = null;
      }

      return this;
    }
    /**
     * <pre>
     * Everything inside experimental is subject to change and is not subject
     * to API stability guarantees in
     * https://www.tensorflow.org/guide/version_compat.
     * </pre>
     *
     * <code>.tensorflow.GPUOptions.Experimental experimental = 9;</code>
     */
    public org.tensorflow.proto.framework.GPUOptions.Experimental.Builder getExperimentalBuilder() {
      
      onChanged();
      return getExperimentalFieldBuilder().getBuilder();
    }
    /**
     * <pre>
     * Everything inside experimental is subject to change and is not subject
     * to API stability guarantees in
     * https://www.tensorflow.org/guide/version_compat.
     * </pre>
     *
     * <code>.tensorflow.GPUOptions.Experimental experimental = 9;</code>
     */
    public org.tensorflow.proto.framework.GPUOptions.ExperimentalOrBuilder getExperimentalOrBuilder() {
      if (experimentalBuilder_ != null) {
        return experimentalBuilder_.getMessageOrBuilder();
      } else {
        return experimental_ == null ?
            org.tensorflow.proto.framework.GPUOptions.Experimental.getDefaultInstance() : experimental_;
      }
    }
    /**
     * <pre>
     * Everything inside experimental is subject to change and is not subject
     * to API stability guarantees in
     * https://www.tensorflow.org/guide/version_compat.
     * </pre>
     *
     * <code>.tensorflow.GPUOptions.Experimental experimental = 9;</code>
     */
    private com.google.protobuf.SingleFieldBuilderV3<
        org.tensorflow.proto.framework.GPUOptions.Experimental, org.tensorflow.proto.framework.GPUOptions.Experimental.Builder, org.tensorflow.proto.framework.GPUOptions.ExperimentalOrBuilder> 
        getExperimentalFieldBuilder() {
      if (experimentalBuilder_ == null) {
        experimentalBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
            org.tensorflow.proto.framework.GPUOptions.Experimental, org.tensorflow.proto.framework.GPUOptions.Experimental.Builder, org.tensorflow.proto.framework.GPUOptions.ExperimentalOrBuilder>(
                getExperimental(),
                getParentForChildren(),
                isClean());
        experimental_ = null;
      }
      return experimentalBuilder_;
    }
    @java.lang.Override
    public final Builder setUnknownFields(
        final com.google.protobuf.UnknownFieldSet unknownFields) {
      return super.setUnknownFields(unknownFields);
    }

    @java.lang.Override
    public final Builder mergeUnknownFields(
        final com.google.protobuf.UnknownFieldSet unknownFields) {
      return super.mergeUnknownFields(unknownFields);
    }


    // @@protoc_insertion_point(builder_scope:tensorflow.GPUOptions)
  }

  // @@protoc_insertion_point(class_scope:tensorflow.GPUOptions)
  private static final org.tensorflow.proto.framework.GPUOptions DEFAULT_INSTANCE;
  static {
    DEFAULT_INSTANCE = new org.tensorflow.proto.framework.GPUOptions();
  }

  public static org.tensorflow.proto.framework.GPUOptions getDefaultInstance() {
    return DEFAULT_INSTANCE;
  }

  private static final com.google.protobuf.Parser<GPUOptions>
      PARSER = new com.google.protobuf.AbstractParser<GPUOptions>() {
    @java.lang.Override
    public GPUOptions parsePartialFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return new GPUOptions(input, extensionRegistry);
    }
  };

  public static com.google.protobuf.Parser<GPUOptions> parser() {
    return PARSER;
  }

  @java.lang.Override
  public com.google.protobuf.Parser<GPUOptions> getParserForType() {
    return PARSER;
  }

  @java.lang.Override
  public org.tensorflow.proto.framework.GPUOptions getDefaultInstanceForType() {
    return DEFAULT_INSTANCE;
  }

}

