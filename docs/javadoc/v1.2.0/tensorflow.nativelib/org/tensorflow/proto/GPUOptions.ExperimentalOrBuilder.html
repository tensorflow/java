<!DOCTYPE HTML>
<html lang>
<head>
<!-- Generated by javadoc (25) on Fri Nov 14 15:24:55 EST 2025 -->
<title>GPUOptions.ExperimentalOrBuilder (TensorFlow Java Parent 1.2.0-SNAPSHOT API)</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="dc.created" content="2025-11-14">
<meta name="description" content="declaration: module: tensorflow.nativelib, package: org.tensorflow.proto, class: GPUOptions, interface: ExperimentalOrBuilder">
<meta name="generator" content="javadoc/ClassWriter">
<link rel="stylesheet" type="text/css" href="../../../../resource-files/jquery-ui.min.css">
<link rel="stylesheet" type="text/css" href="../../../../resource-files/stylesheet.css">
<script type="text/javascript" src="../../../../script-files/script.js"></script>
<script type="text/javascript" src="../../../../script-files/jquery-3.7.1.min.js"></script>
<script type="text/javascript" src="../../../../script-files/jquery-ui.min.js"></script>
</head>
<body class="class-declaration-page">
<script type="text/javascript">const pathtoroot = "../../../../";
loadScripts(document, 'script');</script>
<noscript>
<div>JavaScript is disabled on your browser.</div>
</noscript>
<header role="banner">
<nav role="navigation">
<!-- ========= START OF TOP NAVBAR ======= -->
<div class="top-nav" id="navbar-top">
<div class="nav-content">
<div class="nav-menu-button"><button id="navbar-toggle-button" aria-controls="navbar-top" aria-expanded="false" aria-label="Toggle navigation links"><span class="nav-bar-toggle-icon">&nbsp;</span><span class="nav-bar-toggle-icon">&nbsp;</span><span class="nav-bar-toggle-icon">&nbsp;</span></button></div>
<div class="skip-nav"><a href="#skip-navbar-top" title="Skip navigation links">Skip navigation links</a></div>
<ul id="navbar-top-firstrow" class="nav-list" title="Navigation">
<li><a href="../../../../index.html">Overview</a></li>
<li class="nav-bar-cell1-rev">Class</li>
<li><a href="class-use/GPUOptions.ExperimentalOrBuilder.html">Use</a></li>
<li><a href="package-tree.html">Tree</a></li>
<li><a href="../../../../deprecated-list.html">Deprecated</a></li>
<li><a href="../../../../index-all.html">Index</a></li>
<li><a href="../../../../search.html">Search</a></li>
<li><a href="../../../../help-doc.html#class">Help</a></li>
</ul>
</div>
</div>
<div class="sub-nav">
<div class="nav-content">
<ol class="sub-nav-list">
<li><a href="../../../module-summary.html">tensorflow.nativelib</a></li>
<li><a href="package-summary.html">org.tensorflow.proto</a></li>
<li><a href="GPUOptions.html">GPUOptions</a></li>
<li><a href="GPUOptions.ExperimentalOrBuilder.html" class="current-selection">ExperimentalOrBuilder</a></li>
</ol>
<div class="nav-list-search"><input type="text" id="search-input" disabled placeholder="Search documentation (type /)" aria-label="Search in documentation" autocomplete="off" spellcheck="false"><input type="reset" id="reset-search" disabled value="Reset"></div>
</div>
</div>
<!-- ========= END OF TOP NAVBAR ========= -->
<span class="skip-nav" id="skip-navbar-top"></span></nav>
</header>
<div class="main-grid">
<nav role="navigation" class="toc" aria-label="Table of contents">
<div class="toc-header">Contents&nbsp;<input type="text" class="filter-input" disabled placeholder="Filter contents (type .)" aria-label="Filter table of contents" autocomplete="off" spellcheck="false"><input type="reset" class="reset-filter" disabled tabindex="-1" value="Reset"></div>
<ol class="toc-list" tabindex="-1">
<li><a href="#" tabindex="0">Description</a></li>
<li><a href="#method-summary" tabindex="0">Method Summary</a></li>
<li><a href="#method-detail" tabindex="0">Method Details</a>
<ol class="toc-list">
<li><a href="#getVirtualDevicesList()" tabindex="0">getVirtualDevicesList()</a></li>
<li><a href="#getVirtualDevices(int)" tabindex="0">getVirtualDevices(int)</a></li>
<li><a href="#getVirtualDevicesCount()" tabindex="0">getVirtualDevicesCount()</a></li>
<li><a href="#getVirtualDevicesOrBuilderList()" tabindex="0">getVirtualDevicesOrBuilderList()</a></li>
<li><a href="#getVirtualDevicesOrBuilder(int)" tabindex="0">getVirtualDevicesOrBuilder(int)</a></li>
<li><a href="#getNumVirtualDevicesPerGpu()" tabindex="0">getNumVirtualDevicesPerGpu()</a></li>
<li><a href="#getUseUnifiedMemory()" tabindex="0">getUseUnifiedMemory()</a></li>
<li><a href="#getNumDevToDevCopyStreams()" tabindex="0">getNumDevToDevCopyStreams()</a></li>
<li><a href="#getCollectiveRingOrder()" tabindex="0">getCollectiveRingOrder()</a></li>
<li><a href="#getCollectiveRingOrderBytes()" tabindex="0">getCollectiveRingOrderBytes()</a></li>
<li><a href="#getTimestampedAllocator()" tabindex="0">getTimestampedAllocator()</a></li>
<li><a href="#getKernelTrackerMaxInterval()" tabindex="0">getKernelTrackerMaxInterval()</a></li>
<li><a href="#getKernelTrackerMaxBytes()" tabindex="0">getKernelTrackerMaxBytes()</a></li>
<li><a href="#getKernelTrackerMaxPending()" tabindex="0">getKernelTrackerMaxPending()</a></li>
<li><a href="#getInternalFragmentationFraction()" tabindex="0">getInternalFragmentationFraction()</a></li>
<li><a href="#getUseCudaMallocAsync()" tabindex="0">getUseCudaMallocAsync()</a></li>
<li><a href="#getDisallowRetryOnAllocationFailure()" tabindex="0">getDisallowRetryOnAllocationFailure()</a></li>
<li><a href="#getGpuHostMemLimitInMb()" tabindex="0">getGpuHostMemLimitInMb()</a></li>
<li><a href="#getGpuHostMemDisallowGrowth()" tabindex="0">getGpuHostMemDisallowGrowth()</a></li>
<li><a href="#getGpuSystemMemorySizeInMb()" tabindex="0">getGpuSystemMemorySizeInMb()</a></li>
<li><a href="#getPopulatePjrtGpuClientCreationInfo()" tabindex="0">getPopulatePjrtGpuClientCreationInfo()</a></li>
<li><a href="#getNodeId()" tabindex="0">getNodeId()</a></li>
<li><a href="#hasStreamMergeOptions()" tabindex="0">hasStreamMergeOptions()</a></li>
<li><a href="#getStreamMergeOptions()" tabindex="0">getStreamMergeOptions()</a></li>
<li><a href="#getStreamMergeOptionsOrBuilder()" tabindex="0">getStreamMergeOptionsOrBuilder()</a></li>
</ol>
</li>
</ol>
<button class="hide-sidebar"><span>Hide sidebar&nbsp;</span><img src="../../../../resource-files/left.svg" alt="Hide sidebar"></button><button class="show-sidebar"><img src="../../../../resource-files/right.svg" alt="Show sidebar"><span>&nbsp;Show sidebar</span></button></nav>
<main role="main">
<!-- ======== START OF CLASS DATA ======== -->
<div class="header">
<h1 title="Interface GPUOptions.ExperimentalOrBuilder" class="title">Interface GPUOptions.ExperimentalOrBuilder</h1>
</div>
<section class="class-description" id="class-description">
<dl class="notes">
<dt>All Superinterfaces:</dt>
<dd><code><a href="https://protobuf.dev/reference/java/api-docs/com/google/protobuf/MessageLiteOrBuilder.html" title="class or interface in com.google.protobuf" class="external-link">MessageLiteOrBuilder</a>, <a href="https://protobuf.dev/reference/java/api-docs/com/google/protobuf/MessageOrBuilder.html" title="class or interface in com.google.protobuf" class="external-link">MessageOrBuilder</a></code></dd>
</dl>
<dl class="notes">
<dt>All Known Implementing Classes:</dt>
<dd><code><a href="GPUOptions.Experimental.html" title="class in org.tensorflow.proto">GPUOptions.Experimental</a>, <a href="GPUOptions.Experimental.Builder.html" title="class in org.tensorflow.proto">GPUOptions.Experimental.Builder</a></code></dd>
</dl>
<dl class="notes">
<dt>Enclosing class:</dt>
<dd><code><a href="GPUOptions.html" title="class in org.tensorflow.proto">GPUOptions</a></code></dd>
</dl>
<hr>
<div class="horizontal-scroll">
<div class="type-signature"><span class="modifiers">public static interface </span><span class="element-name type-name-label">GPUOptions.ExperimentalOrBuilder</span><span class="extends-implements">
extends <a href="https://protobuf.dev/reference/java/api-docs/com/google/protobuf/MessageOrBuilder.html" title="class or interface in com.google.protobuf" class="external-link">MessageOrBuilder</a></span></div>
</div>
</section>
<section class="summary">
<ul class="summary-list">
<!-- ========== METHOD SUMMARY =========== -->
<li>
<section class="method-summary" id="method-summary">
<h2>Method Summary</h2>
<div id="method-summary-table">
<div class="table-tabs" role="tablist" aria-orientation="horizontal"><button id="method-summary-table-tab0" role="tab" aria-selected="true" aria-controls="method-summary-table.tabpanel" tabindex="0" onkeydown="switchTab(event)" onclick="show('method-summary-table', 'method-summary-table', 3)" class="active-table-tab">All Methods</button><button id="method-summary-table-tab2" role="tab" aria-selected="false" aria-controls="method-summary-table.tabpanel" tabindex="-1" onkeydown="switchTab(event)" onclick="show('method-summary-table', 'method-summary-table-tab2', 3)" class="table-tab">Instance Methods</button><button id="method-summary-table-tab3" role="tab" aria-selected="false" aria-controls="method-summary-table.tabpanel" tabindex="-1" onkeydown="switchTab(event)" onclick="show('method-summary-table', 'method-summary-table-tab3', 3)" class="table-tab">Abstract Methods</button></div>
<div id="method-summary-table.tabpanel" role="tabpanel" aria-labelledby="method-summary-table-tab0">
<div class="summary-table three-column-summary">
<div class="table-header col-first">Modifier and Type</div>
<div class="table-header col-second">Method</div>
<div class="table-header col-last">Description</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code><a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html" title="class or interface in java.lang" class="external-link">String</a></code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code><a href="#getCollectiveRingOrder()" class="member-name-link">getCollectiveRingOrder</a>()</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3">
<div class="block">
If non-empty, defines a good GPU ring order on a single worker based on
device interconnect.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code><a href="https://protobuf.dev/reference/java/api-docs/com/google/protobuf/ByteString.html" title="class or interface in com.google.protobuf" class="external-link">ByteString</a></code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code><a href="#getCollectiveRingOrderBytes()" class="member-name-link">getCollectiveRingOrderBytes</a>()</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3">
<div class="block">
If non-empty, defines a good GPU ring order on a single worker based on
device interconnect.</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code>boolean</code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code><a href="#getDisallowRetryOnAllocationFailure()" class="member-name-link">getDisallowRetryOnAllocationFailure</a>()</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3">
<div class="block">
By default, BFCAllocator may sleep when it runs out of memory, in the
hopes that another thread will free up memory in the meantime.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code>boolean</code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code><a href="#getGpuHostMemDisallowGrowth()" class="member-name-link">getGpuHostMemDisallowGrowth</a>()</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3">
<div class="block">
If true, then the host allocator allocates its max memory all upfront and
never grows.</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code>float</code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code><a href="#getGpuHostMemLimitInMb()" class="member-name-link">getGpuHostMemLimitInMb</a>()</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3">
<div class="block">
Memory limit for "GPU host allocator", aka pinned memory allocator.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code>int</code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code><a href="#getGpuSystemMemorySizeInMb()" class="member-name-link">getGpuSystemMemorySizeInMb</a>()</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3">
<div class="block">
Memory limit for gpu system.</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code>double</code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code><a href="#getInternalFragmentationFraction()" class="member-name-link">getInternalFragmentationFraction</a>()</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3">
<div class="block">
BFC Allocator can return an allocated chunk of memory upto 2x the
requested size.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code>int</code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code><a href="#getKernelTrackerMaxBytes()" class="member-name-link">getKernelTrackerMaxBytes</a>()</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3">
<div class="block">
If kernel_tracker_max_bytes = n &gt; 0, then a tracking event is
inserted after every series of kernels allocating a sum of
memory &gt;= n.</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code>int</code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code><a href="#getKernelTrackerMaxInterval()" class="member-name-link">getKernelTrackerMaxInterval</a>()</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3">
<div class="block">
Parameters for GPUKernelTracker.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code>int</code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code><a href="#getKernelTrackerMaxPending()" class="member-name-link">getKernelTrackerMaxPending</a>()</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3">
<div class="block">
If kernel_tracker_max_pending &gt; 0 then no more than this many
tracking events can be outstanding at a time.</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code>int</code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code><a href="#getNodeId()" class="member-name-link">getNodeId</a>()</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3">
<div class="block">
node_id for use when creating a PjRt GPU client with remote devices,
which enumerates jobs*tasks from a ServerDef.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code>int</code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code><a href="#getNumDevToDevCopyStreams()" class="member-name-link">getNumDevToDevCopyStreams</a>()</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3">
<div class="block">
If &gt; 1, the number of device-to-device copy streams to create
for each GPUDevice.</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code>int</code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code><a href="#getNumVirtualDevicesPerGpu()" class="member-name-link">getNumVirtualDevicesPerGpu</a>()</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3">
<div class="block">
The number of virtual devices to create on each visible GPU.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code>boolean</code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code><a href="#getPopulatePjrtGpuClientCreationInfo()" class="member-name-link">getPopulatePjrtGpuClientCreationInfo</a>()</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3">
<div class="block">
If true, save information needed for created a PjRt GPU client for
creating a client with remote devices.</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code><a href="GPUOptions.Experimental.StreamMergeOptions.html" title="class in org.tensorflow.proto">GPUOptions.Experimental.StreamMergeOptions</a></code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code><a href="#getStreamMergeOptions()" class="member-name-link">getStreamMergeOptions</a>()</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3">
<div class="block"><code>.tensorflow.GPUOptions.Experimental.StreamMergeOptions stream_merge_options = 19;</code></div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code><a href="GPUOptions.Experimental.StreamMergeOptionsOrBuilder.html" title="interface in org.tensorflow.proto">GPUOptions.Experimental.StreamMergeOptionsOrBuilder</a></code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code><a href="#getStreamMergeOptionsOrBuilder()" class="member-name-link">getStreamMergeOptionsOrBuilder</a>()</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3">
<div class="block"><code>.tensorflow.GPUOptions.Experimental.StreamMergeOptions stream_merge_options = 19;</code></div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code>boolean</code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code><a href="#getTimestampedAllocator()" class="member-name-link">getTimestampedAllocator</a>()</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3">
<div class="block">
If true then extra work is done by GPUDevice and GPUBFCAllocator to
keep track of when GPU memory is freed and when kernels actually
complete so that we can know when a nominally free memory chunk
is really not subject to pending use.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code>boolean</code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code><a href="#getUseCudaMallocAsync()" class="member-name-link">getUseCudaMallocAsync</a>()</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3">
<div class="block">
When true, use CUDA cudaMallocAsync API instead of TF gpu allocator.</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code>boolean</code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code><a href="#getUseUnifiedMemory()" class="member-name-link">getUseUnifiedMemory</a>()</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3">
<div class="block">
If true, uses CUDA unified memory for memory allocations.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code><a href="GPUOptions.Experimental.VirtualDevices.html" title="class in org.tensorflow.proto">GPUOptions.Experimental.VirtualDevices</a></code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code><a href="#getVirtualDevices(int)" class="member-name-link">getVirtualDevices</a><wbr>(int&nbsp;index)</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3">
<div class="block">
The multi virtual device settings.</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code>int</code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code><a href="#getVirtualDevicesCount()" class="member-name-link">getVirtualDevicesCount</a>()</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3">
<div class="block">
The multi virtual device settings.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code><a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/List.html" title="class or interface in java.util" class="external-link">List</a><wbr>&lt;<a href="GPUOptions.Experimental.VirtualDevices.html" title="class in org.tensorflow.proto">GPUOptions.Experimental.VirtualDevices</a>&gt;</code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code><a href="#getVirtualDevicesList()" class="member-name-link">getVirtualDevicesList</a>()</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3">
<div class="block">
The multi virtual device settings.</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code><a href="GPUOptions.Experimental.VirtualDevicesOrBuilder.html" title="interface in org.tensorflow.proto">GPUOptions.Experimental.VirtualDevicesOrBuilder</a></code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code><a href="#getVirtualDevicesOrBuilder(int)" class="member-name-link">getVirtualDevicesOrBuilder</a><wbr>(int&nbsp;index)</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3">
<div class="block">
The multi virtual device settings.</div>
</div>
<div class="col-first odd-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code><a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/List.html" title="class or interface in java.util" class="external-link">List</a><wbr>&lt;? extends <a href="GPUOptions.Experimental.VirtualDevicesOrBuilder.html" title="interface in org.tensorflow.proto">GPUOptions.Experimental.VirtualDevicesOrBuilder</a>&gt;</code></div>
<div class="col-second odd-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code><a href="#getVirtualDevicesOrBuilderList()" class="member-name-link">getVirtualDevicesOrBuilderList</a>()</code></div>
<div class="col-last odd-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3">
<div class="block">
The multi virtual device settings.</div>
</div>
<div class="col-first even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code>boolean</code></div>
<div class="col-second even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3"><code><a href="#hasStreamMergeOptions()" class="member-name-link">hasStreamMergeOptions</a>()</code></div>
<div class="col-last even-row-color method-summary-table method-summary-table-tab2 method-summary-table-tab3">
<div class="block"><code>.tensorflow.GPUOptions.Experimental.StreamMergeOptions stream_merge_options = 19;</code></div>
</div>
</div>
</div>
</div>
<div class="inherited-list">
<h3 id="methods-inherited-from-class-com.google.protobuf.MessageLiteOrBuilder">Methods inherited from interface&nbsp;<a href="https://protobuf.dev/reference/java/api-docs/com/google/protobuf/MessageLiteOrBuilder.html#method-summary" title="class or interface in com.google.protobuf" class="external-link">MessageLiteOrBuilder</a></h3>
<code><a href="https://protobuf.dev/reference/java/api-docs/com/google/protobuf/MessageLiteOrBuilder.html#isInitialized--" title="class or interface in com.google.protobuf" class="external-link">isInitialized</a></code></div>
<div class="inherited-list">
<h3 id="methods-inherited-from-class-com.google.protobuf.MessageOrBuilder">Methods inherited from interface&nbsp;<a href="https://protobuf.dev/reference/java/api-docs/com/google/protobuf/MessageOrBuilder.html#method-summary" title="class or interface in com.google.protobuf" class="external-link">MessageOrBuilder</a></h3>
<code><a href="https://protobuf.dev/reference/java/api-docs/com/google/protobuf/MessageOrBuilder.html#findInitializationErrors--" title="class or interface in com.google.protobuf" class="external-link">findInitializationErrors</a>, <a href="https://protobuf.dev/reference/java/api-docs/com/google/protobuf/MessageOrBuilder.html#getAllFields--" title="class or interface in com.google.protobuf" class="external-link">getAllFields</a>, <a href="https://protobuf.dev/reference/java/api-docs/com/google/protobuf/MessageOrBuilder.html#getDefaultInstanceForType--" title="class or interface in com.google.protobuf" class="external-link">getDefaultInstanceForType</a>, <a href="https://protobuf.dev/reference/java/api-docs/com/google/protobuf/MessageOrBuilder.html#getDescriptorForType--" title="class or interface in com.google.protobuf" class="external-link">getDescriptorForType</a>, <a href="https://protobuf.dev/reference/java/api-docs/com/google/protobuf/MessageOrBuilder.html#getField-com.google.protobuf.Descriptors.FieldDescriptor-" title="class or interface in com.google.protobuf" class="external-link">getField</a>, <a href="https://protobuf.dev/reference/java/api-docs/com/google/protobuf/MessageOrBuilder.html#getInitializationErrorString--" title="class or interface in com.google.protobuf" class="external-link">getInitializationErrorString</a>, <a href="https://protobuf.dev/reference/java/api-docs/com/google/protobuf/MessageOrBuilder.html#getOneofFieldDescriptor-com.google.protobuf.Descriptors.OneofDescriptor-" title="class or interface in com.google.protobuf" class="external-link">getOneofFieldDescriptor</a>, <a href="https://protobuf.dev/reference/java/api-docs/com/google/protobuf/MessageOrBuilder.html#getRepeatedField-com.google.protobuf.Descriptors.FieldDescriptor-int-" title="class or interface in com.google.protobuf" class="external-link">getRepeatedField</a>, <a href="https://protobuf.dev/reference/java/api-docs/com/google/protobuf/MessageOrBuilder.html#getRepeatedFieldCount-com.google.protobuf.Descriptors.FieldDescriptor-" title="class or interface in com.google.protobuf" class="external-link">getRepeatedFieldCount</a>, <a href="https://protobuf.dev/reference/java/api-docs/com/google/protobuf/MessageOrBuilder.html#getUnknownFields--" title="class or interface in com.google.protobuf" class="external-link">getUnknownFields</a>, <a href="https://protobuf.dev/reference/java/api-docs/com/google/protobuf/MessageOrBuilder.html#hasField-com.google.protobuf.Descriptors.FieldDescriptor-" title="class or interface in com.google.protobuf" class="external-link">hasField</a>, <a href="https://protobuf.dev/reference/java/api-docs/com/google/protobuf/MessageOrBuilder.html#hasOneof-com.google.protobuf.Descriptors.OneofDescriptor-" title="class or interface in com.google.protobuf" class="external-link">hasOneof</a></code></div>
</section>
</li>
</ul>
</section>
<section class="details">
<ul class="details-list">
<!-- ============ METHOD DETAIL ========== -->
<li>
<section class="method-details" id="method-detail">
<h2>Method Details</h2>
<ul class="member-list">
<li>
<section class="detail" id="getVirtualDevicesList()">
<h3>getVirtualDevicesList</h3>
<div class="horizontal-scroll">
<div class="member-signature"><span class="return-type"><a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/List.html" title="class or interface in java.util" class="external-link">List</a>&lt;<a href="GPUOptions.Experimental.VirtualDevices.html" title="class in org.tensorflow.proto">GPUOptions.Experimental.VirtualDevices</a>&gt;</span>&nbsp;<span class="element-name">getVirtualDevicesList</span>()</div>
<div class="block"><pre>
The multi virtual device settings. If empty (not set), it will create
single virtual device on each visible GPU, according to the settings
in "visible_device_list" above. Otherwise, the number of elements in the
list must be the same as the number of visible GPUs (after
"visible_device_list" filtering if it is set), and the string represented
device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
devices and have the &lt;id&gt; field assigned sequentially starting from 0,
according to the order of the virtual devices determined by
device_ordinal and the location in the virtual device list.
For example,
  visible_device_list = "1,0"
  virtual_devices { memory_limit: 1GB memory_limit: 2GB }
  virtual_devices { memory_limit: 3GB memory_limit: 4GB }
will create 4 virtual devices as:
  /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
  /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
  /device:GPU:2 -&gt; visible GPU 0 with 3GB memory
  /device:GPU:3 -&gt; visible GPU 0 with 4GB memory
but
  visible_device_list = "1,0"
  virtual_devices { memory_limit: 1GB memory_limit: 2GB
                    device_ordinal: 10 device_ordinal: 20}
  virtual_devices { memory_limit: 3GB memory_limit: 4GB
                    device_ordinal: 10 device_ordinal: 20}
will create 4 virtual devices as:
  /device:GPU:0 -&gt; visible GPU 1 with 1GB memory  (ordinal 10)
  /device:GPU:1 -&gt; visible GPU 0 with 3GB memory  (ordinal 10)
  /device:GPU:2 -&gt; visible GPU 1 with 2GB memory  (ordinal 20)
  /device:GPU:3 -&gt; visible GPU 0 with 4GB memory  (ordinal 20)
NOTE:
1. It's invalid to set both this and "per_process_gpu_memory_fraction"
   at the same time.
2. Currently this setting is per-process, not per-session. Using
   different settings in different sessions within same process will
   result in undefined behavior.
</pre>

<code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code></div>
</div>
</section>
</li>
<li>
<section class="detail" id="getVirtualDevices(int)">
<h3>getVirtualDevices</h3>
<div class="horizontal-scroll">
<div class="member-signature"><span class="return-type"><a href="GPUOptions.Experimental.VirtualDevices.html" title="class in org.tensorflow.proto">GPUOptions.Experimental.VirtualDevices</a></span>&nbsp;<span class="element-name">getVirtualDevices</span><wbr><span class="parameters">(int&nbsp;index)</span></div>
<div class="block"><pre>
The multi virtual device settings. If empty (not set), it will create
single virtual device on each visible GPU, according to the settings
in "visible_device_list" above. Otherwise, the number of elements in the
list must be the same as the number of visible GPUs (after
"visible_device_list" filtering if it is set), and the string represented
device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
devices and have the &lt;id&gt; field assigned sequentially starting from 0,
according to the order of the virtual devices determined by
device_ordinal and the location in the virtual device list.
For example,
  visible_device_list = "1,0"
  virtual_devices { memory_limit: 1GB memory_limit: 2GB }
  virtual_devices { memory_limit: 3GB memory_limit: 4GB }
will create 4 virtual devices as:
  /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
  /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
  /device:GPU:2 -&gt; visible GPU 0 with 3GB memory
  /device:GPU:3 -&gt; visible GPU 0 with 4GB memory
but
  visible_device_list = "1,0"
  virtual_devices { memory_limit: 1GB memory_limit: 2GB
                    device_ordinal: 10 device_ordinal: 20}
  virtual_devices { memory_limit: 3GB memory_limit: 4GB
                    device_ordinal: 10 device_ordinal: 20}
will create 4 virtual devices as:
  /device:GPU:0 -&gt; visible GPU 1 with 1GB memory  (ordinal 10)
  /device:GPU:1 -&gt; visible GPU 0 with 3GB memory  (ordinal 10)
  /device:GPU:2 -&gt; visible GPU 1 with 2GB memory  (ordinal 20)
  /device:GPU:3 -&gt; visible GPU 0 with 4GB memory  (ordinal 20)
NOTE:
1. It's invalid to set both this and "per_process_gpu_memory_fraction"
   at the same time.
2. Currently this setting is per-process, not per-session. Using
   different settings in different sessions within same process will
   result in undefined behavior.
</pre>

<code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code></div>
</div>
</section>
</li>
<li>
<section class="detail" id="getVirtualDevicesCount()">
<h3>getVirtualDevicesCount</h3>
<div class="horizontal-scroll">
<div class="member-signature"><span class="return-type">int</span>&nbsp;<span class="element-name">getVirtualDevicesCount</span>()</div>
<div class="block"><pre>
The multi virtual device settings. If empty (not set), it will create
single virtual device on each visible GPU, according to the settings
in "visible_device_list" above. Otherwise, the number of elements in the
list must be the same as the number of visible GPUs (after
"visible_device_list" filtering if it is set), and the string represented
device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
devices and have the &lt;id&gt; field assigned sequentially starting from 0,
according to the order of the virtual devices determined by
device_ordinal and the location in the virtual device list.
For example,
  visible_device_list = "1,0"
  virtual_devices { memory_limit: 1GB memory_limit: 2GB }
  virtual_devices { memory_limit: 3GB memory_limit: 4GB }
will create 4 virtual devices as:
  /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
  /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
  /device:GPU:2 -&gt; visible GPU 0 with 3GB memory
  /device:GPU:3 -&gt; visible GPU 0 with 4GB memory
but
  visible_device_list = "1,0"
  virtual_devices { memory_limit: 1GB memory_limit: 2GB
                    device_ordinal: 10 device_ordinal: 20}
  virtual_devices { memory_limit: 3GB memory_limit: 4GB
                    device_ordinal: 10 device_ordinal: 20}
will create 4 virtual devices as:
  /device:GPU:0 -&gt; visible GPU 1 with 1GB memory  (ordinal 10)
  /device:GPU:1 -&gt; visible GPU 0 with 3GB memory  (ordinal 10)
  /device:GPU:2 -&gt; visible GPU 1 with 2GB memory  (ordinal 20)
  /device:GPU:3 -&gt; visible GPU 0 with 4GB memory  (ordinal 20)
NOTE:
1. It's invalid to set both this and "per_process_gpu_memory_fraction"
   at the same time.
2. Currently this setting is per-process, not per-session. Using
   different settings in different sessions within same process will
   result in undefined behavior.
</pre>

<code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code></div>
</div>
</section>
</li>
<li>
<section class="detail" id="getVirtualDevicesOrBuilderList()">
<h3>getVirtualDevicesOrBuilderList</h3>
<div class="horizontal-scroll">
<div class="member-signature"><span class="return-type"><a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/List.html" title="class or interface in java.util" class="external-link">List</a>&lt;? extends <a href="GPUOptions.Experimental.VirtualDevicesOrBuilder.html" title="interface in org.tensorflow.proto">GPUOptions.Experimental.VirtualDevicesOrBuilder</a>&gt;</span>&nbsp;<span class="element-name">getVirtualDevicesOrBuilderList</span>()</div>
<div class="block"><pre>
The multi virtual device settings. If empty (not set), it will create
single virtual device on each visible GPU, according to the settings
in "visible_device_list" above. Otherwise, the number of elements in the
list must be the same as the number of visible GPUs (after
"visible_device_list" filtering if it is set), and the string represented
device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
devices and have the &lt;id&gt; field assigned sequentially starting from 0,
according to the order of the virtual devices determined by
device_ordinal and the location in the virtual device list.
For example,
  visible_device_list = "1,0"
  virtual_devices { memory_limit: 1GB memory_limit: 2GB }
  virtual_devices { memory_limit: 3GB memory_limit: 4GB }
will create 4 virtual devices as:
  /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
  /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
  /device:GPU:2 -&gt; visible GPU 0 with 3GB memory
  /device:GPU:3 -&gt; visible GPU 0 with 4GB memory
but
  visible_device_list = "1,0"
  virtual_devices { memory_limit: 1GB memory_limit: 2GB
                    device_ordinal: 10 device_ordinal: 20}
  virtual_devices { memory_limit: 3GB memory_limit: 4GB
                    device_ordinal: 10 device_ordinal: 20}
will create 4 virtual devices as:
  /device:GPU:0 -&gt; visible GPU 1 with 1GB memory  (ordinal 10)
  /device:GPU:1 -&gt; visible GPU 0 with 3GB memory  (ordinal 10)
  /device:GPU:2 -&gt; visible GPU 1 with 2GB memory  (ordinal 20)
  /device:GPU:3 -&gt; visible GPU 0 with 4GB memory  (ordinal 20)
NOTE:
1. It's invalid to set both this and "per_process_gpu_memory_fraction"
   at the same time.
2. Currently this setting is per-process, not per-session. Using
   different settings in different sessions within same process will
   result in undefined behavior.
</pre>

<code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code></div>
</div>
</section>
</li>
<li>
<section class="detail" id="getVirtualDevicesOrBuilder(int)">
<h3>getVirtualDevicesOrBuilder</h3>
<div class="horizontal-scroll">
<div class="member-signature"><span class="return-type"><a href="GPUOptions.Experimental.VirtualDevicesOrBuilder.html" title="interface in org.tensorflow.proto">GPUOptions.Experimental.VirtualDevicesOrBuilder</a></span>&nbsp;<span class="element-name">getVirtualDevicesOrBuilder</span><wbr><span class="parameters">(int&nbsp;index)</span></div>
<div class="block"><pre>
The multi virtual device settings. If empty (not set), it will create
single virtual device on each visible GPU, according to the settings
in "visible_device_list" above. Otherwise, the number of elements in the
list must be the same as the number of visible GPUs (after
"visible_device_list" filtering if it is set), and the string represented
device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
devices and have the &lt;id&gt; field assigned sequentially starting from 0,
according to the order of the virtual devices determined by
device_ordinal and the location in the virtual device list.
For example,
  visible_device_list = "1,0"
  virtual_devices { memory_limit: 1GB memory_limit: 2GB }
  virtual_devices { memory_limit: 3GB memory_limit: 4GB }
will create 4 virtual devices as:
  /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
  /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
  /device:GPU:2 -&gt; visible GPU 0 with 3GB memory
  /device:GPU:3 -&gt; visible GPU 0 with 4GB memory
but
  visible_device_list = "1,0"
  virtual_devices { memory_limit: 1GB memory_limit: 2GB
                    device_ordinal: 10 device_ordinal: 20}
  virtual_devices { memory_limit: 3GB memory_limit: 4GB
                    device_ordinal: 10 device_ordinal: 20}
will create 4 virtual devices as:
  /device:GPU:0 -&gt; visible GPU 1 with 1GB memory  (ordinal 10)
  /device:GPU:1 -&gt; visible GPU 0 with 3GB memory  (ordinal 10)
  /device:GPU:2 -&gt; visible GPU 1 with 2GB memory  (ordinal 20)
  /device:GPU:3 -&gt; visible GPU 0 with 4GB memory  (ordinal 20)
NOTE:
1. It's invalid to set both this and "per_process_gpu_memory_fraction"
   at the same time.
2. Currently this setting is per-process, not per-session. Using
   different settings in different sessions within same process will
   result in undefined behavior.
</pre>

<code>repeated .tensorflow.GPUOptions.Experimental.VirtualDevices virtual_devices = 1;</code></div>
</div>
</section>
</li>
<li>
<section class="detail" id="getNumVirtualDevicesPerGpu()">
<h3>getNumVirtualDevicesPerGpu</h3>
<div class="horizontal-scroll">
<div class="member-signature"><span class="return-type">int</span>&nbsp;<span class="element-name">getNumVirtualDevicesPerGpu</span>()</div>
<div class="block"><pre>
The number of virtual devices to create on each visible GPU. The
available memory will be split equally among all virtual devices. If the
field `memory_limit_mb` in `VirtualDevices` is not empty, this field will
be ignored.
</pre>

<code>int32 num_virtual_devices_per_gpu = 15;</code></div>
<dl class="notes">
<dt>Returns:</dt>
<dd>The numVirtualDevicesPerGpu.</dd>
</dl>
</div>
</section>
</li>
<li>
<section class="detail" id="getUseUnifiedMemory()">
<h3>getUseUnifiedMemory</h3>
<div class="horizontal-scroll">
<div class="member-signature"><span class="return-type">boolean</span>&nbsp;<span class="element-name">getUseUnifiedMemory</span>()</div>
<div class="block"><pre>
If true, uses CUDA unified memory for memory allocations. If
per_process_gpu_memory_fraction option is greater than 1.0, then unified
memory is used regardless of the value for this field. See comments for
per_process_gpu_memory_fraction field for more details and requirements
of the unified memory. This option is useful to oversubscribe memory if
multiple processes are sharing a single GPU while individually using less
than 1.0 per process memory fraction.
</pre>

<code>bool use_unified_memory = 2;</code></div>
<dl class="notes">
<dt>Returns:</dt>
<dd>The useUnifiedMemory.</dd>
</dl>
</div>
</section>
</li>
<li>
<section class="detail" id="getNumDevToDevCopyStreams()">
<h3>getNumDevToDevCopyStreams</h3>
<div class="horizontal-scroll">
<div class="member-signature"><span class="return-type">int</span>&nbsp;<span class="element-name">getNumDevToDevCopyStreams</span>()</div>
<div class="block"><pre>
If &gt; 1, the number of device-to-device copy streams to create
for each GPUDevice.  Default value is 0, which is automatically
converted to 1.
</pre>

<code>int32 num_dev_to_dev_copy_streams = 3;</code></div>
<dl class="notes">
<dt>Returns:</dt>
<dd>The numDevToDevCopyStreams.</dd>
</dl>
</div>
</section>
</li>
<li>
<section class="detail" id="getCollectiveRingOrder()">
<h3>getCollectiveRingOrder</h3>
<div class="horizontal-scroll">
<div class="member-signature"><span class="return-type"><a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html" title="class or interface in java.lang" class="external-link">String</a></span>&nbsp;<span class="element-name">getCollectiveRingOrder</span>()</div>
<div class="block"><pre>
If non-empty, defines a good GPU ring order on a single worker based on
device interconnect.  This assumes that all workers have the same GPU
topology.  Specify as a comma-separated string, e.g. "3,2,1,0,7,6,5,4".
This ring order is used by the RingReducer implementation of
CollectiveReduce, and serves as an override to automatic ring order
generation in OrderTaskDeviceMap() during CollectiveParam resolution.
</pre>

<code>string collective_ring_order = 4;</code></div>
<dl class="notes">
<dt>Returns:</dt>
<dd>The collectiveRingOrder.</dd>
</dl>
</div>
</section>
</li>
<li>
<section class="detail" id="getCollectiveRingOrderBytes()">
<h3>getCollectiveRingOrderBytes</h3>
<div class="horizontal-scroll">
<div class="member-signature"><span class="return-type"><a href="https://protobuf.dev/reference/java/api-docs/com/google/protobuf/ByteString.html" title="class or interface in com.google.protobuf" class="external-link">ByteString</a></span>&nbsp;<span class="element-name">getCollectiveRingOrderBytes</span>()</div>
<div class="block"><pre>
If non-empty, defines a good GPU ring order on a single worker based on
device interconnect.  This assumes that all workers have the same GPU
topology.  Specify as a comma-separated string, e.g. "3,2,1,0,7,6,5,4".
This ring order is used by the RingReducer implementation of
CollectiveReduce, and serves as an override to automatic ring order
generation in OrderTaskDeviceMap() during CollectiveParam resolution.
</pre>

<code>string collective_ring_order = 4;</code></div>
<dl class="notes">
<dt>Returns:</dt>
<dd>The bytes for collectiveRingOrder.</dd>
</dl>
</div>
</section>
</li>
<li>
<section class="detail" id="getTimestampedAllocator()">
<h3>getTimestampedAllocator</h3>
<div class="horizontal-scroll">
<div class="member-signature"><span class="return-type">boolean</span>&nbsp;<span class="element-name">getTimestampedAllocator</span>()</div>
<div class="block"><pre>
If true then extra work is done by GPUDevice and GPUBFCAllocator to
keep track of when GPU memory is freed and when kernels actually
complete so that we can know when a nominally free memory chunk
is really not subject to pending use.
</pre>

<code>bool timestamped_allocator = 5;</code></div>
<dl class="notes">
<dt>Returns:</dt>
<dd>The timestampedAllocator.</dd>
</dl>
</div>
</section>
</li>
<li>
<section class="detail" id="getKernelTrackerMaxInterval()">
<h3>getKernelTrackerMaxInterval</h3>
<div class="horizontal-scroll">
<div class="member-signature"><span class="return-type">int</span>&nbsp;<span class="element-name">getKernelTrackerMaxInterval</span>()</div>
<div class="block"><pre>
Parameters for GPUKernelTracker.  By default no kernel tracking is done.
Note that timestamped_allocator is only effective if some tracking is
specified.
If kernel_tracker_max_interval = n &gt; 0, then a tracking event
is inserted after every n kernels without an event.
</pre>

<code>int32 kernel_tracker_max_interval = 7;</code></div>
<dl class="notes">
<dt>Returns:</dt>
<dd>The kernelTrackerMaxInterval.</dd>
</dl>
</div>
</section>
</li>
<li>
<section class="detail" id="getKernelTrackerMaxBytes()">
<h3>getKernelTrackerMaxBytes</h3>
<div class="horizontal-scroll">
<div class="member-signature"><span class="return-type">int</span>&nbsp;<span class="element-name">getKernelTrackerMaxBytes</span>()</div>
<div class="block"><pre>
If kernel_tracker_max_bytes = n &gt; 0, then a tracking event is
inserted after every series of kernels allocating a sum of
memory &gt;= n.  If one kernel allocates b * n bytes, then one
event will be inserted after it, but it will count as b against
the pending limit.
</pre>

<code>int32 kernel_tracker_max_bytes = 8;</code></div>
<dl class="notes">
<dt>Returns:</dt>
<dd>The kernelTrackerMaxBytes.</dd>
</dl>
</div>
</section>
</li>
<li>
<section class="detail" id="getKernelTrackerMaxPending()">
<h3>getKernelTrackerMaxPending</h3>
<div class="horizontal-scroll">
<div class="member-signature"><span class="return-type">int</span>&nbsp;<span class="element-name">getKernelTrackerMaxPending</span>()</div>
<div class="block"><pre>
If kernel_tracker_max_pending &gt; 0 then no more than this many
tracking events can be outstanding at a time.  An attempt to
launch an additional kernel will stall until an event
completes.
</pre>

<code>int32 kernel_tracker_max_pending = 9;</code></div>
<dl class="notes">
<dt>Returns:</dt>
<dd>The kernelTrackerMaxPending.</dd>
</dl>
</div>
</section>
</li>
<li>
<section class="detail" id="getInternalFragmentationFraction()">
<h3>getInternalFragmentationFraction</h3>
<div class="horizontal-scroll">
<div class="member-signature"><span class="return-type">double</span>&nbsp;<span class="element-name">getInternalFragmentationFraction</span>()</div>
<div class="block"><pre>
BFC Allocator can return an allocated chunk of memory upto 2x the
requested size. For virtual devices with tight memory constraints, and
proportionately large allocation requests, this can lead to a significant
reduction in available memory. The threshold below controls when a chunk
should be split if the chunk size exceeds requested memory size. It is
expressed as a fraction of total available memory for the tf device. For
example setting it to 0.05 would imply a chunk needs to be split if its
size exceeds the requested memory by 5% of the total virtual device/gpu
memory size.
</pre>

<code>double internal_fragmentation_fraction = 10;</code></div>
<dl class="notes">
<dt>Returns:</dt>
<dd>The internalFragmentationFraction.</dd>
</dl>
</div>
</section>
</li>
<li>
<section class="detail" id="getUseCudaMallocAsync()">
<h3>getUseCudaMallocAsync</h3>
<div class="horizontal-scroll">
<div class="member-signature"><span class="return-type">boolean</span>&nbsp;<span class="element-name">getUseCudaMallocAsync</span>()</div>
<div class="block"><pre>
When true, use CUDA cudaMallocAsync API instead of TF gpu allocator.
</pre>

<code>bool use_cuda_malloc_async = 11;</code></div>
<dl class="notes">
<dt>Returns:</dt>
<dd>The useCudaMallocAsync.</dd>
</dl>
</div>
</section>
</li>
<li>
<section class="detail" id="getDisallowRetryOnAllocationFailure()">
<h3>getDisallowRetryOnAllocationFailure</h3>
<div class="horizontal-scroll">
<div class="member-signature"><span class="return-type">boolean</span>&nbsp;<span class="element-name">getDisallowRetryOnAllocationFailure</span>()</div>
<div class="block"><pre>
By default, BFCAllocator may sleep when it runs out of memory, in the
hopes that another thread will free up memory in the meantime.  Setting
this to true disables the sleep; instead we'll OOM immediately.
</pre>

<code>bool disallow_retry_on_allocation_failure = 12;</code></div>
<dl class="notes">
<dt>Returns:</dt>
<dd>The disallowRetryOnAllocationFailure.</dd>
</dl>
</div>
</section>
</li>
<li>
<section class="detail" id="getGpuHostMemLimitInMb()">
<h3>getGpuHostMemLimitInMb</h3>
<div class="horizontal-scroll">
<div class="member-signature"><span class="return-type">float</span>&nbsp;<span class="element-name">getGpuHostMemLimitInMb</span>()</div>
<div class="block"><pre>
Memory limit for "GPU host allocator", aka pinned memory allocator.  This
can also be set via the envvar TF_GPU_HOST_MEM_LIMIT_IN_MB.
</pre>

<code>float gpu_host_mem_limit_in_mb = 13;</code></div>
<dl class="notes">
<dt>Returns:</dt>
<dd>The gpuHostMemLimitInMb.</dd>
</dl>
</div>
</section>
</li>
<li>
<section class="detail" id="getGpuHostMemDisallowGrowth()">
<h3>getGpuHostMemDisallowGrowth</h3>
<div class="horizontal-scroll">
<div class="member-signature"><span class="return-type">boolean</span>&nbsp;<span class="element-name">getGpuHostMemDisallowGrowth</span>()</div>
<div class="block"><pre>
If true, then the host allocator allocates its max memory all upfront and
never grows.  This can be useful for latency-sensitive systems, because
growing the GPU host memory pool can be expensive.
You probably only want to use this in combination with
gpu_host_mem_limit_in_mb, because the default GPU host memory limit is
quite high.
</pre>

<code>bool gpu_host_mem_disallow_growth = 14;</code></div>
<dl class="notes">
<dt>Returns:</dt>
<dd>The gpuHostMemDisallowGrowth.</dd>
</dl>
</div>
</section>
</li>
<li>
<section class="detail" id="getGpuSystemMemorySizeInMb()">
<h3>getGpuSystemMemorySizeInMb</h3>
<div class="horizontal-scroll">
<div class="member-signature"><span class="return-type">int</span>&nbsp;<span class="element-name">getGpuSystemMemorySizeInMb</span>()</div>
<div class="block"><pre>
Memory limit for gpu system. This can also be set by
TF_DEVICE_MIN_SYS_MEMORY_IN_MB, which takes precedence over
gpu_system_memory_size_in_mb. With this, user can configure the gpu
system memory size for better resource estimation of multi-tenancy(one
gpu with multiple model) use case.
</pre>

<code>int32 gpu_system_memory_size_in_mb = 16;</code></div>
<dl class="notes">
<dt>Returns:</dt>
<dd>The gpuSystemMemorySizeInMb.</dd>
</dl>
</div>
</section>
</li>
<li>
<section class="detail" id="getPopulatePjrtGpuClientCreationInfo()">
<h3>getPopulatePjrtGpuClientCreationInfo</h3>
<div class="horizontal-scroll">
<div class="member-signature"><span class="return-type">boolean</span>&nbsp;<span class="element-name">getPopulatePjrtGpuClientCreationInfo</span>()</div>
<div class="block"><pre>
If true, save information needed for created a PjRt GPU client for
creating a client with remote devices.
</pre>

<code>bool populate_pjrt_gpu_client_creation_info = 17;</code></div>
<dl class="notes">
<dt>Returns:</dt>
<dd>The populatePjrtGpuClientCreationInfo.</dd>
</dl>
</div>
</section>
</li>
<li>
<section class="detail" id="getNodeId()">
<h3>getNodeId</h3>
<div class="horizontal-scroll">
<div class="member-signature"><span class="return-type">int</span>&nbsp;<span class="element-name">getNodeId</span>()</div>
<div class="block"><pre>
node_id for use when creating a PjRt GPU client with remote devices,
which enumerates jobs*tasks from a ServerDef.
</pre>

<code>int32 node_id = 18;</code></div>
<dl class="notes">
<dt>Returns:</dt>
<dd>The nodeId.</dd>
</dl>
</div>
</section>
</li>
<li>
<section class="detail" id="hasStreamMergeOptions()">
<h3>hasStreamMergeOptions</h3>
<div class="horizontal-scroll">
<div class="member-signature"><span class="return-type">boolean</span>&nbsp;<span class="element-name">hasStreamMergeOptions</span>()</div>
<div class="block"><code>.tensorflow.GPUOptions.Experimental.StreamMergeOptions stream_merge_options = 19;</code></div>
<dl class="notes">
<dt>Returns:</dt>
<dd>Whether the streamMergeOptions field is set.</dd>
</dl>
</div>
</section>
</li>
<li>
<section class="detail" id="getStreamMergeOptions()">
<h3>getStreamMergeOptions</h3>
<div class="horizontal-scroll">
<div class="member-signature"><span class="return-type"><a href="GPUOptions.Experimental.StreamMergeOptions.html" title="class in org.tensorflow.proto">GPUOptions.Experimental.StreamMergeOptions</a></span>&nbsp;<span class="element-name">getStreamMergeOptions</span>()</div>
<div class="block"><code>.tensorflow.GPUOptions.Experimental.StreamMergeOptions stream_merge_options = 19;</code></div>
<dl class="notes">
<dt>Returns:</dt>
<dd>The streamMergeOptions.</dd>
</dl>
</div>
</section>
</li>
<li>
<section class="detail" id="getStreamMergeOptionsOrBuilder()">
<h3>getStreamMergeOptionsOrBuilder</h3>
<div class="horizontal-scroll">
<div class="member-signature"><span class="return-type"><a href="GPUOptions.Experimental.StreamMergeOptionsOrBuilder.html" title="interface in org.tensorflow.proto">GPUOptions.Experimental.StreamMergeOptionsOrBuilder</a></span>&nbsp;<span class="element-name">getStreamMergeOptionsOrBuilder</span>()</div>
<div class="block"><code>.tensorflow.GPUOptions.Experimental.StreamMergeOptions stream_merge_options = 19;</code></div>
</div>
</section>
</li>
</ul>
</section>
</li>
</ul>
</section>
<!-- ========= END OF CLASS DATA ========= -->
<footer role="contentinfo">
<hr>
<p class="legal-copy"><small>Copyright 2015, 2025 The TensorFlow Authors. All Rights Reserved.</small></p>
</footer>
</main>
</div>
</body>
</html>
