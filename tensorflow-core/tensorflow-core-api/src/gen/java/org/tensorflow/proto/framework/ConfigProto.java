// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: tensorflow/core/protobuf/config.proto

package org.tensorflow.proto.framework;

/**
 * <pre>
 * Session configuration parameters.
 * The system picks appropriate values for fields that are not set.
 * </pre>
 *
 * Protobuf type {@code tensorflow.ConfigProto}
 */
public  final class ConfigProto extends
    com.google.protobuf.GeneratedMessageV3 implements
    // @@protoc_insertion_point(message_implements:tensorflow.ConfigProto)
    ConfigProtoOrBuilder {
private static final long serialVersionUID = 0L;
  // Use ConfigProto.newBuilder() to construct.
  private ConfigProto(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
    super(builder);
  }
  private ConfigProto() {
    sessionInterOpThreadPool_ = java.util.Collections.emptyList();
    deviceFilters_ = com.google.protobuf.LazyStringArrayList.EMPTY;
  }

  @java.lang.Override
  @SuppressWarnings({"unused"})
  protected java.lang.Object newInstance(
      UnusedPrivateParameter unused) {
    return new ConfigProto();
  }

  @java.lang.Override
  public final com.google.protobuf.UnknownFieldSet
  getUnknownFields() {
    return this.unknownFields;
  }
  private ConfigProto(
      com.google.protobuf.CodedInputStream input,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws com.google.protobuf.InvalidProtocolBufferException {
    this();
    if (extensionRegistry == null) {
      throw new java.lang.NullPointerException();
    }
    int mutable_bitField0_ = 0;
    com.google.protobuf.UnknownFieldSet.Builder unknownFields =
        com.google.protobuf.UnknownFieldSet.newBuilder();
    try {
      boolean done = false;
      while (!done) {
        int tag = input.readTag();
        switch (tag) {
          case 0:
            done = true;
            break;
          case 10: {
            if (!((mutable_bitField0_ & 0x00000001) != 0)) {
              deviceCount_ = com.google.protobuf.MapField.newMapField(
                  DeviceCountDefaultEntryHolder.defaultEntry);
              mutable_bitField0_ |= 0x00000001;
            }
            com.google.protobuf.MapEntry<java.lang.String, java.lang.Integer>
            deviceCount__ = input.readMessage(
                DeviceCountDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
            deviceCount_.getMutableMap().put(
                deviceCount__.getKey(), deviceCount__.getValue());
            break;
          }
          case 16: {

            intraOpParallelismThreads_ = input.readInt32();
            break;
          }
          case 24: {

            placementPeriod_ = input.readInt32();
            break;
          }
          case 34: {
            java.lang.String s = input.readStringRequireUtf8();
            if (!((mutable_bitField0_ & 0x00000004) != 0)) {
              deviceFilters_ = new com.google.protobuf.LazyStringArrayList();
              mutable_bitField0_ |= 0x00000004;
            }
            deviceFilters_.add(s);
            break;
          }
          case 40: {

            interOpParallelismThreads_ = input.readInt32();
            break;
          }
          case 50: {
            org.tensorflow.proto.framework.GPUOptions.Builder subBuilder = null;
            if (gpuOptions_ != null) {
              subBuilder = gpuOptions_.toBuilder();
            }
            gpuOptions_ = input.readMessage(org.tensorflow.proto.framework.GPUOptions.parser(), extensionRegistry);
            if (subBuilder != null) {
              subBuilder.mergeFrom(gpuOptions_);
              gpuOptions_ = subBuilder.buildPartial();
            }

            break;
          }
          case 56: {

            allowSoftPlacement_ = input.readBool();
            break;
          }
          case 64: {

            logDevicePlacement_ = input.readBool();
            break;
          }
          case 72: {

            usePerSessionThreads_ = input.readBool();
            break;
          }
          case 82: {
            org.tensorflow.proto.framework.GraphOptions.Builder subBuilder = null;
            if (graphOptions_ != null) {
              subBuilder = graphOptions_.toBuilder();
            }
            graphOptions_ = input.readMessage(org.tensorflow.proto.framework.GraphOptions.parser(), extensionRegistry);
            if (subBuilder != null) {
              subBuilder.mergeFrom(graphOptions_);
              graphOptions_ = subBuilder.buildPartial();
            }

            break;
          }
          case 88: {

            operationTimeoutInMs_ = input.readInt64();
            break;
          }
          case 98: {
            if (!((mutable_bitField0_ & 0x00000002) != 0)) {
              sessionInterOpThreadPool_ = new java.util.ArrayList<org.tensorflow.proto.framework.ThreadPoolOptionProto>();
              mutable_bitField0_ |= 0x00000002;
            }
            sessionInterOpThreadPool_.add(
                input.readMessage(org.tensorflow.proto.framework.ThreadPoolOptionProto.parser(), extensionRegistry));
            break;
          }
          case 106: {
            org.tensorflow.proto.framework.RPCOptions.Builder subBuilder = null;
            if (rpcOptions_ != null) {
              subBuilder = rpcOptions_.toBuilder();
            }
            rpcOptions_ = input.readMessage(org.tensorflow.proto.framework.RPCOptions.parser(), extensionRegistry);
            if (subBuilder != null) {
              subBuilder.mergeFrom(rpcOptions_);
              rpcOptions_ = subBuilder.buildPartial();
            }

            break;
          }
          case 114: {
            org.tensorflow.proto.distruntime.ClusterDef.Builder subBuilder = null;
            if (clusterDef_ != null) {
              subBuilder = clusterDef_.toBuilder();
            }
            clusterDef_ = input.readMessage(org.tensorflow.proto.distruntime.ClusterDef.parser(), extensionRegistry);
            if (subBuilder != null) {
              subBuilder.mergeFrom(clusterDef_);
              clusterDef_ = subBuilder.buildPartial();
            }

            break;
          }
          case 120: {

            isolateSessionState_ = input.readBool();
            break;
          }
          case 130: {
            org.tensorflow.proto.framework.ConfigProto.Experimental.Builder subBuilder = null;
            if (experimental_ != null) {
              subBuilder = experimental_.toBuilder();
            }
            experimental_ = input.readMessage(org.tensorflow.proto.framework.ConfigProto.Experimental.parser(), extensionRegistry);
            if (subBuilder != null) {
              subBuilder.mergeFrom(experimental_);
              experimental_ = subBuilder.buildPartial();
            }

            break;
          }
          case 136: {

            shareClusterDevicesInSession_ = input.readBool();
            break;
          }
          default: {
            if (!parseUnknownField(
                input, unknownFields, extensionRegistry, tag)) {
              done = true;
            }
            break;
          }
        }
      }
    } catch (com.google.protobuf.InvalidProtocolBufferException e) {
      throw e.setUnfinishedMessage(this);
    } catch (java.io.IOException e) {
      throw new com.google.protobuf.InvalidProtocolBufferException(
          e).setUnfinishedMessage(this);
    } finally {
      if (((mutable_bitField0_ & 0x00000004) != 0)) {
        deviceFilters_ = deviceFilters_.getUnmodifiableView();
      }
      if (((mutable_bitField0_ & 0x00000002) != 0)) {
        sessionInterOpThreadPool_ = java.util.Collections.unmodifiableList(sessionInterOpThreadPool_);
      }
      this.unknownFields = unknownFields.build();
      makeExtensionsImmutable();
    }
  }
  public static final com.google.protobuf.Descriptors.Descriptor
      getDescriptor() {
    return org.tensorflow.proto.framework.ConfigProtos.internal_static_tensorflow_ConfigProto_descriptor;
  }

  @SuppressWarnings({"rawtypes"})
  @java.lang.Override
  protected com.google.protobuf.MapField internalGetMapField(
      int number) {
    switch (number) {
      case 1:
        return internalGetDeviceCount();
      default:
        throw new RuntimeException(
            "Invalid map field number: " + number);
    }
  }
  @java.lang.Override
  protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internalGetFieldAccessorTable() {
    return org.tensorflow.proto.framework.ConfigProtos.internal_static_tensorflow_ConfigProto_fieldAccessorTable
        .ensureFieldAccessorsInitialized(
            org.tensorflow.proto.framework.ConfigProto.class, org.tensorflow.proto.framework.ConfigProto.Builder.class);
  }

  public interface ExperimentalOrBuilder extends
      // @@protoc_insertion_point(interface_extends:tensorflow.ConfigProto.Experimental)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     * Task name for group resolution.
     * </pre>
     *
     * <code>string collective_group_leader = 1;</code>
     */
    java.lang.String getCollectiveGroupLeader();
    /**
     * <pre>
     * Task name for group resolution.
     * </pre>
     *
     * <code>string collective_group_leader = 1;</code>
     */
    com.google.protobuf.ByteString
        getCollectiveGroupLeaderBytes();

    /**
     * <pre>
     * Which executor to use, the default executor will be used
     * if it is an empty string or "DEFAULT"
     * </pre>
     *
     * <code>string executor_type = 3;</code>
     */
    java.lang.String getExecutorType();
    /**
     * <pre>
     * Which executor to use, the default executor will be used
     * if it is an empty string or "DEFAULT"
     * </pre>
     *
     * <code>string executor_type = 3;</code>
     */
    com.google.protobuf.ByteString
        getExecutorTypeBytes();

    /**
     * <pre>
     * Guidance to formatting of large RecvBuf fields for transfer.
     * Any positive value sets the max chunk size.  0 defaults to 4096.
     * Any negative value indicates no max, i.e. one chunk only.
     * </pre>
     *
     * <code>int32 recv_buf_max_chunk = 4;</code>
     */
    int getRecvBufMaxChunk();

    /**
     * <pre>
     * If true, and supported by the platform, the runtime will attempt to
     * use NUMA affinity where applicable.  One consequence will be the
     * existence of as many CPU devices as there are available NUMA nodes.
     * </pre>
     *
     * <code>bool use_numa_affinity = 5;</code>
     */
    boolean getUseNumaAffinity();

    /**
     * <pre>
     * If true, make collective op execution order sequential and deterministic
     * for potentially concurrent collective instances.
     * </pre>
     *
     * <code>bool collective_deterministic_sequential_execution = 6;</code>
     */
    boolean getCollectiveDeterministicSequentialExecution();

    /**
     * <pre>
     * If true, use NCCL for CollectiveOps.  This feature is highly
     * experimental.
     * </pre>
     *
     * <code>bool collective_nccl = 7;</code>
     */
    boolean getCollectiveNccl();

    /**
     * <pre>
     * In the following, session state means the value of a variable, elements
     * in a hash table, or any other resource, accessible by worker sessions
     * held by a TF server.
     * When ClusterSpec propagation is enabled, the value of
     * isolate_session_state is ignored when deciding whether to share session
     * states in a TF server (for backwards compatibility reasons).
     * - If share_session_state_in_clusterspec_propagation is true, the session
     * states are shared.
     * - If share_session_state_in_clusterspec_propagation is false, session
     * states are isolated.
     * When clusterspec propagation is not used, the value of
     * share_session_state_in_clusterspec_propagation is ignored when deciding
     * whether to share session states in a TF server.
     * - If isolate_session_state is true, session states are isolated.
     * - If isolate_session_state is false, session states are shared.
     * TODO(b/129330037): Add a single API that consistently treats
     * isolate_session_state and ClusterSpec propagation.
     * </pre>
     *
     * <code>bool share_session_state_in_clusterspec_propagation = 8;</code>
     */
    boolean getShareSessionStateInClusterspecPropagation();

    /**
     * <pre>
     * If using a direct session, disable spinning while waiting for work in
     * the thread pool. This may result in higher latency for completing ops,
     * but in the case where there is a lot of spinning may result in lower
     * CPU usage.
     * </pre>
     *
     * <code>bool disable_thread_spinning = 9;</code>
     */
    boolean getDisableThreadSpinning();

    /**
     * <pre>
     * This was promoted to a non-experimental API. Please use
     * ConfigProto.share_cluster_devices_in_session instead.
     * </pre>
     *
     * <code>bool share_cluster_devices_in_session = 10;</code>
     */
    boolean getShareClusterDevicesInSession();

    /**
     * <pre>
     * Metadata about the session.
     * If set, this can be used by the runtime and the Ops for debugging,
     * monitoring, etc.
     * NOTE: This is currently used and propagated only by the direct session.
     * </pre>
     *
     * <code>.tensorflow.SessionMetadata session_metadata = 11;</code>
     */
    boolean hasSessionMetadata();
    /**
     * <pre>
     * Metadata about the session.
     * If set, this can be used by the runtime and the Ops for debugging,
     * monitoring, etc.
     * NOTE: This is currently used and propagated only by the direct session.
     * </pre>
     *
     * <code>.tensorflow.SessionMetadata session_metadata = 11;</code>
     */
    org.tensorflow.proto.framework.SessionMetadata getSessionMetadata();
    /**
     * <pre>
     * Metadata about the session.
     * If set, this can be used by the runtime and the Ops for debugging,
     * monitoring, etc.
     * NOTE: This is currently used and propagated only by the direct session.
     * </pre>
     *
     * <code>.tensorflow.SessionMetadata session_metadata = 11;</code>
     */
    org.tensorflow.proto.framework.SessionMetadataOrBuilder getSessionMetadataOrBuilder();

    /**
     * <pre>
     * If true, the session may treat the graph as being static for optimization
     * purposes.
     * If this option is set to true when a session is created, the full
     * GraphDef must be passed in a single call to Session::Create(), and
     * Session::Extend() may not be supported.
     * </pre>
     *
     * <code>bool optimize_for_static_graph = 12;</code>
     */
    boolean getOptimizeForStaticGraph();

    /**
     * <pre>
     * This field will eventually be deprecated and replaced by
     * mlir_bridge_rollout (b/166038521).
     * Whether to enable the MLIR-based TF-&gt;XLA bridge.
     * This is a replacement to the existing bridge, and not ready for
     * production usage yet.
     * If this option is set to true when a session is created, MLIR is used to
     * perform the set of graph transformations to put the graph in a form that
     * can be executed with delegation of some computations to an accelerator.
     * This builds on the model of XLA where a subset of the graph is
     * encapsulated and attached to a "compile" operation, whose result is fed
     * to an "execute" operation. The kernel for these operations is responsible
     * to lower the encapsulated graph to a particular device.
     * </pre>
     *
     * <code>bool enable_mlir_bridge = 13;</code>
     */
    boolean getEnableMlirBridge();

    /**
     * <pre>
     * This field is underdevelopment, for now use enable_mlir_bridge
     * (b/166038521).
     * Whether to enable the MLIR-based TF-&gt;XLA bridge.
     * </pre>
     *
     * <code>.tensorflow.ConfigProto.Experimental.MlirBridgeRollout mlir_bridge_rollout = 17;</code>
     */
    int getMlirBridgeRolloutValue();
    /**
     * <pre>
     * This field is underdevelopment, for now use enable_mlir_bridge
     * (b/166038521).
     * Whether to enable the MLIR-based TF-&gt;XLA bridge.
     * </pre>
     *
     * <code>.tensorflow.ConfigProto.Experimental.MlirBridgeRollout mlir_bridge_rollout = 17;</code>
     */
    org.tensorflow.proto.framework.ConfigProto.Experimental.MlirBridgeRollout getMlirBridgeRollout();

    /**
     * <pre>
     * Whether to enable the MLIR-based Graph optimizations.
     * This will become a part of standard Tensorflow graph optimization
     * pipeline, currently this is only used for gradual migration and testing
     * new passes that are replacing existing optimizations in Grappler.
     * </pre>
     *
     * <code>bool enable_mlir_graph_optimization = 16;</code>
     */
    boolean getEnableMlirGraphOptimization();

    /**
     * <pre>
     * If true, the session will not store an additional copy of the graph for
     * each subgraph.
     * If this option is set to true when a session is created, the
     * `RunOptions.output_partition_graphs` options must not be set.
     * </pre>
     *
     * <code>bool disable_output_partition_graphs = 14;</code>
     */
    boolean getDisableOutputPartitionGraphs();

    /**
     * <pre>
     * Minimum number of batches run through the XLA graph before XLA fusion
     * autotuner is enabled. Default value of zero disables the autotuner.
     * The XLA fusion autotuner can improve performance by executing a heuristic
     * search on the compiler parameters.
     * </pre>
     *
     * <code>int64 xla_fusion_autotuner_thresh = 15;</code>
     */
    long getXlaFusionAutotunerThresh();

    /**
     * <pre>
     * Whether runtime execution uses TFRT.
     * </pre>
     *
     * <code>bool use_tfrt = 18;</code>
     */
    boolean getUseTfrt();

    /**
     * <pre>
     * Whether functional control flow op lowering should be disabled. This is
     * useful when executing within a portable runtime where control flow op
     * kernels may not be loaded due to selective registration.
     * </pre>
     *
     * <code>bool disable_functional_ops_lowering = 21;</code>
     */
    boolean getDisableFunctionalOpsLowering();

    /**
     * <pre>
     * Provides a hint to XLA auto clustering to prefer forming a single large
     * cluster that encompases most of the graph.
     * </pre>
     *
     * <code>bool xla_prefer_single_graph_cluster = 22;</code>
     */
    boolean getXlaPreferSingleGraphCluster();

    /**
     * <pre>
     * Distributed coordination service configurations.
     * </pre>
     *
     * <code>.tensorflow.CoordinationServiceConfig coordination_config = 23;</code>
     */
    boolean hasCoordinationConfig();
    /**
     * <pre>
     * Distributed coordination service configurations.
     * </pre>
     *
     * <code>.tensorflow.CoordinationServiceConfig coordination_config = 23;</code>
     */
    org.tensorflow.proto.distruntime.CoordinationConfig.CoordinationServiceConfig getCoordinationConfig();
    /**
     * <pre>
     * Distributed coordination service configurations.
     * </pre>
     *
     * <code>.tensorflow.CoordinationServiceConfig coordination_config = 23;</code>
     */
    org.tensorflow.proto.distruntime.CoordinationConfig.CoordinationServiceConfigOrBuilder getCoordinationConfigOrBuilder();
  }
  /**
   * <pre>
   * Everything inside Experimental is subject to change and is not subject
   * to API stability guarantees in
   * https://www.tensorflow.org/guide/version_compat.
   * </pre>
   *
   * Protobuf type {@code tensorflow.ConfigProto.Experimental}
   */
  public  static final class Experimental extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:tensorflow.ConfigProto.Experimental)
      ExperimentalOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use Experimental.newBuilder() to construct.
    private Experimental(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private Experimental() {
      collectiveGroupLeader_ = "";
      executorType_ = "";
      mlirBridgeRollout_ = 0;
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new Experimental();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private Experimental(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();

              collectiveGroupLeader_ = s;
              break;
            }
            case 26: {
              java.lang.String s = input.readStringRequireUtf8();

              executorType_ = s;
              break;
            }
            case 32: {

              recvBufMaxChunk_ = input.readInt32();
              break;
            }
            case 40: {

              useNumaAffinity_ = input.readBool();
              break;
            }
            case 48: {

              collectiveDeterministicSequentialExecution_ = input.readBool();
              break;
            }
            case 56: {

              collectiveNccl_ = input.readBool();
              break;
            }
            case 64: {

              shareSessionStateInClusterspecPropagation_ = input.readBool();
              break;
            }
            case 72: {

              disableThreadSpinning_ = input.readBool();
              break;
            }
            case 80: {

              shareClusterDevicesInSession_ = input.readBool();
              break;
            }
            case 90: {
              org.tensorflow.proto.framework.SessionMetadata.Builder subBuilder = null;
              if (sessionMetadata_ != null) {
                subBuilder = sessionMetadata_.toBuilder();
              }
              sessionMetadata_ = input.readMessage(org.tensorflow.proto.framework.SessionMetadata.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(sessionMetadata_);
                sessionMetadata_ = subBuilder.buildPartial();
              }

              break;
            }
            case 96: {

              optimizeForStaticGraph_ = input.readBool();
              break;
            }
            case 104: {

              enableMlirBridge_ = input.readBool();
              break;
            }
            case 112: {

              disableOutputPartitionGraphs_ = input.readBool();
              break;
            }
            case 120: {

              xlaFusionAutotunerThresh_ = input.readInt64();
              break;
            }
            case 128: {

              enableMlirGraphOptimization_ = input.readBool();
              break;
            }
            case 136: {
              int rawValue = input.readEnum();

              mlirBridgeRollout_ = rawValue;
              break;
            }
            case 144: {

              useTfrt_ = input.readBool();
              break;
            }
            case 168: {

              disableFunctionalOpsLowering_ = input.readBool();
              break;
            }
            case 176: {

              xlaPreferSingleGraphCluster_ = input.readBool();
              break;
            }
            case 186: {
              org.tensorflow.proto.distruntime.CoordinationConfig.CoordinationServiceConfig.Builder subBuilder = null;
              if (coordinationConfig_ != null) {
                subBuilder = coordinationConfig_.toBuilder();
              }
              coordinationConfig_ = input.readMessage(org.tensorflow.proto.distruntime.CoordinationConfig.CoordinationServiceConfig.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(coordinationConfig_);
                coordinationConfig_ = subBuilder.buildPartial();
              }

              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.tensorflow.proto.framework.ConfigProtos.internal_static_tensorflow_ConfigProto_Experimental_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.tensorflow.proto.framework.ConfigProtos.internal_static_tensorflow_ConfigProto_Experimental_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.tensorflow.proto.framework.ConfigProto.Experimental.class, org.tensorflow.proto.framework.ConfigProto.Experimental.Builder.class);
    }

    /**
     * <pre>
     * An enum that describes the state of the MLIR bridge rollout.
     * </pre>
     *
     * Protobuf enum {@code tensorflow.ConfigProto.Experimental.MlirBridgeRollout}
     */
    public enum MlirBridgeRollout
        implements com.google.protobuf.ProtocolMessageEnum {
      /**
       * <pre>
       * If this field is left unspecified, the MLIR bridge may be selectively
       * enabled on a per graph basis.
       * </pre>
       *
       * <code>MLIR_BRIDGE_ROLLOUT_UNSPECIFIED = 0;</code>
       */
      MLIR_BRIDGE_ROLLOUT_UNSPECIFIED(0),
      /**
       * <pre>
       * Enabling the MLIR bridge enables it for all graphs in this session.
       * </pre>
       *
       * <code>MLIR_BRIDGE_ROLLOUT_ENABLED = 1;</code>
       */
      MLIR_BRIDGE_ROLLOUT_ENABLED(1),
      /**
       * <pre>
       * Disabling the MLIR bridge disables it for all graphs in this session.
       * </pre>
       *
       * <code>MLIR_BRIDGE_ROLLOUT_DISABLED = 2;</code>
       */
      MLIR_BRIDGE_ROLLOUT_DISABLED(2),
      /**
       * <pre>
       * Enable the MLIR bridge on a per graph basis based on an analysis of
       * the features used in the graph. If the features used by the graph are
       * supported by the MLIR bridge, the MLIR bridge will be used to run the
       * graph.
       * </pre>
       *
       * <code>MLIR_BRIDGE_ROLLOUT_SAFE_MODE_ENABLED = 3;</code>
       */
      MLIR_BRIDGE_ROLLOUT_SAFE_MODE_ENABLED(3),
      /**
       * <pre>
       * Enable the MLIR bridge in a fallback mode on a per graph basis based
       * on an analysis of the features used in the graph.
       * Running the MLIR bridge in the fallback mode means that it is
       * executed and it commits all the changes to the TF graph in case
       * of success. And it does not in case of failures and let the old bridge
       * to process the TF graph.
       * </pre>
       *
       * <code>MLIR_BRIDGE_ROLLOUT_SAFE_MODE_FALLBACK_ENABLED = 4;</code>
       */
      MLIR_BRIDGE_ROLLOUT_SAFE_MODE_FALLBACK_ENABLED(4),
      UNRECOGNIZED(-1),
      ;

      /**
       * <pre>
       * If this field is left unspecified, the MLIR bridge may be selectively
       * enabled on a per graph basis.
       * </pre>
       *
       * <code>MLIR_BRIDGE_ROLLOUT_UNSPECIFIED = 0;</code>
       */
      public static final int MLIR_BRIDGE_ROLLOUT_UNSPECIFIED_VALUE = 0;
      /**
       * <pre>
       * Enabling the MLIR bridge enables it for all graphs in this session.
       * </pre>
       *
       * <code>MLIR_BRIDGE_ROLLOUT_ENABLED = 1;</code>
       */
      public static final int MLIR_BRIDGE_ROLLOUT_ENABLED_VALUE = 1;
      /**
       * <pre>
       * Disabling the MLIR bridge disables it for all graphs in this session.
       * </pre>
       *
       * <code>MLIR_BRIDGE_ROLLOUT_DISABLED = 2;</code>
       */
      public static final int MLIR_BRIDGE_ROLLOUT_DISABLED_VALUE = 2;
      /**
       * <pre>
       * Enable the MLIR bridge on a per graph basis based on an analysis of
       * the features used in the graph. If the features used by the graph are
       * supported by the MLIR bridge, the MLIR bridge will be used to run the
       * graph.
       * </pre>
       *
       * <code>MLIR_BRIDGE_ROLLOUT_SAFE_MODE_ENABLED = 3;</code>
       */
      public static final int MLIR_BRIDGE_ROLLOUT_SAFE_MODE_ENABLED_VALUE = 3;
      /**
       * <pre>
       * Enable the MLIR bridge in a fallback mode on a per graph basis based
       * on an analysis of the features used in the graph.
       * Running the MLIR bridge in the fallback mode means that it is
       * executed and it commits all the changes to the TF graph in case
       * of success. And it does not in case of failures and let the old bridge
       * to process the TF graph.
       * </pre>
       *
       * <code>MLIR_BRIDGE_ROLLOUT_SAFE_MODE_FALLBACK_ENABLED = 4;</code>
       */
      public static final int MLIR_BRIDGE_ROLLOUT_SAFE_MODE_FALLBACK_ENABLED_VALUE = 4;


      public final int getNumber() {
        if (this == UNRECOGNIZED) {
          throw new java.lang.IllegalArgumentException(
              "Can't get the number of an unknown enum value.");
        }
        return value;
      }

      /**
       * @deprecated Use {@link #forNumber(int)} instead.
       */
      @java.lang.Deprecated
      public static MlirBridgeRollout valueOf(int value) {
        return forNumber(value);
      }

      public static MlirBridgeRollout forNumber(int value) {
        switch (value) {
          case 0: return MLIR_BRIDGE_ROLLOUT_UNSPECIFIED;
          case 1: return MLIR_BRIDGE_ROLLOUT_ENABLED;
          case 2: return MLIR_BRIDGE_ROLLOUT_DISABLED;
          case 3: return MLIR_BRIDGE_ROLLOUT_SAFE_MODE_ENABLED;
          case 4: return MLIR_BRIDGE_ROLLOUT_SAFE_MODE_FALLBACK_ENABLED;
          default: return null;
        }
      }

      public static com.google.protobuf.Internal.EnumLiteMap<MlirBridgeRollout>
          internalGetValueMap() {
        return internalValueMap;
      }
      private static final com.google.protobuf.Internal.EnumLiteMap<
          MlirBridgeRollout> internalValueMap =
            new com.google.protobuf.Internal.EnumLiteMap<MlirBridgeRollout>() {
              public MlirBridgeRollout findValueByNumber(int number) {
                return MlirBridgeRollout.forNumber(number);
              }
            };

      public final com.google.protobuf.Descriptors.EnumValueDescriptor
          getValueDescriptor() {
        return getDescriptor().getValues().get(ordinal());
      }
      public final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptorForType() {
        return getDescriptor();
      }
      public static final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptor() {
        return org.tensorflow.proto.framework.ConfigProto.Experimental.getDescriptor().getEnumTypes().get(0);
      }

      private static final MlirBridgeRollout[] VALUES = values();

      public static MlirBridgeRollout valueOf(
          com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
        if (desc.getType() != getDescriptor()) {
          throw new java.lang.IllegalArgumentException(
            "EnumValueDescriptor is not for this type.");
        }
        if (desc.getIndex() == -1) {
          return UNRECOGNIZED;
        }
        return VALUES[desc.getIndex()];
      }

      private final int value;

      private MlirBridgeRollout(int value) {
        this.value = value;
      }

      // @@protoc_insertion_point(enum_scope:tensorflow.ConfigProto.Experimental.MlirBridgeRollout)
    }

    public static final int COLLECTIVE_GROUP_LEADER_FIELD_NUMBER = 1;
    private volatile java.lang.Object collectiveGroupLeader_;
    /**
     * <pre>
     * Task name for group resolution.
     * </pre>
     *
     * <code>string collective_group_leader = 1;</code>
     */
    public java.lang.String getCollectiveGroupLeader() {
      java.lang.Object ref = collectiveGroupLeader_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        collectiveGroupLeader_ = s;
        return s;
      }
    }
    /**
     * <pre>
     * Task name for group resolution.
     * </pre>
     *
     * <code>string collective_group_leader = 1;</code>
     */
    public com.google.protobuf.ByteString
        getCollectiveGroupLeaderBytes() {
      java.lang.Object ref = collectiveGroupLeader_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        collectiveGroupLeader_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int EXECUTOR_TYPE_FIELD_NUMBER = 3;
    private volatile java.lang.Object executorType_;
    /**
     * <pre>
     * Which executor to use, the default executor will be used
     * if it is an empty string or "DEFAULT"
     * </pre>
     *
     * <code>string executor_type = 3;</code>
     */
    public java.lang.String getExecutorType() {
      java.lang.Object ref = executorType_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        executorType_ = s;
        return s;
      }
    }
    /**
     * <pre>
     * Which executor to use, the default executor will be used
     * if it is an empty string or "DEFAULT"
     * </pre>
     *
     * <code>string executor_type = 3;</code>
     */
    public com.google.protobuf.ByteString
        getExecutorTypeBytes() {
      java.lang.Object ref = executorType_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        executorType_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int RECV_BUF_MAX_CHUNK_FIELD_NUMBER = 4;
    private int recvBufMaxChunk_;
    /**
     * <pre>
     * Guidance to formatting of large RecvBuf fields for transfer.
     * Any positive value sets the max chunk size.  0 defaults to 4096.
     * Any negative value indicates no max, i.e. one chunk only.
     * </pre>
     *
     * <code>int32 recv_buf_max_chunk = 4;</code>
     */
    public int getRecvBufMaxChunk() {
      return recvBufMaxChunk_;
    }

    public static final int USE_NUMA_AFFINITY_FIELD_NUMBER = 5;
    private boolean useNumaAffinity_;
    /**
     * <pre>
     * If true, and supported by the platform, the runtime will attempt to
     * use NUMA affinity where applicable.  One consequence will be the
     * existence of as many CPU devices as there are available NUMA nodes.
     * </pre>
     *
     * <code>bool use_numa_affinity = 5;</code>
     */
    public boolean getUseNumaAffinity() {
      return useNumaAffinity_;
    }

    public static final int COLLECTIVE_DETERMINISTIC_SEQUENTIAL_EXECUTION_FIELD_NUMBER = 6;
    private boolean collectiveDeterministicSequentialExecution_;
    /**
     * <pre>
     * If true, make collective op execution order sequential and deterministic
     * for potentially concurrent collective instances.
     * </pre>
     *
     * <code>bool collective_deterministic_sequential_execution = 6;</code>
     */
    public boolean getCollectiveDeterministicSequentialExecution() {
      return collectiveDeterministicSequentialExecution_;
    }

    public static final int COLLECTIVE_NCCL_FIELD_NUMBER = 7;
    private boolean collectiveNccl_;
    /**
     * <pre>
     * If true, use NCCL for CollectiveOps.  This feature is highly
     * experimental.
     * </pre>
     *
     * <code>bool collective_nccl = 7;</code>
     */
    public boolean getCollectiveNccl() {
      return collectiveNccl_;
    }

    public static final int SHARE_SESSION_STATE_IN_CLUSTERSPEC_PROPAGATION_FIELD_NUMBER = 8;
    private boolean shareSessionStateInClusterspecPropagation_;
    /**
     * <pre>
     * In the following, session state means the value of a variable, elements
     * in a hash table, or any other resource, accessible by worker sessions
     * held by a TF server.
     * When ClusterSpec propagation is enabled, the value of
     * isolate_session_state is ignored when deciding whether to share session
     * states in a TF server (for backwards compatibility reasons).
     * - If share_session_state_in_clusterspec_propagation is true, the session
     * states are shared.
     * - If share_session_state_in_clusterspec_propagation is false, session
     * states are isolated.
     * When clusterspec propagation is not used, the value of
     * share_session_state_in_clusterspec_propagation is ignored when deciding
     * whether to share session states in a TF server.
     * - If isolate_session_state is true, session states are isolated.
     * - If isolate_session_state is false, session states are shared.
     * TODO(b/129330037): Add a single API that consistently treats
     * isolate_session_state and ClusterSpec propagation.
     * </pre>
     *
     * <code>bool share_session_state_in_clusterspec_propagation = 8;</code>
     */
    public boolean getShareSessionStateInClusterspecPropagation() {
      return shareSessionStateInClusterspecPropagation_;
    }

    public static final int DISABLE_THREAD_SPINNING_FIELD_NUMBER = 9;
    private boolean disableThreadSpinning_;
    /**
     * <pre>
     * If using a direct session, disable spinning while waiting for work in
     * the thread pool. This may result in higher latency for completing ops,
     * but in the case where there is a lot of spinning may result in lower
     * CPU usage.
     * </pre>
     *
     * <code>bool disable_thread_spinning = 9;</code>
     */
    public boolean getDisableThreadSpinning() {
      return disableThreadSpinning_;
    }

    public static final int SHARE_CLUSTER_DEVICES_IN_SESSION_FIELD_NUMBER = 10;
    private boolean shareClusterDevicesInSession_;
    /**
     * <pre>
     * This was promoted to a non-experimental API. Please use
     * ConfigProto.share_cluster_devices_in_session instead.
     * </pre>
     *
     * <code>bool share_cluster_devices_in_session = 10;</code>
     */
    public boolean getShareClusterDevicesInSession() {
      return shareClusterDevicesInSession_;
    }

    public static final int SESSION_METADATA_FIELD_NUMBER = 11;
    private org.tensorflow.proto.framework.SessionMetadata sessionMetadata_;
    /**
     * <pre>
     * Metadata about the session.
     * If set, this can be used by the runtime and the Ops for debugging,
     * monitoring, etc.
     * NOTE: This is currently used and propagated only by the direct session.
     * </pre>
     *
     * <code>.tensorflow.SessionMetadata session_metadata = 11;</code>
     */
    public boolean hasSessionMetadata() {
      return sessionMetadata_ != null;
    }
    /**
     * <pre>
     * Metadata about the session.
     * If set, this can be used by the runtime and the Ops for debugging,
     * monitoring, etc.
     * NOTE: This is currently used and propagated only by the direct session.
     * </pre>
     *
     * <code>.tensorflow.SessionMetadata session_metadata = 11;</code>
     */
    public org.tensorflow.proto.framework.SessionMetadata getSessionMetadata() {
      return sessionMetadata_ == null ? org.tensorflow.proto.framework.SessionMetadata.getDefaultInstance() : sessionMetadata_;
    }
    /**
     * <pre>
     * Metadata about the session.
     * If set, this can be used by the runtime and the Ops for debugging,
     * monitoring, etc.
     * NOTE: This is currently used and propagated only by the direct session.
     * </pre>
     *
     * <code>.tensorflow.SessionMetadata session_metadata = 11;</code>
     */
    public org.tensorflow.proto.framework.SessionMetadataOrBuilder getSessionMetadataOrBuilder() {
      return getSessionMetadata();
    }

    public static final int OPTIMIZE_FOR_STATIC_GRAPH_FIELD_NUMBER = 12;
    private boolean optimizeForStaticGraph_;
    /**
     * <pre>
     * If true, the session may treat the graph as being static for optimization
     * purposes.
     * If this option is set to true when a session is created, the full
     * GraphDef must be passed in a single call to Session::Create(), and
     * Session::Extend() may not be supported.
     * </pre>
     *
     * <code>bool optimize_for_static_graph = 12;</code>
     */
    public boolean getOptimizeForStaticGraph() {
      return optimizeForStaticGraph_;
    }

    public static final int ENABLE_MLIR_BRIDGE_FIELD_NUMBER = 13;
    private boolean enableMlirBridge_;
    /**
     * <pre>
     * This field will eventually be deprecated and replaced by
     * mlir_bridge_rollout (b/166038521).
     * Whether to enable the MLIR-based TF-&gt;XLA bridge.
     * This is a replacement to the existing bridge, and not ready for
     * production usage yet.
     * If this option is set to true when a session is created, MLIR is used to
     * perform the set of graph transformations to put the graph in a form that
     * can be executed with delegation of some computations to an accelerator.
     * This builds on the model of XLA where a subset of the graph is
     * encapsulated and attached to a "compile" operation, whose result is fed
     * to an "execute" operation. The kernel for these operations is responsible
     * to lower the encapsulated graph to a particular device.
     * </pre>
     *
     * <code>bool enable_mlir_bridge = 13;</code>
     */
    public boolean getEnableMlirBridge() {
      return enableMlirBridge_;
    }

    public static final int MLIR_BRIDGE_ROLLOUT_FIELD_NUMBER = 17;
    private int mlirBridgeRollout_;
    /**
     * <pre>
     * This field is underdevelopment, for now use enable_mlir_bridge
     * (b/166038521).
     * Whether to enable the MLIR-based TF-&gt;XLA bridge.
     * </pre>
     *
     * <code>.tensorflow.ConfigProto.Experimental.MlirBridgeRollout mlir_bridge_rollout = 17;</code>
     */
    public int getMlirBridgeRolloutValue() {
      return mlirBridgeRollout_;
    }
    /**
     * <pre>
     * This field is underdevelopment, for now use enable_mlir_bridge
     * (b/166038521).
     * Whether to enable the MLIR-based TF-&gt;XLA bridge.
     * </pre>
     *
     * <code>.tensorflow.ConfigProto.Experimental.MlirBridgeRollout mlir_bridge_rollout = 17;</code>
     */
    public org.tensorflow.proto.framework.ConfigProto.Experimental.MlirBridgeRollout getMlirBridgeRollout() {
      @SuppressWarnings("deprecation")
      org.tensorflow.proto.framework.ConfigProto.Experimental.MlirBridgeRollout result = org.tensorflow.proto.framework.ConfigProto.Experimental.MlirBridgeRollout.valueOf(mlirBridgeRollout_);
      return result == null ? org.tensorflow.proto.framework.ConfigProto.Experimental.MlirBridgeRollout.UNRECOGNIZED : result;
    }

    public static final int ENABLE_MLIR_GRAPH_OPTIMIZATION_FIELD_NUMBER = 16;
    private boolean enableMlirGraphOptimization_;
    /**
     * <pre>
     * Whether to enable the MLIR-based Graph optimizations.
     * This will become a part of standard Tensorflow graph optimization
     * pipeline, currently this is only used for gradual migration and testing
     * new passes that are replacing existing optimizations in Grappler.
     * </pre>
     *
     * <code>bool enable_mlir_graph_optimization = 16;</code>
     */
    public boolean getEnableMlirGraphOptimization() {
      return enableMlirGraphOptimization_;
    }

    public static final int DISABLE_OUTPUT_PARTITION_GRAPHS_FIELD_NUMBER = 14;
    private boolean disableOutputPartitionGraphs_;
    /**
     * <pre>
     * If true, the session will not store an additional copy of the graph for
     * each subgraph.
     * If this option is set to true when a session is created, the
     * `RunOptions.output_partition_graphs` options must not be set.
     * </pre>
     *
     * <code>bool disable_output_partition_graphs = 14;</code>
     */
    public boolean getDisableOutputPartitionGraphs() {
      return disableOutputPartitionGraphs_;
    }

    public static final int XLA_FUSION_AUTOTUNER_THRESH_FIELD_NUMBER = 15;
    private long xlaFusionAutotunerThresh_;
    /**
     * <pre>
     * Minimum number of batches run through the XLA graph before XLA fusion
     * autotuner is enabled. Default value of zero disables the autotuner.
     * The XLA fusion autotuner can improve performance by executing a heuristic
     * search on the compiler parameters.
     * </pre>
     *
     * <code>int64 xla_fusion_autotuner_thresh = 15;</code>
     */
    public long getXlaFusionAutotunerThresh() {
      return xlaFusionAutotunerThresh_;
    }

    public static final int USE_TFRT_FIELD_NUMBER = 18;
    private boolean useTfrt_;
    /**
     * <pre>
     * Whether runtime execution uses TFRT.
     * </pre>
     *
     * <code>bool use_tfrt = 18;</code>
     */
    public boolean getUseTfrt() {
      return useTfrt_;
    }

    public static final int DISABLE_FUNCTIONAL_OPS_LOWERING_FIELD_NUMBER = 21;
    private boolean disableFunctionalOpsLowering_;
    /**
     * <pre>
     * Whether functional control flow op lowering should be disabled. This is
     * useful when executing within a portable runtime where control flow op
     * kernels may not be loaded due to selective registration.
     * </pre>
     *
     * <code>bool disable_functional_ops_lowering = 21;</code>
     */
    public boolean getDisableFunctionalOpsLowering() {
      return disableFunctionalOpsLowering_;
    }

    public static final int XLA_PREFER_SINGLE_GRAPH_CLUSTER_FIELD_NUMBER = 22;
    private boolean xlaPreferSingleGraphCluster_;
    /**
     * <pre>
     * Provides a hint to XLA auto clustering to prefer forming a single large
     * cluster that encompases most of the graph.
     * </pre>
     *
     * <code>bool xla_prefer_single_graph_cluster = 22;</code>
     */
    public boolean getXlaPreferSingleGraphCluster() {
      return xlaPreferSingleGraphCluster_;
    }

    public static final int COORDINATION_CONFIG_FIELD_NUMBER = 23;
    private org.tensorflow.proto.distruntime.CoordinationConfig.CoordinationServiceConfig coordinationConfig_;
    /**
     * <pre>
     * Distributed coordination service configurations.
     * </pre>
     *
     * <code>.tensorflow.CoordinationServiceConfig coordination_config = 23;</code>
     */
    public boolean hasCoordinationConfig() {
      return coordinationConfig_ != null;
    }
    /**
     * <pre>
     * Distributed coordination service configurations.
     * </pre>
     *
     * <code>.tensorflow.CoordinationServiceConfig coordination_config = 23;</code>
     */
    public org.tensorflow.proto.distruntime.CoordinationConfig.CoordinationServiceConfig getCoordinationConfig() {
      return coordinationConfig_ == null ? org.tensorflow.proto.distruntime.CoordinationConfig.CoordinationServiceConfig.getDefaultInstance() : coordinationConfig_;
    }
    /**
     * <pre>
     * Distributed coordination service configurations.
     * </pre>
     *
     * <code>.tensorflow.CoordinationServiceConfig coordination_config = 23;</code>
     */
    public org.tensorflow.proto.distruntime.CoordinationConfig.CoordinationServiceConfigOrBuilder getCoordinationConfigOrBuilder() {
      return getCoordinationConfig();
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (!getCollectiveGroupLeaderBytes().isEmpty()) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, collectiveGroupLeader_);
      }
      if (!getExecutorTypeBytes().isEmpty()) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 3, executorType_);
      }
      if (recvBufMaxChunk_ != 0) {
        output.writeInt32(4, recvBufMaxChunk_);
      }
      if (useNumaAffinity_ != false) {
        output.writeBool(5, useNumaAffinity_);
      }
      if (collectiveDeterministicSequentialExecution_ != false) {
        output.writeBool(6, collectiveDeterministicSequentialExecution_);
      }
      if (collectiveNccl_ != false) {
        output.writeBool(7, collectiveNccl_);
      }
      if (shareSessionStateInClusterspecPropagation_ != false) {
        output.writeBool(8, shareSessionStateInClusterspecPropagation_);
      }
      if (disableThreadSpinning_ != false) {
        output.writeBool(9, disableThreadSpinning_);
      }
      if (shareClusterDevicesInSession_ != false) {
        output.writeBool(10, shareClusterDevicesInSession_);
      }
      if (sessionMetadata_ != null) {
        output.writeMessage(11, getSessionMetadata());
      }
      if (optimizeForStaticGraph_ != false) {
        output.writeBool(12, optimizeForStaticGraph_);
      }
      if (enableMlirBridge_ != false) {
        output.writeBool(13, enableMlirBridge_);
      }
      if (disableOutputPartitionGraphs_ != false) {
        output.writeBool(14, disableOutputPartitionGraphs_);
      }
      if (xlaFusionAutotunerThresh_ != 0L) {
        output.writeInt64(15, xlaFusionAutotunerThresh_);
      }
      if (enableMlirGraphOptimization_ != false) {
        output.writeBool(16, enableMlirGraphOptimization_);
      }
      if (mlirBridgeRollout_ != org.tensorflow.proto.framework.ConfigProto.Experimental.MlirBridgeRollout.MLIR_BRIDGE_ROLLOUT_UNSPECIFIED.getNumber()) {
        output.writeEnum(17, mlirBridgeRollout_);
      }
      if (useTfrt_ != false) {
        output.writeBool(18, useTfrt_);
      }
      if (disableFunctionalOpsLowering_ != false) {
        output.writeBool(21, disableFunctionalOpsLowering_);
      }
      if (xlaPreferSingleGraphCluster_ != false) {
        output.writeBool(22, xlaPreferSingleGraphCluster_);
      }
      if (coordinationConfig_ != null) {
        output.writeMessage(23, getCoordinationConfig());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (!getCollectiveGroupLeaderBytes().isEmpty()) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, collectiveGroupLeader_);
      }
      if (!getExecutorTypeBytes().isEmpty()) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(3, executorType_);
      }
      if (recvBufMaxChunk_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(4, recvBufMaxChunk_);
      }
      if (useNumaAffinity_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(5, useNumaAffinity_);
      }
      if (collectiveDeterministicSequentialExecution_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(6, collectiveDeterministicSequentialExecution_);
      }
      if (collectiveNccl_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(7, collectiveNccl_);
      }
      if (shareSessionStateInClusterspecPropagation_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(8, shareSessionStateInClusterspecPropagation_);
      }
      if (disableThreadSpinning_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(9, disableThreadSpinning_);
      }
      if (shareClusterDevicesInSession_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(10, shareClusterDevicesInSession_);
      }
      if (sessionMetadata_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(11, getSessionMetadata());
      }
      if (optimizeForStaticGraph_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(12, optimizeForStaticGraph_);
      }
      if (enableMlirBridge_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(13, enableMlirBridge_);
      }
      if (disableOutputPartitionGraphs_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(14, disableOutputPartitionGraphs_);
      }
      if (xlaFusionAutotunerThresh_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(15, xlaFusionAutotunerThresh_);
      }
      if (enableMlirGraphOptimization_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(16, enableMlirGraphOptimization_);
      }
      if (mlirBridgeRollout_ != org.tensorflow.proto.framework.ConfigProto.Experimental.MlirBridgeRollout.MLIR_BRIDGE_ROLLOUT_UNSPECIFIED.getNumber()) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(17, mlirBridgeRollout_);
      }
      if (useTfrt_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(18, useTfrt_);
      }
      if (disableFunctionalOpsLowering_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(21, disableFunctionalOpsLowering_);
      }
      if (xlaPreferSingleGraphCluster_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(22, xlaPreferSingleGraphCluster_);
      }
      if (coordinationConfig_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(23, getCoordinationConfig());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.tensorflow.proto.framework.ConfigProto.Experimental)) {
        return super.equals(obj);
      }
      org.tensorflow.proto.framework.ConfigProto.Experimental other = (org.tensorflow.proto.framework.ConfigProto.Experimental) obj;

      if (!getCollectiveGroupLeader()
          .equals(other.getCollectiveGroupLeader())) return false;
      if (!getExecutorType()
          .equals(other.getExecutorType())) return false;
      if (getRecvBufMaxChunk()
          != other.getRecvBufMaxChunk()) return false;
      if (getUseNumaAffinity()
          != other.getUseNumaAffinity()) return false;
      if (getCollectiveDeterministicSequentialExecution()
          != other.getCollectiveDeterministicSequentialExecution()) return false;
      if (getCollectiveNccl()
          != other.getCollectiveNccl()) return false;
      if (getShareSessionStateInClusterspecPropagation()
          != other.getShareSessionStateInClusterspecPropagation()) return false;
      if (getDisableThreadSpinning()
          != other.getDisableThreadSpinning()) return false;
      if (getShareClusterDevicesInSession()
          != other.getShareClusterDevicesInSession()) return false;
      if (hasSessionMetadata() != other.hasSessionMetadata()) return false;
      if (hasSessionMetadata()) {
        if (!getSessionMetadata()
            .equals(other.getSessionMetadata())) return false;
      }
      if (getOptimizeForStaticGraph()
          != other.getOptimizeForStaticGraph()) return false;
      if (getEnableMlirBridge()
          != other.getEnableMlirBridge()) return false;
      if (mlirBridgeRollout_ != other.mlirBridgeRollout_) return false;
      if (getEnableMlirGraphOptimization()
          != other.getEnableMlirGraphOptimization()) return false;
      if (getDisableOutputPartitionGraphs()
          != other.getDisableOutputPartitionGraphs()) return false;
      if (getXlaFusionAutotunerThresh()
          != other.getXlaFusionAutotunerThresh()) return false;
      if (getUseTfrt()
          != other.getUseTfrt()) return false;
      if (getDisableFunctionalOpsLowering()
          != other.getDisableFunctionalOpsLowering()) return false;
      if (getXlaPreferSingleGraphCluster()
          != other.getXlaPreferSingleGraphCluster()) return false;
      if (hasCoordinationConfig() != other.hasCoordinationConfig()) return false;
      if (hasCoordinationConfig()) {
        if (!getCoordinationConfig()
            .equals(other.getCoordinationConfig())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + COLLECTIVE_GROUP_LEADER_FIELD_NUMBER;
      hash = (53 * hash) + getCollectiveGroupLeader().hashCode();
      hash = (37 * hash) + EXECUTOR_TYPE_FIELD_NUMBER;
      hash = (53 * hash) + getExecutorType().hashCode();
      hash = (37 * hash) + RECV_BUF_MAX_CHUNK_FIELD_NUMBER;
      hash = (53 * hash) + getRecvBufMaxChunk();
      hash = (37 * hash) + USE_NUMA_AFFINITY_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getUseNumaAffinity());
      hash = (37 * hash) + COLLECTIVE_DETERMINISTIC_SEQUENTIAL_EXECUTION_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getCollectiveDeterministicSequentialExecution());
      hash = (37 * hash) + COLLECTIVE_NCCL_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getCollectiveNccl());
      hash = (37 * hash) + SHARE_SESSION_STATE_IN_CLUSTERSPEC_PROPAGATION_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getShareSessionStateInClusterspecPropagation());
      hash = (37 * hash) + DISABLE_THREAD_SPINNING_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getDisableThreadSpinning());
      hash = (37 * hash) + SHARE_CLUSTER_DEVICES_IN_SESSION_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getShareClusterDevicesInSession());
      if (hasSessionMetadata()) {
        hash = (37 * hash) + SESSION_METADATA_FIELD_NUMBER;
        hash = (53 * hash) + getSessionMetadata().hashCode();
      }
      hash = (37 * hash) + OPTIMIZE_FOR_STATIC_GRAPH_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getOptimizeForStaticGraph());
      hash = (37 * hash) + ENABLE_MLIR_BRIDGE_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getEnableMlirBridge());
      hash = (37 * hash) + MLIR_BRIDGE_ROLLOUT_FIELD_NUMBER;
      hash = (53 * hash) + mlirBridgeRollout_;
      hash = (37 * hash) + ENABLE_MLIR_GRAPH_OPTIMIZATION_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getEnableMlirGraphOptimization());
      hash = (37 * hash) + DISABLE_OUTPUT_PARTITION_GRAPHS_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getDisableOutputPartitionGraphs());
      hash = (37 * hash) + XLA_FUSION_AUTOTUNER_THRESH_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getXlaFusionAutotunerThresh());
      hash = (37 * hash) + USE_TFRT_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getUseTfrt());
      hash = (37 * hash) + DISABLE_FUNCTIONAL_OPS_LOWERING_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getDisableFunctionalOpsLowering());
      hash = (37 * hash) + XLA_PREFER_SINGLE_GRAPH_CLUSTER_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getXlaPreferSingleGraphCluster());
      if (hasCoordinationConfig()) {
        hash = (37 * hash) + COORDINATION_CONFIG_FIELD_NUMBER;
        hash = (53 * hash) + getCoordinationConfig().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.tensorflow.proto.framework.ConfigProto.Experimental parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.tensorflow.proto.framework.ConfigProto.Experimental parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.tensorflow.proto.framework.ConfigProto.Experimental parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.tensorflow.proto.framework.ConfigProto.Experimental parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.tensorflow.proto.framework.ConfigProto.Experimental parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.tensorflow.proto.framework.ConfigProto.Experimental parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.tensorflow.proto.framework.ConfigProto.Experimental parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.tensorflow.proto.framework.ConfigProto.Experimental parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.tensorflow.proto.framework.ConfigProto.Experimental parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static org.tensorflow.proto.framework.ConfigProto.Experimental parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.tensorflow.proto.framework.ConfigProto.Experimental parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.tensorflow.proto.framework.ConfigProto.Experimental parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.tensorflow.proto.framework.ConfigProto.Experimental prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     * Everything inside Experimental is subject to change and is not subject
     * to API stability guarantees in
     * https://www.tensorflow.org/guide/version_compat.
     * </pre>
     *
     * Protobuf type {@code tensorflow.ConfigProto.Experimental}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:tensorflow.ConfigProto.Experimental)
        org.tensorflow.proto.framework.ConfigProto.ExperimentalOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.tensorflow.proto.framework.ConfigProtos.internal_static_tensorflow_ConfigProto_Experimental_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.tensorflow.proto.framework.ConfigProtos.internal_static_tensorflow_ConfigProto_Experimental_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.tensorflow.proto.framework.ConfigProto.Experimental.class, org.tensorflow.proto.framework.ConfigProto.Experimental.Builder.class);
      }

      // Construct using org.tensorflow.proto.framework.ConfigProto.Experimental.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        collectiveGroupLeader_ = "";

        executorType_ = "";

        recvBufMaxChunk_ = 0;

        useNumaAffinity_ = false;

        collectiveDeterministicSequentialExecution_ = false;

        collectiveNccl_ = false;

        shareSessionStateInClusterspecPropagation_ = false;

        disableThreadSpinning_ = false;

        shareClusterDevicesInSession_ = false;

        if (sessionMetadataBuilder_ == null) {
          sessionMetadata_ = null;
        } else {
          sessionMetadata_ = null;
          sessionMetadataBuilder_ = null;
        }
        optimizeForStaticGraph_ = false;

        enableMlirBridge_ = false;

        mlirBridgeRollout_ = 0;

        enableMlirGraphOptimization_ = false;

        disableOutputPartitionGraphs_ = false;

        xlaFusionAutotunerThresh_ = 0L;

        useTfrt_ = false;

        disableFunctionalOpsLowering_ = false;

        xlaPreferSingleGraphCluster_ = false;

        if (coordinationConfigBuilder_ == null) {
          coordinationConfig_ = null;
        } else {
          coordinationConfig_ = null;
          coordinationConfigBuilder_ = null;
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.tensorflow.proto.framework.ConfigProtos.internal_static_tensorflow_ConfigProto_Experimental_descriptor;
      }

      @java.lang.Override
      public org.tensorflow.proto.framework.ConfigProto.Experimental getDefaultInstanceForType() {
        return org.tensorflow.proto.framework.ConfigProto.Experimental.getDefaultInstance();
      }

      @java.lang.Override
      public org.tensorflow.proto.framework.ConfigProto.Experimental build() {
        org.tensorflow.proto.framework.ConfigProto.Experimental result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public org.tensorflow.proto.framework.ConfigProto.Experimental buildPartial() {
        org.tensorflow.proto.framework.ConfigProto.Experimental result = new org.tensorflow.proto.framework.ConfigProto.Experimental(this);
        result.collectiveGroupLeader_ = collectiveGroupLeader_;
        result.executorType_ = executorType_;
        result.recvBufMaxChunk_ = recvBufMaxChunk_;
        result.useNumaAffinity_ = useNumaAffinity_;
        result.collectiveDeterministicSequentialExecution_ = collectiveDeterministicSequentialExecution_;
        result.collectiveNccl_ = collectiveNccl_;
        result.shareSessionStateInClusterspecPropagation_ = shareSessionStateInClusterspecPropagation_;
        result.disableThreadSpinning_ = disableThreadSpinning_;
        result.shareClusterDevicesInSession_ = shareClusterDevicesInSession_;
        if (sessionMetadataBuilder_ == null) {
          result.sessionMetadata_ = sessionMetadata_;
        } else {
          result.sessionMetadata_ = sessionMetadataBuilder_.build();
        }
        result.optimizeForStaticGraph_ = optimizeForStaticGraph_;
        result.enableMlirBridge_ = enableMlirBridge_;
        result.mlirBridgeRollout_ = mlirBridgeRollout_;
        result.enableMlirGraphOptimization_ = enableMlirGraphOptimization_;
        result.disableOutputPartitionGraphs_ = disableOutputPartitionGraphs_;
        result.xlaFusionAutotunerThresh_ = xlaFusionAutotunerThresh_;
        result.useTfrt_ = useTfrt_;
        result.disableFunctionalOpsLowering_ = disableFunctionalOpsLowering_;
        result.xlaPreferSingleGraphCluster_ = xlaPreferSingleGraphCluster_;
        if (coordinationConfigBuilder_ == null) {
          result.coordinationConfig_ = coordinationConfig_;
        } else {
          result.coordinationConfig_ = coordinationConfigBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.tensorflow.proto.framework.ConfigProto.Experimental) {
          return mergeFrom((org.tensorflow.proto.framework.ConfigProto.Experimental)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.tensorflow.proto.framework.ConfigProto.Experimental other) {
        if (other == org.tensorflow.proto.framework.ConfigProto.Experimental.getDefaultInstance()) return this;
        if (!other.getCollectiveGroupLeader().isEmpty()) {
          collectiveGroupLeader_ = other.collectiveGroupLeader_;
          onChanged();
        }
        if (!other.getExecutorType().isEmpty()) {
          executorType_ = other.executorType_;
          onChanged();
        }
        if (other.getRecvBufMaxChunk() != 0) {
          setRecvBufMaxChunk(other.getRecvBufMaxChunk());
        }
        if (other.getUseNumaAffinity() != false) {
          setUseNumaAffinity(other.getUseNumaAffinity());
        }
        if (other.getCollectiveDeterministicSequentialExecution() != false) {
          setCollectiveDeterministicSequentialExecution(other.getCollectiveDeterministicSequentialExecution());
        }
        if (other.getCollectiveNccl() != false) {
          setCollectiveNccl(other.getCollectiveNccl());
        }
        if (other.getShareSessionStateInClusterspecPropagation() != false) {
          setShareSessionStateInClusterspecPropagation(other.getShareSessionStateInClusterspecPropagation());
        }
        if (other.getDisableThreadSpinning() != false) {
          setDisableThreadSpinning(other.getDisableThreadSpinning());
        }
        if (other.getShareClusterDevicesInSession() != false) {
          setShareClusterDevicesInSession(other.getShareClusterDevicesInSession());
        }
        if (other.hasSessionMetadata()) {
          mergeSessionMetadata(other.getSessionMetadata());
        }
        if (other.getOptimizeForStaticGraph() != false) {
          setOptimizeForStaticGraph(other.getOptimizeForStaticGraph());
        }
        if (other.getEnableMlirBridge() != false) {
          setEnableMlirBridge(other.getEnableMlirBridge());
        }
        if (other.mlirBridgeRollout_ != 0) {
          setMlirBridgeRolloutValue(other.getMlirBridgeRolloutValue());
        }
        if (other.getEnableMlirGraphOptimization() != false) {
          setEnableMlirGraphOptimization(other.getEnableMlirGraphOptimization());
        }
        if (other.getDisableOutputPartitionGraphs() != false) {
          setDisableOutputPartitionGraphs(other.getDisableOutputPartitionGraphs());
        }
        if (other.getXlaFusionAutotunerThresh() != 0L) {
          setXlaFusionAutotunerThresh(other.getXlaFusionAutotunerThresh());
        }
        if (other.getUseTfrt() != false) {
          setUseTfrt(other.getUseTfrt());
        }
        if (other.getDisableFunctionalOpsLowering() != false) {
          setDisableFunctionalOpsLowering(other.getDisableFunctionalOpsLowering());
        }
        if (other.getXlaPreferSingleGraphCluster() != false) {
          setXlaPreferSingleGraphCluster(other.getXlaPreferSingleGraphCluster());
        }
        if (other.hasCoordinationConfig()) {
          mergeCoordinationConfig(other.getCoordinationConfig());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.tensorflow.proto.framework.ConfigProto.Experimental parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.tensorflow.proto.framework.ConfigProto.Experimental) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private java.lang.Object collectiveGroupLeader_ = "";
      /**
       * <pre>
       * Task name for group resolution.
       * </pre>
       *
       * <code>string collective_group_leader = 1;</code>
       */
      public java.lang.String getCollectiveGroupLeader() {
        java.lang.Object ref = collectiveGroupLeader_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          collectiveGroupLeader_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <pre>
       * Task name for group resolution.
       * </pre>
       *
       * <code>string collective_group_leader = 1;</code>
       */
      public com.google.protobuf.ByteString
          getCollectiveGroupLeaderBytes() {
        java.lang.Object ref = collectiveGroupLeader_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          collectiveGroupLeader_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <pre>
       * Task name for group resolution.
       * </pre>
       *
       * <code>string collective_group_leader = 1;</code>
       */
      public Builder setCollectiveGroupLeader(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        collectiveGroupLeader_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * Task name for group resolution.
       * </pre>
       *
       * <code>string collective_group_leader = 1;</code>
       */
      public Builder clearCollectiveGroupLeader() {
        
        collectiveGroupLeader_ = getDefaultInstance().getCollectiveGroupLeader();
        onChanged();
        return this;
      }
      /**
       * <pre>
       * Task name for group resolution.
       * </pre>
       *
       * <code>string collective_group_leader = 1;</code>
       */
      public Builder setCollectiveGroupLeaderBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        collectiveGroupLeader_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object executorType_ = "";
      /**
       * <pre>
       * Which executor to use, the default executor will be used
       * if it is an empty string or "DEFAULT"
       * </pre>
       *
       * <code>string executor_type = 3;</code>
       */
      public java.lang.String getExecutorType() {
        java.lang.Object ref = executorType_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          executorType_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <pre>
       * Which executor to use, the default executor will be used
       * if it is an empty string or "DEFAULT"
       * </pre>
       *
       * <code>string executor_type = 3;</code>
       */
      public com.google.protobuf.ByteString
          getExecutorTypeBytes() {
        java.lang.Object ref = executorType_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          executorType_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <pre>
       * Which executor to use, the default executor will be used
       * if it is an empty string or "DEFAULT"
       * </pre>
       *
       * <code>string executor_type = 3;</code>
       */
      public Builder setExecutorType(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        executorType_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * Which executor to use, the default executor will be used
       * if it is an empty string or "DEFAULT"
       * </pre>
       *
       * <code>string executor_type = 3;</code>
       */
      public Builder clearExecutorType() {
        
        executorType_ = getDefaultInstance().getExecutorType();
        onChanged();
        return this;
      }
      /**
       * <pre>
       * Which executor to use, the default executor will be used
       * if it is an empty string or "DEFAULT"
       * </pre>
       *
       * <code>string executor_type = 3;</code>
       */
      public Builder setExecutorTypeBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        executorType_ = value;
        onChanged();
        return this;
      }

      private int recvBufMaxChunk_ ;
      /**
       * <pre>
       * Guidance to formatting of large RecvBuf fields for transfer.
       * Any positive value sets the max chunk size.  0 defaults to 4096.
       * Any negative value indicates no max, i.e. one chunk only.
       * </pre>
       *
       * <code>int32 recv_buf_max_chunk = 4;</code>
       */
      public int getRecvBufMaxChunk() {
        return recvBufMaxChunk_;
      }
      /**
       * <pre>
       * Guidance to formatting of large RecvBuf fields for transfer.
       * Any positive value sets the max chunk size.  0 defaults to 4096.
       * Any negative value indicates no max, i.e. one chunk only.
       * </pre>
       *
       * <code>int32 recv_buf_max_chunk = 4;</code>
       */
      public Builder setRecvBufMaxChunk(int value) {
        
        recvBufMaxChunk_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * Guidance to formatting of large RecvBuf fields for transfer.
       * Any positive value sets the max chunk size.  0 defaults to 4096.
       * Any negative value indicates no max, i.e. one chunk only.
       * </pre>
       *
       * <code>int32 recv_buf_max_chunk = 4;</code>
       */
      public Builder clearRecvBufMaxChunk() {
        
        recvBufMaxChunk_ = 0;
        onChanged();
        return this;
      }

      private boolean useNumaAffinity_ ;
      /**
       * <pre>
       * If true, and supported by the platform, the runtime will attempt to
       * use NUMA affinity where applicable.  One consequence will be the
       * existence of as many CPU devices as there are available NUMA nodes.
       * </pre>
       *
       * <code>bool use_numa_affinity = 5;</code>
       */
      public boolean getUseNumaAffinity() {
        return useNumaAffinity_;
      }
      /**
       * <pre>
       * If true, and supported by the platform, the runtime will attempt to
       * use NUMA affinity where applicable.  One consequence will be the
       * existence of as many CPU devices as there are available NUMA nodes.
       * </pre>
       *
       * <code>bool use_numa_affinity = 5;</code>
       */
      public Builder setUseNumaAffinity(boolean value) {
        
        useNumaAffinity_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * If true, and supported by the platform, the runtime will attempt to
       * use NUMA affinity where applicable.  One consequence will be the
       * existence of as many CPU devices as there are available NUMA nodes.
       * </pre>
       *
       * <code>bool use_numa_affinity = 5;</code>
       */
      public Builder clearUseNumaAffinity() {
        
        useNumaAffinity_ = false;
        onChanged();
        return this;
      }

      private boolean collectiveDeterministicSequentialExecution_ ;
      /**
       * <pre>
       * If true, make collective op execution order sequential and deterministic
       * for potentially concurrent collective instances.
       * </pre>
       *
       * <code>bool collective_deterministic_sequential_execution = 6;</code>
       */
      public boolean getCollectiveDeterministicSequentialExecution() {
        return collectiveDeterministicSequentialExecution_;
      }
      /**
       * <pre>
       * If true, make collective op execution order sequential and deterministic
       * for potentially concurrent collective instances.
       * </pre>
       *
       * <code>bool collective_deterministic_sequential_execution = 6;</code>
       */
      public Builder setCollectiveDeterministicSequentialExecution(boolean value) {
        
        collectiveDeterministicSequentialExecution_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * If true, make collective op execution order sequential and deterministic
       * for potentially concurrent collective instances.
       * </pre>
       *
       * <code>bool collective_deterministic_sequential_execution = 6;</code>
       */
      public Builder clearCollectiveDeterministicSequentialExecution() {
        
        collectiveDeterministicSequentialExecution_ = false;
        onChanged();
        return this;
      }

      private boolean collectiveNccl_ ;
      /**
       * <pre>
       * If true, use NCCL for CollectiveOps.  This feature is highly
       * experimental.
       * </pre>
       *
       * <code>bool collective_nccl = 7;</code>
       */
      public boolean getCollectiveNccl() {
        return collectiveNccl_;
      }
      /**
       * <pre>
       * If true, use NCCL for CollectiveOps.  This feature is highly
       * experimental.
       * </pre>
       *
       * <code>bool collective_nccl = 7;</code>
       */
      public Builder setCollectiveNccl(boolean value) {
        
        collectiveNccl_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * If true, use NCCL for CollectiveOps.  This feature is highly
       * experimental.
       * </pre>
       *
       * <code>bool collective_nccl = 7;</code>
       */
      public Builder clearCollectiveNccl() {
        
        collectiveNccl_ = false;
        onChanged();
        return this;
      }

      private boolean shareSessionStateInClusterspecPropagation_ ;
      /**
       * <pre>
       * In the following, session state means the value of a variable, elements
       * in a hash table, or any other resource, accessible by worker sessions
       * held by a TF server.
       * When ClusterSpec propagation is enabled, the value of
       * isolate_session_state is ignored when deciding whether to share session
       * states in a TF server (for backwards compatibility reasons).
       * - If share_session_state_in_clusterspec_propagation is true, the session
       * states are shared.
       * - If share_session_state_in_clusterspec_propagation is false, session
       * states are isolated.
       * When clusterspec propagation is not used, the value of
       * share_session_state_in_clusterspec_propagation is ignored when deciding
       * whether to share session states in a TF server.
       * - If isolate_session_state is true, session states are isolated.
       * - If isolate_session_state is false, session states are shared.
       * TODO(b/129330037): Add a single API that consistently treats
       * isolate_session_state and ClusterSpec propagation.
       * </pre>
       *
       * <code>bool share_session_state_in_clusterspec_propagation = 8;</code>
       */
      public boolean getShareSessionStateInClusterspecPropagation() {
        return shareSessionStateInClusterspecPropagation_;
      }
      /**
       * <pre>
       * In the following, session state means the value of a variable, elements
       * in a hash table, or any other resource, accessible by worker sessions
       * held by a TF server.
       * When ClusterSpec propagation is enabled, the value of
       * isolate_session_state is ignored when deciding whether to share session
       * states in a TF server (for backwards compatibility reasons).
       * - If share_session_state_in_clusterspec_propagation is true, the session
       * states are shared.
       * - If share_session_state_in_clusterspec_propagation is false, session
       * states are isolated.
       * When clusterspec propagation is not used, the value of
       * share_session_state_in_clusterspec_propagation is ignored when deciding
       * whether to share session states in a TF server.
       * - If isolate_session_state is true, session states are isolated.
       * - If isolate_session_state is false, session states are shared.
       * TODO(b/129330037): Add a single API that consistently treats
       * isolate_session_state and ClusterSpec propagation.
       * </pre>
       *
       * <code>bool share_session_state_in_clusterspec_propagation = 8;</code>
       */
      public Builder setShareSessionStateInClusterspecPropagation(boolean value) {
        
        shareSessionStateInClusterspecPropagation_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * In the following, session state means the value of a variable, elements
       * in a hash table, or any other resource, accessible by worker sessions
       * held by a TF server.
       * When ClusterSpec propagation is enabled, the value of
       * isolate_session_state is ignored when deciding whether to share session
       * states in a TF server (for backwards compatibility reasons).
       * - If share_session_state_in_clusterspec_propagation is true, the session
       * states are shared.
       * - If share_session_state_in_clusterspec_propagation is false, session
       * states are isolated.
       * When clusterspec propagation is not used, the value of
       * share_session_state_in_clusterspec_propagation is ignored when deciding
       * whether to share session states in a TF server.
       * - If isolate_session_state is true, session states are isolated.
       * - If isolate_session_state is false, session states are shared.
       * TODO(b/129330037): Add a single API that consistently treats
       * isolate_session_state and ClusterSpec propagation.
       * </pre>
       *
       * <code>bool share_session_state_in_clusterspec_propagation = 8;</code>
       */
      public Builder clearShareSessionStateInClusterspecPropagation() {
        
        shareSessionStateInClusterspecPropagation_ = false;
        onChanged();
        return this;
      }

      private boolean disableThreadSpinning_ ;
      /**
       * <pre>
       * If using a direct session, disable spinning while waiting for work in
       * the thread pool. This may result in higher latency for completing ops,
       * but in the case where there is a lot of spinning may result in lower
       * CPU usage.
       * </pre>
       *
       * <code>bool disable_thread_spinning = 9;</code>
       */
      public boolean getDisableThreadSpinning() {
        return disableThreadSpinning_;
      }
      /**
       * <pre>
       * If using a direct session, disable spinning while waiting for work in
       * the thread pool. This may result in higher latency for completing ops,
       * but in the case where there is a lot of spinning may result in lower
       * CPU usage.
       * </pre>
       *
       * <code>bool disable_thread_spinning = 9;</code>
       */
      public Builder setDisableThreadSpinning(boolean value) {
        
        disableThreadSpinning_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * If using a direct session, disable spinning while waiting for work in
       * the thread pool. This may result in higher latency for completing ops,
       * but in the case where there is a lot of spinning may result in lower
       * CPU usage.
       * </pre>
       *
       * <code>bool disable_thread_spinning = 9;</code>
       */
      public Builder clearDisableThreadSpinning() {
        
        disableThreadSpinning_ = false;
        onChanged();
        return this;
      }

      private boolean shareClusterDevicesInSession_ ;
      /**
       * <pre>
       * This was promoted to a non-experimental API. Please use
       * ConfigProto.share_cluster_devices_in_session instead.
       * </pre>
       *
       * <code>bool share_cluster_devices_in_session = 10;</code>
       */
      public boolean getShareClusterDevicesInSession() {
        return shareClusterDevicesInSession_;
      }
      /**
       * <pre>
       * This was promoted to a non-experimental API. Please use
       * ConfigProto.share_cluster_devices_in_session instead.
       * </pre>
       *
       * <code>bool share_cluster_devices_in_session = 10;</code>
       */
      public Builder setShareClusterDevicesInSession(boolean value) {
        
        shareClusterDevicesInSession_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * This was promoted to a non-experimental API. Please use
       * ConfigProto.share_cluster_devices_in_session instead.
       * </pre>
       *
       * <code>bool share_cluster_devices_in_session = 10;</code>
       */
      public Builder clearShareClusterDevicesInSession() {
        
        shareClusterDevicesInSession_ = false;
        onChanged();
        return this;
      }

      private org.tensorflow.proto.framework.SessionMetadata sessionMetadata_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.tensorflow.proto.framework.SessionMetadata, org.tensorflow.proto.framework.SessionMetadata.Builder, org.tensorflow.proto.framework.SessionMetadataOrBuilder> sessionMetadataBuilder_;
      /**
       * <pre>
       * Metadata about the session.
       * If set, this can be used by the runtime and the Ops for debugging,
       * monitoring, etc.
       * NOTE: This is currently used and propagated only by the direct session.
       * </pre>
       *
       * <code>.tensorflow.SessionMetadata session_metadata = 11;</code>
       */
      public boolean hasSessionMetadata() {
        return sessionMetadataBuilder_ != null || sessionMetadata_ != null;
      }
      /**
       * <pre>
       * Metadata about the session.
       * If set, this can be used by the runtime and the Ops for debugging,
       * monitoring, etc.
       * NOTE: This is currently used and propagated only by the direct session.
       * </pre>
       *
       * <code>.tensorflow.SessionMetadata session_metadata = 11;</code>
       */
      public org.tensorflow.proto.framework.SessionMetadata getSessionMetadata() {
        if (sessionMetadataBuilder_ == null) {
          return sessionMetadata_ == null ? org.tensorflow.proto.framework.SessionMetadata.getDefaultInstance() : sessionMetadata_;
        } else {
          return sessionMetadataBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       * Metadata about the session.
       * If set, this can be used by the runtime and the Ops for debugging,
       * monitoring, etc.
       * NOTE: This is currently used and propagated only by the direct session.
       * </pre>
       *
       * <code>.tensorflow.SessionMetadata session_metadata = 11;</code>
       */
      public Builder setSessionMetadata(org.tensorflow.proto.framework.SessionMetadata value) {
        if (sessionMetadataBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          sessionMetadata_ = value;
          onChanged();
        } else {
          sessionMetadataBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <pre>
       * Metadata about the session.
       * If set, this can be used by the runtime and the Ops for debugging,
       * monitoring, etc.
       * NOTE: This is currently used and propagated only by the direct session.
       * </pre>
       *
       * <code>.tensorflow.SessionMetadata session_metadata = 11;</code>
       */
      public Builder setSessionMetadata(
          org.tensorflow.proto.framework.SessionMetadata.Builder builderForValue) {
        if (sessionMetadataBuilder_ == null) {
          sessionMetadata_ = builderForValue.build();
          onChanged();
        } else {
          sessionMetadataBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <pre>
       * Metadata about the session.
       * If set, this can be used by the runtime and the Ops for debugging,
       * monitoring, etc.
       * NOTE: This is currently used and propagated only by the direct session.
       * </pre>
       *
       * <code>.tensorflow.SessionMetadata session_metadata = 11;</code>
       */
      public Builder mergeSessionMetadata(org.tensorflow.proto.framework.SessionMetadata value) {
        if (sessionMetadataBuilder_ == null) {
          if (sessionMetadata_ != null) {
            sessionMetadata_ =
              org.tensorflow.proto.framework.SessionMetadata.newBuilder(sessionMetadata_).mergeFrom(value).buildPartial();
          } else {
            sessionMetadata_ = value;
          }
          onChanged();
        } else {
          sessionMetadataBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <pre>
       * Metadata about the session.
       * If set, this can be used by the runtime and the Ops for debugging,
       * monitoring, etc.
       * NOTE: This is currently used and propagated only by the direct session.
       * </pre>
       *
       * <code>.tensorflow.SessionMetadata session_metadata = 11;</code>
       */
      public Builder clearSessionMetadata() {
        if (sessionMetadataBuilder_ == null) {
          sessionMetadata_ = null;
          onChanged();
        } else {
          sessionMetadata_ = null;
          sessionMetadataBuilder_ = null;
        }

        return this;
      }
      /**
       * <pre>
       * Metadata about the session.
       * If set, this can be used by the runtime and the Ops for debugging,
       * monitoring, etc.
       * NOTE: This is currently used and propagated only by the direct session.
       * </pre>
       *
       * <code>.tensorflow.SessionMetadata session_metadata = 11;</code>
       */
      public org.tensorflow.proto.framework.SessionMetadata.Builder getSessionMetadataBuilder() {
        
        onChanged();
        return getSessionMetadataFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       * Metadata about the session.
       * If set, this can be used by the runtime and the Ops for debugging,
       * monitoring, etc.
       * NOTE: This is currently used and propagated only by the direct session.
       * </pre>
       *
       * <code>.tensorflow.SessionMetadata session_metadata = 11;</code>
       */
      public org.tensorflow.proto.framework.SessionMetadataOrBuilder getSessionMetadataOrBuilder() {
        if (sessionMetadataBuilder_ != null) {
          return sessionMetadataBuilder_.getMessageOrBuilder();
        } else {
          return sessionMetadata_ == null ?
              org.tensorflow.proto.framework.SessionMetadata.getDefaultInstance() : sessionMetadata_;
        }
      }
      /**
       * <pre>
       * Metadata about the session.
       * If set, this can be used by the runtime and the Ops for debugging,
       * monitoring, etc.
       * NOTE: This is currently used and propagated only by the direct session.
       * </pre>
       *
       * <code>.tensorflow.SessionMetadata session_metadata = 11;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.tensorflow.proto.framework.SessionMetadata, org.tensorflow.proto.framework.SessionMetadata.Builder, org.tensorflow.proto.framework.SessionMetadataOrBuilder> 
          getSessionMetadataFieldBuilder() {
        if (sessionMetadataBuilder_ == null) {
          sessionMetadataBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.tensorflow.proto.framework.SessionMetadata, org.tensorflow.proto.framework.SessionMetadata.Builder, org.tensorflow.proto.framework.SessionMetadataOrBuilder>(
                  getSessionMetadata(),
                  getParentForChildren(),
                  isClean());
          sessionMetadata_ = null;
        }
        return sessionMetadataBuilder_;
      }

      private boolean optimizeForStaticGraph_ ;
      /**
       * <pre>
       * If true, the session may treat the graph as being static for optimization
       * purposes.
       * If this option is set to true when a session is created, the full
       * GraphDef must be passed in a single call to Session::Create(), and
       * Session::Extend() may not be supported.
       * </pre>
       *
       * <code>bool optimize_for_static_graph = 12;</code>
       */
      public boolean getOptimizeForStaticGraph() {
        return optimizeForStaticGraph_;
      }
      /**
       * <pre>
       * If true, the session may treat the graph as being static for optimization
       * purposes.
       * If this option is set to true when a session is created, the full
       * GraphDef must be passed in a single call to Session::Create(), and
       * Session::Extend() may not be supported.
       * </pre>
       *
       * <code>bool optimize_for_static_graph = 12;</code>
       */
      public Builder setOptimizeForStaticGraph(boolean value) {
        
        optimizeForStaticGraph_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * If true, the session may treat the graph as being static for optimization
       * purposes.
       * If this option is set to true when a session is created, the full
       * GraphDef must be passed in a single call to Session::Create(), and
       * Session::Extend() may not be supported.
       * </pre>
       *
       * <code>bool optimize_for_static_graph = 12;</code>
       */
      public Builder clearOptimizeForStaticGraph() {
        
        optimizeForStaticGraph_ = false;
        onChanged();
        return this;
      }

      private boolean enableMlirBridge_ ;
      /**
       * <pre>
       * This field will eventually be deprecated and replaced by
       * mlir_bridge_rollout (b/166038521).
       * Whether to enable the MLIR-based TF-&gt;XLA bridge.
       * This is a replacement to the existing bridge, and not ready for
       * production usage yet.
       * If this option is set to true when a session is created, MLIR is used to
       * perform the set of graph transformations to put the graph in a form that
       * can be executed with delegation of some computations to an accelerator.
       * This builds on the model of XLA where a subset of the graph is
       * encapsulated and attached to a "compile" operation, whose result is fed
       * to an "execute" operation. The kernel for these operations is responsible
       * to lower the encapsulated graph to a particular device.
       * </pre>
       *
       * <code>bool enable_mlir_bridge = 13;</code>
       */
      public boolean getEnableMlirBridge() {
        return enableMlirBridge_;
      }
      /**
       * <pre>
       * This field will eventually be deprecated and replaced by
       * mlir_bridge_rollout (b/166038521).
       * Whether to enable the MLIR-based TF-&gt;XLA bridge.
       * This is a replacement to the existing bridge, and not ready for
       * production usage yet.
       * If this option is set to true when a session is created, MLIR is used to
       * perform the set of graph transformations to put the graph in a form that
       * can be executed with delegation of some computations to an accelerator.
       * This builds on the model of XLA where a subset of the graph is
       * encapsulated and attached to a "compile" operation, whose result is fed
       * to an "execute" operation. The kernel for these operations is responsible
       * to lower the encapsulated graph to a particular device.
       * </pre>
       *
       * <code>bool enable_mlir_bridge = 13;</code>
       */
      public Builder setEnableMlirBridge(boolean value) {
        
        enableMlirBridge_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * This field will eventually be deprecated and replaced by
       * mlir_bridge_rollout (b/166038521).
       * Whether to enable the MLIR-based TF-&gt;XLA bridge.
       * This is a replacement to the existing bridge, and not ready for
       * production usage yet.
       * If this option is set to true when a session is created, MLIR is used to
       * perform the set of graph transformations to put the graph in a form that
       * can be executed with delegation of some computations to an accelerator.
       * This builds on the model of XLA where a subset of the graph is
       * encapsulated and attached to a "compile" operation, whose result is fed
       * to an "execute" operation. The kernel for these operations is responsible
       * to lower the encapsulated graph to a particular device.
       * </pre>
       *
       * <code>bool enable_mlir_bridge = 13;</code>
       */
      public Builder clearEnableMlirBridge() {
        
        enableMlirBridge_ = false;
        onChanged();
        return this;
      }

      private int mlirBridgeRollout_ = 0;
      /**
       * <pre>
       * This field is underdevelopment, for now use enable_mlir_bridge
       * (b/166038521).
       * Whether to enable the MLIR-based TF-&gt;XLA bridge.
       * </pre>
       *
       * <code>.tensorflow.ConfigProto.Experimental.MlirBridgeRollout mlir_bridge_rollout = 17;</code>
       */
      public int getMlirBridgeRolloutValue() {
        return mlirBridgeRollout_;
      }
      /**
       * <pre>
       * This field is underdevelopment, for now use enable_mlir_bridge
       * (b/166038521).
       * Whether to enable the MLIR-based TF-&gt;XLA bridge.
       * </pre>
       *
       * <code>.tensorflow.ConfigProto.Experimental.MlirBridgeRollout mlir_bridge_rollout = 17;</code>
       */
      public Builder setMlirBridgeRolloutValue(int value) {
        mlirBridgeRollout_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * This field is underdevelopment, for now use enable_mlir_bridge
       * (b/166038521).
       * Whether to enable the MLIR-based TF-&gt;XLA bridge.
       * </pre>
       *
       * <code>.tensorflow.ConfigProto.Experimental.MlirBridgeRollout mlir_bridge_rollout = 17;</code>
       */
      public org.tensorflow.proto.framework.ConfigProto.Experimental.MlirBridgeRollout getMlirBridgeRollout() {
        @SuppressWarnings("deprecation")
        org.tensorflow.proto.framework.ConfigProto.Experimental.MlirBridgeRollout result = org.tensorflow.proto.framework.ConfigProto.Experimental.MlirBridgeRollout.valueOf(mlirBridgeRollout_);
        return result == null ? org.tensorflow.proto.framework.ConfigProto.Experimental.MlirBridgeRollout.UNRECOGNIZED : result;
      }
      /**
       * <pre>
       * This field is underdevelopment, for now use enable_mlir_bridge
       * (b/166038521).
       * Whether to enable the MLIR-based TF-&gt;XLA bridge.
       * </pre>
       *
       * <code>.tensorflow.ConfigProto.Experimental.MlirBridgeRollout mlir_bridge_rollout = 17;</code>
       */
      public Builder setMlirBridgeRollout(org.tensorflow.proto.framework.ConfigProto.Experimental.MlirBridgeRollout value) {
        if (value == null) {
          throw new NullPointerException();
        }
        
        mlirBridgeRollout_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <pre>
       * This field is underdevelopment, for now use enable_mlir_bridge
       * (b/166038521).
       * Whether to enable the MLIR-based TF-&gt;XLA bridge.
       * </pre>
       *
       * <code>.tensorflow.ConfigProto.Experimental.MlirBridgeRollout mlir_bridge_rollout = 17;</code>
       */
      public Builder clearMlirBridgeRollout() {
        
        mlirBridgeRollout_ = 0;
        onChanged();
        return this;
      }

      private boolean enableMlirGraphOptimization_ ;
      /**
       * <pre>
       * Whether to enable the MLIR-based Graph optimizations.
       * This will become a part of standard Tensorflow graph optimization
       * pipeline, currently this is only used for gradual migration and testing
       * new passes that are replacing existing optimizations in Grappler.
       * </pre>
       *
       * <code>bool enable_mlir_graph_optimization = 16;</code>
       */
      public boolean getEnableMlirGraphOptimization() {
        return enableMlirGraphOptimization_;
      }
      /**
       * <pre>
       * Whether to enable the MLIR-based Graph optimizations.
       * This will become a part of standard Tensorflow graph optimization
       * pipeline, currently this is only used for gradual migration and testing
       * new passes that are replacing existing optimizations in Grappler.
       * </pre>
       *
       * <code>bool enable_mlir_graph_optimization = 16;</code>
       */
      public Builder setEnableMlirGraphOptimization(boolean value) {
        
        enableMlirGraphOptimization_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * Whether to enable the MLIR-based Graph optimizations.
       * This will become a part of standard Tensorflow graph optimization
       * pipeline, currently this is only used for gradual migration and testing
       * new passes that are replacing existing optimizations in Grappler.
       * </pre>
       *
       * <code>bool enable_mlir_graph_optimization = 16;</code>
       */
      public Builder clearEnableMlirGraphOptimization() {
        
        enableMlirGraphOptimization_ = false;
        onChanged();
        return this;
      }

      private boolean disableOutputPartitionGraphs_ ;
      /**
       * <pre>
       * If true, the session will not store an additional copy of the graph for
       * each subgraph.
       * If this option is set to true when a session is created, the
       * `RunOptions.output_partition_graphs` options must not be set.
       * </pre>
       *
       * <code>bool disable_output_partition_graphs = 14;</code>
       */
      public boolean getDisableOutputPartitionGraphs() {
        return disableOutputPartitionGraphs_;
      }
      /**
       * <pre>
       * If true, the session will not store an additional copy of the graph for
       * each subgraph.
       * If this option is set to true when a session is created, the
       * `RunOptions.output_partition_graphs` options must not be set.
       * </pre>
       *
       * <code>bool disable_output_partition_graphs = 14;</code>
       */
      public Builder setDisableOutputPartitionGraphs(boolean value) {
        
        disableOutputPartitionGraphs_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * If true, the session will not store an additional copy of the graph for
       * each subgraph.
       * If this option is set to true when a session is created, the
       * `RunOptions.output_partition_graphs` options must not be set.
       * </pre>
       *
       * <code>bool disable_output_partition_graphs = 14;</code>
       */
      public Builder clearDisableOutputPartitionGraphs() {
        
        disableOutputPartitionGraphs_ = false;
        onChanged();
        return this;
      }

      private long xlaFusionAutotunerThresh_ ;
      /**
       * <pre>
       * Minimum number of batches run through the XLA graph before XLA fusion
       * autotuner is enabled. Default value of zero disables the autotuner.
       * The XLA fusion autotuner can improve performance by executing a heuristic
       * search on the compiler parameters.
       * </pre>
       *
       * <code>int64 xla_fusion_autotuner_thresh = 15;</code>
       */
      public long getXlaFusionAutotunerThresh() {
        return xlaFusionAutotunerThresh_;
      }
      /**
       * <pre>
       * Minimum number of batches run through the XLA graph before XLA fusion
       * autotuner is enabled. Default value of zero disables the autotuner.
       * The XLA fusion autotuner can improve performance by executing a heuristic
       * search on the compiler parameters.
       * </pre>
       *
       * <code>int64 xla_fusion_autotuner_thresh = 15;</code>
       */
      public Builder setXlaFusionAutotunerThresh(long value) {
        
        xlaFusionAutotunerThresh_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * Minimum number of batches run through the XLA graph before XLA fusion
       * autotuner is enabled. Default value of zero disables the autotuner.
       * The XLA fusion autotuner can improve performance by executing a heuristic
       * search on the compiler parameters.
       * </pre>
       *
       * <code>int64 xla_fusion_autotuner_thresh = 15;</code>
       */
      public Builder clearXlaFusionAutotunerThresh() {
        
        xlaFusionAutotunerThresh_ = 0L;
        onChanged();
        return this;
      }

      private boolean useTfrt_ ;
      /**
       * <pre>
       * Whether runtime execution uses TFRT.
       * </pre>
       *
       * <code>bool use_tfrt = 18;</code>
       */
      public boolean getUseTfrt() {
        return useTfrt_;
      }
      /**
       * <pre>
       * Whether runtime execution uses TFRT.
       * </pre>
       *
       * <code>bool use_tfrt = 18;</code>
       */
      public Builder setUseTfrt(boolean value) {
        
        useTfrt_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * Whether runtime execution uses TFRT.
       * </pre>
       *
       * <code>bool use_tfrt = 18;</code>
       */
      public Builder clearUseTfrt() {
        
        useTfrt_ = false;
        onChanged();
        return this;
      }

      private boolean disableFunctionalOpsLowering_ ;
      /**
       * <pre>
       * Whether functional control flow op lowering should be disabled. This is
       * useful when executing within a portable runtime where control flow op
       * kernels may not be loaded due to selective registration.
       * </pre>
       *
       * <code>bool disable_functional_ops_lowering = 21;</code>
       */
      public boolean getDisableFunctionalOpsLowering() {
        return disableFunctionalOpsLowering_;
      }
      /**
       * <pre>
       * Whether functional control flow op lowering should be disabled. This is
       * useful when executing within a portable runtime where control flow op
       * kernels may not be loaded due to selective registration.
       * </pre>
       *
       * <code>bool disable_functional_ops_lowering = 21;</code>
       */
      public Builder setDisableFunctionalOpsLowering(boolean value) {
        
        disableFunctionalOpsLowering_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * Whether functional control flow op lowering should be disabled. This is
       * useful when executing within a portable runtime where control flow op
       * kernels may not be loaded due to selective registration.
       * </pre>
       *
       * <code>bool disable_functional_ops_lowering = 21;</code>
       */
      public Builder clearDisableFunctionalOpsLowering() {
        
        disableFunctionalOpsLowering_ = false;
        onChanged();
        return this;
      }

      private boolean xlaPreferSingleGraphCluster_ ;
      /**
       * <pre>
       * Provides a hint to XLA auto clustering to prefer forming a single large
       * cluster that encompases most of the graph.
       * </pre>
       *
       * <code>bool xla_prefer_single_graph_cluster = 22;</code>
       */
      public boolean getXlaPreferSingleGraphCluster() {
        return xlaPreferSingleGraphCluster_;
      }
      /**
       * <pre>
       * Provides a hint to XLA auto clustering to prefer forming a single large
       * cluster that encompases most of the graph.
       * </pre>
       *
       * <code>bool xla_prefer_single_graph_cluster = 22;</code>
       */
      public Builder setXlaPreferSingleGraphCluster(boolean value) {
        
        xlaPreferSingleGraphCluster_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * Provides a hint to XLA auto clustering to prefer forming a single large
       * cluster that encompases most of the graph.
       * </pre>
       *
       * <code>bool xla_prefer_single_graph_cluster = 22;</code>
       */
      public Builder clearXlaPreferSingleGraphCluster() {
        
        xlaPreferSingleGraphCluster_ = false;
        onChanged();
        return this;
      }

      private org.tensorflow.proto.distruntime.CoordinationConfig.CoordinationServiceConfig coordinationConfig_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.tensorflow.proto.distruntime.CoordinationConfig.CoordinationServiceConfig, org.tensorflow.proto.distruntime.CoordinationConfig.CoordinationServiceConfig.Builder, org.tensorflow.proto.distruntime.CoordinationConfig.CoordinationServiceConfigOrBuilder> coordinationConfigBuilder_;
      /**
       * <pre>
       * Distributed coordination service configurations.
       * </pre>
       *
       * <code>.tensorflow.CoordinationServiceConfig coordination_config = 23;</code>
       */
      public boolean hasCoordinationConfig() {
        return coordinationConfigBuilder_ != null || coordinationConfig_ != null;
      }
      /**
       * <pre>
       * Distributed coordination service configurations.
       * </pre>
       *
       * <code>.tensorflow.CoordinationServiceConfig coordination_config = 23;</code>
       */
      public org.tensorflow.proto.distruntime.CoordinationConfig.CoordinationServiceConfig getCoordinationConfig() {
        if (coordinationConfigBuilder_ == null) {
          return coordinationConfig_ == null ? org.tensorflow.proto.distruntime.CoordinationConfig.CoordinationServiceConfig.getDefaultInstance() : coordinationConfig_;
        } else {
          return coordinationConfigBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       * Distributed coordination service configurations.
       * </pre>
       *
       * <code>.tensorflow.CoordinationServiceConfig coordination_config = 23;</code>
       */
      public Builder setCoordinationConfig(org.tensorflow.proto.distruntime.CoordinationConfig.CoordinationServiceConfig value) {
        if (coordinationConfigBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          coordinationConfig_ = value;
          onChanged();
        } else {
          coordinationConfigBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <pre>
       * Distributed coordination service configurations.
       * </pre>
       *
       * <code>.tensorflow.CoordinationServiceConfig coordination_config = 23;</code>
       */
      public Builder setCoordinationConfig(
          org.tensorflow.proto.distruntime.CoordinationConfig.CoordinationServiceConfig.Builder builderForValue) {
        if (coordinationConfigBuilder_ == null) {
          coordinationConfig_ = builderForValue.build();
          onChanged();
        } else {
          coordinationConfigBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <pre>
       * Distributed coordination service configurations.
       * </pre>
       *
       * <code>.tensorflow.CoordinationServiceConfig coordination_config = 23;</code>
       */
      public Builder mergeCoordinationConfig(org.tensorflow.proto.distruntime.CoordinationConfig.CoordinationServiceConfig value) {
        if (coordinationConfigBuilder_ == null) {
          if (coordinationConfig_ != null) {
            coordinationConfig_ =
              org.tensorflow.proto.distruntime.CoordinationConfig.CoordinationServiceConfig.newBuilder(coordinationConfig_).mergeFrom(value).buildPartial();
          } else {
            coordinationConfig_ = value;
          }
          onChanged();
        } else {
          coordinationConfigBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <pre>
       * Distributed coordination service configurations.
       * </pre>
       *
       * <code>.tensorflow.CoordinationServiceConfig coordination_config = 23;</code>
       */
      public Builder clearCoordinationConfig() {
        if (coordinationConfigBuilder_ == null) {
          coordinationConfig_ = null;
          onChanged();
        } else {
          coordinationConfig_ = null;
          coordinationConfigBuilder_ = null;
        }

        return this;
      }
      /**
       * <pre>
       * Distributed coordination service configurations.
       * </pre>
       *
       * <code>.tensorflow.CoordinationServiceConfig coordination_config = 23;</code>
       */
      public org.tensorflow.proto.distruntime.CoordinationConfig.CoordinationServiceConfig.Builder getCoordinationConfigBuilder() {
        
        onChanged();
        return getCoordinationConfigFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       * Distributed coordination service configurations.
       * </pre>
       *
       * <code>.tensorflow.CoordinationServiceConfig coordination_config = 23;</code>
       */
      public org.tensorflow.proto.distruntime.CoordinationConfig.CoordinationServiceConfigOrBuilder getCoordinationConfigOrBuilder() {
        if (coordinationConfigBuilder_ != null) {
          return coordinationConfigBuilder_.getMessageOrBuilder();
        } else {
          return coordinationConfig_ == null ?
              org.tensorflow.proto.distruntime.CoordinationConfig.CoordinationServiceConfig.getDefaultInstance() : coordinationConfig_;
        }
      }
      /**
       * <pre>
       * Distributed coordination service configurations.
       * </pre>
       *
       * <code>.tensorflow.CoordinationServiceConfig coordination_config = 23;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.tensorflow.proto.distruntime.CoordinationConfig.CoordinationServiceConfig, org.tensorflow.proto.distruntime.CoordinationConfig.CoordinationServiceConfig.Builder, org.tensorflow.proto.distruntime.CoordinationConfig.CoordinationServiceConfigOrBuilder> 
          getCoordinationConfigFieldBuilder() {
        if (coordinationConfigBuilder_ == null) {
          coordinationConfigBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.tensorflow.proto.distruntime.CoordinationConfig.CoordinationServiceConfig, org.tensorflow.proto.distruntime.CoordinationConfig.CoordinationServiceConfig.Builder, org.tensorflow.proto.distruntime.CoordinationConfig.CoordinationServiceConfigOrBuilder>(
                  getCoordinationConfig(),
                  getParentForChildren(),
                  isClean());
          coordinationConfig_ = null;
        }
        return coordinationConfigBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:tensorflow.ConfigProto.Experimental)
    }

    // @@protoc_insertion_point(class_scope:tensorflow.ConfigProto.Experimental)
    private static final org.tensorflow.proto.framework.ConfigProto.Experimental DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.tensorflow.proto.framework.ConfigProto.Experimental();
    }

    public static org.tensorflow.proto.framework.ConfigProto.Experimental getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<Experimental>
        PARSER = new com.google.protobuf.AbstractParser<Experimental>() {
      @java.lang.Override
      public Experimental parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new Experimental(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<Experimental> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<Experimental> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public org.tensorflow.proto.framework.ConfigProto.Experimental getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public static final int DEVICE_COUNT_FIELD_NUMBER = 1;
  private static final class DeviceCountDefaultEntryHolder {
    static final com.google.protobuf.MapEntry<
        java.lang.String, java.lang.Integer> defaultEntry =
            com.google.protobuf.MapEntry
            .<java.lang.String, java.lang.Integer>newDefaultInstance(
                org.tensorflow.proto.framework.ConfigProtos.internal_static_tensorflow_ConfigProto_DeviceCountEntry_descriptor, 
                com.google.protobuf.WireFormat.FieldType.STRING,
                "",
                com.google.protobuf.WireFormat.FieldType.INT32,
                0);
  }
  private com.google.protobuf.MapField<
      java.lang.String, java.lang.Integer> deviceCount_;
  private com.google.protobuf.MapField<java.lang.String, java.lang.Integer>
  internalGetDeviceCount() {
    if (deviceCount_ == null) {
      return com.google.protobuf.MapField.emptyMapField(
          DeviceCountDefaultEntryHolder.defaultEntry);
    }
    return deviceCount_;
  }

  public int getDeviceCountCount() {
    return internalGetDeviceCount().getMap().size();
  }
  /**
   * <pre>
   * Map from device type name (e.g., "CPU" or "GPU" ) to maximum
   * number of devices of that type to use.  If a particular device
   * type is not found in the map, the system picks an appropriate
   * number.
   * </pre>
   *
   * <code>map&lt;string, int32&gt; device_count = 1;</code>
   */

  public boolean containsDeviceCount(
      java.lang.String key) {
    if (key == null) { throw new java.lang.NullPointerException(); }
    return internalGetDeviceCount().getMap().containsKey(key);
  }
  /**
   * Use {@link #getDeviceCountMap()} instead.
   */
  @java.lang.Deprecated
  public java.util.Map<java.lang.String, java.lang.Integer> getDeviceCount() {
    return getDeviceCountMap();
  }
  /**
   * <pre>
   * Map from device type name (e.g., "CPU" or "GPU" ) to maximum
   * number of devices of that type to use.  If a particular device
   * type is not found in the map, the system picks an appropriate
   * number.
   * </pre>
   *
   * <code>map&lt;string, int32&gt; device_count = 1;</code>
   */

  public java.util.Map<java.lang.String, java.lang.Integer> getDeviceCountMap() {
    return internalGetDeviceCount().getMap();
  }
  /**
   * <pre>
   * Map from device type name (e.g., "CPU" or "GPU" ) to maximum
   * number of devices of that type to use.  If a particular device
   * type is not found in the map, the system picks an appropriate
   * number.
   * </pre>
   *
   * <code>map&lt;string, int32&gt; device_count = 1;</code>
   */

  public int getDeviceCountOrDefault(
      java.lang.String key,
      int defaultValue) {
    if (key == null) { throw new java.lang.NullPointerException(); }
    java.util.Map<java.lang.String, java.lang.Integer> map =
        internalGetDeviceCount().getMap();
    return map.containsKey(key) ? map.get(key) : defaultValue;
  }
  /**
   * <pre>
   * Map from device type name (e.g., "CPU" or "GPU" ) to maximum
   * number of devices of that type to use.  If a particular device
   * type is not found in the map, the system picks an appropriate
   * number.
   * </pre>
   *
   * <code>map&lt;string, int32&gt; device_count = 1;</code>
   */

  public int getDeviceCountOrThrow(
      java.lang.String key) {
    if (key == null) { throw new java.lang.NullPointerException(); }
    java.util.Map<java.lang.String, java.lang.Integer> map =
        internalGetDeviceCount().getMap();
    if (!map.containsKey(key)) {
      throw new java.lang.IllegalArgumentException();
    }
    return map.get(key);
  }

  public static final int INTRA_OP_PARALLELISM_THREADS_FIELD_NUMBER = 2;
  private int intraOpParallelismThreads_;
  /**
   * <pre>
   * The execution of an individual op (for some op types) can be
   * parallelized on a pool of intra_op_parallelism_threads.
   * 0 means the system picks an appropriate number.
   * If you create an ordinary session, e.g., from Python or C++,
   * then there is exactly one intra op thread pool per process.
   * The first session created determines the number of threads in this pool.
   * All subsequent sessions reuse/share this one global pool.
   * There are notable exceptions to the default behavior described above:
   * 1. There is an environment variable  for overriding this thread pool,
   *    named TF_OVERRIDE_GLOBAL_THREADPOOL.
   * 2. When connecting to a server, such as a remote `tf.train.Server`
   *    instance, then this option will be ignored altogether.
   * </pre>
   *
   * <code>int32 intra_op_parallelism_threads = 2;</code>
   */
  public int getIntraOpParallelismThreads() {
    return intraOpParallelismThreads_;
  }

  public static final int INTER_OP_PARALLELISM_THREADS_FIELD_NUMBER = 5;
  private int interOpParallelismThreads_;
  /**
   * <pre>
   * Nodes that perform blocking operations are enqueued on a pool of
   * inter_op_parallelism_threads available in each process.
   * 0 means the system picks an appropriate number.
   * Negative means all operations are performed in caller's thread.
   * Note that the first Session created in the process sets the
   * number of threads for all future sessions unless use_per_session_threads is
   * true or session_inter_op_thread_pool is configured.
   * </pre>
   *
   * <code>int32 inter_op_parallelism_threads = 5;</code>
   */
  public int getInterOpParallelismThreads() {
    return interOpParallelismThreads_;
  }

  public static final int USE_PER_SESSION_THREADS_FIELD_NUMBER = 9;
  private boolean usePerSessionThreads_;
  /**
   * <pre>
   * If true, use a new set of threads for this session rather than the global
   * pool of threads. Only supported by direct sessions.
   * If false, use the global threads created by the first session, or the
   * per-session thread pools configured by session_inter_op_thread_pool.
   * This option is deprecated. The same effect can be achieved by setting
   * session_inter_op_thread_pool to have one element, whose num_threads equals
   * inter_op_parallelism_threads.
   * </pre>
   *
   * <code>bool use_per_session_threads = 9;</code>
   */
  public boolean getUsePerSessionThreads() {
    return usePerSessionThreads_;
  }

  public static final int SESSION_INTER_OP_THREAD_POOL_FIELD_NUMBER = 12;
  private java.util.List<org.tensorflow.proto.framework.ThreadPoolOptionProto> sessionInterOpThreadPool_;
  /**
   * <pre>
   * This option is experimental - it may be replaced with a different mechanism
   * in the future.
   * Configures session thread pools. If this is configured, then RunOptions for
   * a Run call can select the thread pool to use.
   * The intended use is for when some session invocations need to run in a
   * background pool limited to a small number of threads:
   * - For example, a session may be configured to have one large pool (for
   * regular compute) and one small pool (for periodic, low priority work);
   * using the small pool is currently the mechanism for limiting the inter-op
   * parallelism of the low priority work.  Note that it does not limit the
   * parallelism of work spawned by a single op kernel implementation.
   * - Using this setting is normally not needed in training, but may help some
   * serving use cases.
   * - It is also generally recommended to set the global_name field of this
   * proto, to avoid creating multiple large pools. It is typically better to
   * run the non-low-priority work, even across sessions, in a single large
   * pool.
   * </pre>
   *
   * <code>repeated .tensorflow.ThreadPoolOptionProto session_inter_op_thread_pool = 12;</code>
   */
  public java.util.List<org.tensorflow.proto.framework.ThreadPoolOptionProto> getSessionInterOpThreadPoolList() {
    return sessionInterOpThreadPool_;
  }
  /**
   * <pre>
   * This option is experimental - it may be replaced with a different mechanism
   * in the future.
   * Configures session thread pools. If this is configured, then RunOptions for
   * a Run call can select the thread pool to use.
   * The intended use is for when some session invocations need to run in a
   * background pool limited to a small number of threads:
   * - For example, a session may be configured to have one large pool (for
   * regular compute) and one small pool (for periodic, low priority work);
   * using the small pool is currently the mechanism for limiting the inter-op
   * parallelism of the low priority work.  Note that it does not limit the
   * parallelism of work spawned by a single op kernel implementation.
   * - Using this setting is normally not needed in training, but may help some
   * serving use cases.
   * - It is also generally recommended to set the global_name field of this
   * proto, to avoid creating multiple large pools. It is typically better to
   * run the non-low-priority work, even across sessions, in a single large
   * pool.
   * </pre>
   *
   * <code>repeated .tensorflow.ThreadPoolOptionProto session_inter_op_thread_pool = 12;</code>
   */
  public java.util.List<? extends org.tensorflow.proto.framework.ThreadPoolOptionProtoOrBuilder> 
      getSessionInterOpThreadPoolOrBuilderList() {
    return sessionInterOpThreadPool_;
  }
  /**
   * <pre>
   * This option is experimental - it may be replaced with a different mechanism
   * in the future.
   * Configures session thread pools. If this is configured, then RunOptions for
   * a Run call can select the thread pool to use.
   * The intended use is for when some session invocations need to run in a
   * background pool limited to a small number of threads:
   * - For example, a session may be configured to have one large pool (for
   * regular compute) and one small pool (for periodic, low priority work);
   * using the small pool is currently the mechanism for limiting the inter-op
   * parallelism of the low priority work.  Note that it does not limit the
   * parallelism of work spawned by a single op kernel implementation.
   * - Using this setting is normally not needed in training, but may help some
   * serving use cases.
   * - It is also generally recommended to set the global_name field of this
   * proto, to avoid creating multiple large pools. It is typically better to
   * run the non-low-priority work, even across sessions, in a single large
   * pool.
   * </pre>
   *
   * <code>repeated .tensorflow.ThreadPoolOptionProto session_inter_op_thread_pool = 12;</code>
   */
  public int getSessionInterOpThreadPoolCount() {
    return sessionInterOpThreadPool_.size();
  }
  /**
   * <pre>
   * This option is experimental - it may be replaced with a different mechanism
   * in the future.
   * Configures session thread pools. If this is configured, then RunOptions for
   * a Run call can select the thread pool to use.
   * The intended use is for when some session invocations need to run in a
   * background pool limited to a small number of threads:
   * - For example, a session may be configured to have one large pool (for
   * regular compute) and one small pool (for periodic, low priority work);
   * using the small pool is currently the mechanism for limiting the inter-op
   * parallelism of the low priority work.  Note that it does not limit the
   * parallelism of work spawned by a single op kernel implementation.
   * - Using this setting is normally not needed in training, but may help some
   * serving use cases.
   * - It is also generally recommended to set the global_name field of this
   * proto, to avoid creating multiple large pools. It is typically better to
   * run the non-low-priority work, even across sessions, in a single large
   * pool.
   * </pre>
   *
   * <code>repeated .tensorflow.ThreadPoolOptionProto session_inter_op_thread_pool = 12;</code>
   */
  public org.tensorflow.proto.framework.ThreadPoolOptionProto getSessionInterOpThreadPool(int index) {
    return sessionInterOpThreadPool_.get(index);
  }
  /**
   * <pre>
   * This option is experimental - it may be replaced with a different mechanism
   * in the future.
   * Configures session thread pools. If this is configured, then RunOptions for
   * a Run call can select the thread pool to use.
   * The intended use is for when some session invocations need to run in a
   * background pool limited to a small number of threads:
   * - For example, a session may be configured to have one large pool (for
   * regular compute) and one small pool (for periodic, low priority work);
   * using the small pool is currently the mechanism for limiting the inter-op
   * parallelism of the low priority work.  Note that it does not limit the
   * parallelism of work spawned by a single op kernel implementation.
   * - Using this setting is normally not needed in training, but may help some
   * serving use cases.
   * - It is also generally recommended to set the global_name field of this
   * proto, to avoid creating multiple large pools. It is typically better to
   * run the non-low-priority work, even across sessions, in a single large
   * pool.
   * </pre>
   *
   * <code>repeated .tensorflow.ThreadPoolOptionProto session_inter_op_thread_pool = 12;</code>
   */
  public org.tensorflow.proto.framework.ThreadPoolOptionProtoOrBuilder getSessionInterOpThreadPoolOrBuilder(
      int index) {
    return sessionInterOpThreadPool_.get(index);
  }

  public static final int PLACEMENT_PERIOD_FIELD_NUMBER = 3;
  private int placementPeriod_;
  /**
   * <pre>
   * Assignment of Nodes to Devices is recomputed every placement_period
   * steps until the system warms up (at which point the recomputation
   * typically slows down automatically).
   * </pre>
   *
   * <code>int32 placement_period = 3;</code>
   */
  public int getPlacementPeriod() {
    return placementPeriod_;
  }

  public static final int DEVICE_FILTERS_FIELD_NUMBER = 4;
  private com.google.protobuf.LazyStringList deviceFilters_;
  /**
   * <pre>
   * When any filters are present sessions will ignore all devices which do not
   * match the filters. Each filter can be partially specified, e.g. "/job:ps"
   * "/job:worker/replica:3", etc.
   * </pre>
   *
   * <code>repeated string device_filters = 4;</code>
   */
  public com.google.protobuf.ProtocolStringList
      getDeviceFiltersList() {
    return deviceFilters_;
  }
  /**
   * <pre>
   * When any filters are present sessions will ignore all devices which do not
   * match the filters. Each filter can be partially specified, e.g. "/job:ps"
   * "/job:worker/replica:3", etc.
   * </pre>
   *
   * <code>repeated string device_filters = 4;</code>
   */
  public int getDeviceFiltersCount() {
    return deviceFilters_.size();
  }
  /**
   * <pre>
   * When any filters are present sessions will ignore all devices which do not
   * match the filters. Each filter can be partially specified, e.g. "/job:ps"
   * "/job:worker/replica:3", etc.
   * </pre>
   *
   * <code>repeated string device_filters = 4;</code>
   */
  public java.lang.String getDeviceFilters(int index) {
    return deviceFilters_.get(index);
  }
  /**
   * <pre>
   * When any filters are present sessions will ignore all devices which do not
   * match the filters. Each filter can be partially specified, e.g. "/job:ps"
   * "/job:worker/replica:3", etc.
   * </pre>
   *
   * <code>repeated string device_filters = 4;</code>
   */
  public com.google.protobuf.ByteString
      getDeviceFiltersBytes(int index) {
    return deviceFilters_.getByteString(index);
  }

  public static final int GPU_OPTIONS_FIELD_NUMBER = 6;
  private org.tensorflow.proto.framework.GPUOptions gpuOptions_;
  /**
   * <pre>
   * Options that apply to all GPUs.
   * </pre>
   *
   * <code>.tensorflow.GPUOptions gpu_options = 6;</code>
   */
  public boolean hasGpuOptions() {
    return gpuOptions_ != null;
  }
  /**
   * <pre>
   * Options that apply to all GPUs.
   * </pre>
   *
   * <code>.tensorflow.GPUOptions gpu_options = 6;</code>
   */
  public org.tensorflow.proto.framework.GPUOptions getGpuOptions() {
    return gpuOptions_ == null ? org.tensorflow.proto.framework.GPUOptions.getDefaultInstance() : gpuOptions_;
  }
  /**
   * <pre>
   * Options that apply to all GPUs.
   * </pre>
   *
   * <code>.tensorflow.GPUOptions gpu_options = 6;</code>
   */
  public org.tensorflow.proto.framework.GPUOptionsOrBuilder getGpuOptionsOrBuilder() {
    return getGpuOptions();
  }

  public static final int ALLOW_SOFT_PLACEMENT_FIELD_NUMBER = 7;
  private boolean allowSoftPlacement_;
  /**
   * <pre>
   * Whether soft placement is allowed. If allow_soft_placement is true,
   * an op will be placed on CPU if
   *   1. there's no GPU implementation for the OP
   * or
   *   2. no GPU devices are known or registered
   * or
   *   3. need to co-locate with reftype input(s) which are from CPU.
   * </pre>
   *
   * <code>bool allow_soft_placement = 7;</code>
   */
  public boolean getAllowSoftPlacement() {
    return allowSoftPlacement_;
  }

  public static final int LOG_DEVICE_PLACEMENT_FIELD_NUMBER = 8;
  private boolean logDevicePlacement_;
  /**
   * <pre>
   * Whether device placements should be logged.
   * </pre>
   *
   * <code>bool log_device_placement = 8;</code>
   */
  public boolean getLogDevicePlacement() {
    return logDevicePlacement_;
  }

  public static final int GRAPH_OPTIONS_FIELD_NUMBER = 10;
  private org.tensorflow.proto.framework.GraphOptions graphOptions_;
  /**
   * <pre>
   * Options that apply to all graphs.
   * </pre>
   *
   * <code>.tensorflow.GraphOptions graph_options = 10;</code>
   */
  public boolean hasGraphOptions() {
    return graphOptions_ != null;
  }
  /**
   * <pre>
   * Options that apply to all graphs.
   * </pre>
   *
   * <code>.tensorflow.GraphOptions graph_options = 10;</code>
   */
  public org.tensorflow.proto.framework.GraphOptions getGraphOptions() {
    return graphOptions_ == null ? org.tensorflow.proto.framework.GraphOptions.getDefaultInstance() : graphOptions_;
  }
  /**
   * <pre>
   * Options that apply to all graphs.
   * </pre>
   *
   * <code>.tensorflow.GraphOptions graph_options = 10;</code>
   */
  public org.tensorflow.proto.framework.GraphOptionsOrBuilder getGraphOptionsOrBuilder() {
    return getGraphOptions();
  }

  public static final int OPERATION_TIMEOUT_IN_MS_FIELD_NUMBER = 11;
  private long operationTimeoutInMs_;
  /**
   * <pre>
   * Global timeout for all blocking operations in this session.  If non-zero,
   * and not overridden on a per-operation basis, this value will be used as the
   * deadline for all blocking operations.
   * </pre>
   *
   * <code>int64 operation_timeout_in_ms = 11;</code>
   */
  public long getOperationTimeoutInMs() {
    return operationTimeoutInMs_;
  }

  public static final int RPC_OPTIONS_FIELD_NUMBER = 13;
  private org.tensorflow.proto.framework.RPCOptions rpcOptions_;
  /**
   * <pre>
   * Options that apply when this session uses the distributed runtime.
   * </pre>
   *
   * <code>.tensorflow.RPCOptions rpc_options = 13;</code>
   */
  public boolean hasRpcOptions() {
    return rpcOptions_ != null;
  }
  /**
   * <pre>
   * Options that apply when this session uses the distributed runtime.
   * </pre>
   *
   * <code>.tensorflow.RPCOptions rpc_options = 13;</code>
   */
  public org.tensorflow.proto.framework.RPCOptions getRpcOptions() {
    return rpcOptions_ == null ? org.tensorflow.proto.framework.RPCOptions.getDefaultInstance() : rpcOptions_;
  }
  /**
   * <pre>
   * Options that apply when this session uses the distributed runtime.
   * </pre>
   *
   * <code>.tensorflow.RPCOptions rpc_options = 13;</code>
   */
  public org.tensorflow.proto.framework.RPCOptionsOrBuilder getRpcOptionsOrBuilder() {
    return getRpcOptions();
  }

  public static final int CLUSTER_DEF_FIELD_NUMBER = 14;
  private org.tensorflow.proto.distruntime.ClusterDef clusterDef_;
  /**
   * <pre>
   * Optional list of all workers to use in this session.
   * </pre>
   *
   * <code>.tensorflow.ClusterDef cluster_def = 14;</code>
   */
  public boolean hasClusterDef() {
    return clusterDef_ != null;
  }
  /**
   * <pre>
   * Optional list of all workers to use in this session.
   * </pre>
   *
   * <code>.tensorflow.ClusterDef cluster_def = 14;</code>
   */
  public org.tensorflow.proto.distruntime.ClusterDef getClusterDef() {
    return clusterDef_ == null ? org.tensorflow.proto.distruntime.ClusterDef.getDefaultInstance() : clusterDef_;
  }
  /**
   * <pre>
   * Optional list of all workers to use in this session.
   * </pre>
   *
   * <code>.tensorflow.ClusterDef cluster_def = 14;</code>
   */
  public org.tensorflow.proto.distruntime.ClusterDefOrBuilder getClusterDefOrBuilder() {
    return getClusterDef();
  }

  public static final int ISOLATE_SESSION_STATE_FIELD_NUMBER = 15;
  private boolean isolateSessionState_;
  /**
   * <pre>
   * If true, any resources such as Variables used in the session will not be
   * shared with other sessions. However, when clusterspec propagation is
   * enabled, this field is ignored and sessions are always isolated.
   * </pre>
   *
   * <code>bool isolate_session_state = 15;</code>
   */
  public boolean getIsolateSessionState() {
    return isolateSessionState_;
  }

  public static final int SHARE_CLUSTER_DEVICES_IN_SESSION_FIELD_NUMBER = 17;
  private boolean shareClusterDevicesInSession_;
  /**
   * <pre>
   * When true, WorkerSessions are created with device attributes from the
   * full cluster.
   * This is helpful when a worker wants to partition a graph
   * (for example during a PartitionedCallOp).
   * </pre>
   *
   * <code>bool share_cluster_devices_in_session = 17;</code>
   */
  public boolean getShareClusterDevicesInSession() {
    return shareClusterDevicesInSession_;
  }

  public static final int EXPERIMENTAL_FIELD_NUMBER = 16;
  private org.tensorflow.proto.framework.ConfigProto.Experimental experimental_;
  /**
   * <code>.tensorflow.ConfigProto.Experimental experimental = 16;</code>
   */
  public boolean hasExperimental() {
    return experimental_ != null;
  }
  /**
   * <code>.tensorflow.ConfigProto.Experimental experimental = 16;</code>
   */
  public org.tensorflow.proto.framework.ConfigProto.Experimental getExperimental() {
    return experimental_ == null ? org.tensorflow.proto.framework.ConfigProto.Experimental.getDefaultInstance() : experimental_;
  }
  /**
   * <code>.tensorflow.ConfigProto.Experimental experimental = 16;</code>
   */
  public org.tensorflow.proto.framework.ConfigProto.ExperimentalOrBuilder getExperimentalOrBuilder() {
    return getExperimental();
  }

  private byte memoizedIsInitialized = -1;
  @java.lang.Override
  public final boolean isInitialized() {
    byte isInitialized = memoizedIsInitialized;
    if (isInitialized == 1) return true;
    if (isInitialized == 0) return false;

    memoizedIsInitialized = 1;
    return true;
  }

  @java.lang.Override
  public void writeTo(com.google.protobuf.CodedOutputStream output)
                      throws java.io.IOException {
    com.google.protobuf.GeneratedMessageV3
      .serializeStringMapTo(
        output,
        internalGetDeviceCount(),
        DeviceCountDefaultEntryHolder.defaultEntry,
        1);
    if (intraOpParallelismThreads_ != 0) {
      output.writeInt32(2, intraOpParallelismThreads_);
    }
    if (placementPeriod_ != 0) {
      output.writeInt32(3, placementPeriod_);
    }
    for (int i = 0; i < deviceFilters_.size(); i++) {
      com.google.protobuf.GeneratedMessageV3.writeString(output, 4, deviceFilters_.getRaw(i));
    }
    if (interOpParallelismThreads_ != 0) {
      output.writeInt32(5, interOpParallelismThreads_);
    }
    if (gpuOptions_ != null) {
      output.writeMessage(6, getGpuOptions());
    }
    if (allowSoftPlacement_ != false) {
      output.writeBool(7, allowSoftPlacement_);
    }
    if (logDevicePlacement_ != false) {
      output.writeBool(8, logDevicePlacement_);
    }
    if (usePerSessionThreads_ != false) {
      output.writeBool(9, usePerSessionThreads_);
    }
    if (graphOptions_ != null) {
      output.writeMessage(10, getGraphOptions());
    }
    if (operationTimeoutInMs_ != 0L) {
      output.writeInt64(11, operationTimeoutInMs_);
    }
    for (int i = 0; i < sessionInterOpThreadPool_.size(); i++) {
      output.writeMessage(12, sessionInterOpThreadPool_.get(i));
    }
    if (rpcOptions_ != null) {
      output.writeMessage(13, getRpcOptions());
    }
    if (clusterDef_ != null) {
      output.writeMessage(14, getClusterDef());
    }
    if (isolateSessionState_ != false) {
      output.writeBool(15, isolateSessionState_);
    }
    if (experimental_ != null) {
      output.writeMessage(16, getExperimental());
    }
    if (shareClusterDevicesInSession_ != false) {
      output.writeBool(17, shareClusterDevicesInSession_);
    }
    unknownFields.writeTo(output);
  }

  @java.lang.Override
  public int getSerializedSize() {
    int size = memoizedSize;
    if (size != -1) return size;

    size = 0;
    for (java.util.Map.Entry<java.lang.String, java.lang.Integer> entry
         : internalGetDeviceCount().getMap().entrySet()) {
      com.google.protobuf.MapEntry<java.lang.String, java.lang.Integer>
      deviceCount__ = DeviceCountDefaultEntryHolder.defaultEntry.newBuilderForType()
          .setKey(entry.getKey())
          .setValue(entry.getValue())
          .build();
      size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, deviceCount__);
    }
    if (intraOpParallelismThreads_ != 0) {
      size += com.google.protobuf.CodedOutputStream
        .computeInt32Size(2, intraOpParallelismThreads_);
    }
    if (placementPeriod_ != 0) {
      size += com.google.protobuf.CodedOutputStream
        .computeInt32Size(3, placementPeriod_);
    }
    {
      int dataSize = 0;
      for (int i = 0; i < deviceFilters_.size(); i++) {
        dataSize += computeStringSizeNoTag(deviceFilters_.getRaw(i));
      }
      size += dataSize;
      size += 1 * getDeviceFiltersList().size();
    }
    if (interOpParallelismThreads_ != 0) {
      size += com.google.protobuf.CodedOutputStream
        .computeInt32Size(5, interOpParallelismThreads_);
    }
    if (gpuOptions_ != null) {
      size += com.google.protobuf.CodedOutputStream
        .computeMessageSize(6, getGpuOptions());
    }
    if (allowSoftPlacement_ != false) {
      size += com.google.protobuf.CodedOutputStream
        .computeBoolSize(7, allowSoftPlacement_);
    }
    if (logDevicePlacement_ != false) {
      size += com.google.protobuf.CodedOutputStream
        .computeBoolSize(8, logDevicePlacement_);
    }
    if (usePerSessionThreads_ != false) {
      size += com.google.protobuf.CodedOutputStream
        .computeBoolSize(9, usePerSessionThreads_);
    }
    if (graphOptions_ != null) {
      size += com.google.protobuf.CodedOutputStream
        .computeMessageSize(10, getGraphOptions());
    }
    if (operationTimeoutInMs_ != 0L) {
      size += com.google.protobuf.CodedOutputStream
        .computeInt64Size(11, operationTimeoutInMs_);
    }
    for (int i = 0; i < sessionInterOpThreadPool_.size(); i++) {
      size += com.google.protobuf.CodedOutputStream
        .computeMessageSize(12, sessionInterOpThreadPool_.get(i));
    }
    if (rpcOptions_ != null) {
      size += com.google.protobuf.CodedOutputStream
        .computeMessageSize(13, getRpcOptions());
    }
    if (clusterDef_ != null) {
      size += com.google.protobuf.CodedOutputStream
        .computeMessageSize(14, getClusterDef());
    }
    if (isolateSessionState_ != false) {
      size += com.google.protobuf.CodedOutputStream
        .computeBoolSize(15, isolateSessionState_);
    }
    if (experimental_ != null) {
      size += com.google.protobuf.CodedOutputStream
        .computeMessageSize(16, getExperimental());
    }
    if (shareClusterDevicesInSession_ != false) {
      size += com.google.protobuf.CodedOutputStream
        .computeBoolSize(17, shareClusterDevicesInSession_);
    }
    size += unknownFields.getSerializedSize();
    memoizedSize = size;
    return size;
  }

  @java.lang.Override
  public boolean equals(final java.lang.Object obj) {
    if (obj == this) {
     return true;
    }
    if (!(obj instanceof org.tensorflow.proto.framework.ConfigProto)) {
      return super.equals(obj);
    }
    org.tensorflow.proto.framework.ConfigProto other = (org.tensorflow.proto.framework.ConfigProto) obj;

    if (!internalGetDeviceCount().equals(
        other.internalGetDeviceCount())) return false;
    if (getIntraOpParallelismThreads()
        != other.getIntraOpParallelismThreads()) return false;
    if (getInterOpParallelismThreads()
        != other.getInterOpParallelismThreads()) return false;
    if (getUsePerSessionThreads()
        != other.getUsePerSessionThreads()) return false;
    if (!getSessionInterOpThreadPoolList()
        .equals(other.getSessionInterOpThreadPoolList())) return false;
    if (getPlacementPeriod()
        != other.getPlacementPeriod()) return false;
    if (!getDeviceFiltersList()
        .equals(other.getDeviceFiltersList())) return false;
    if (hasGpuOptions() != other.hasGpuOptions()) return false;
    if (hasGpuOptions()) {
      if (!getGpuOptions()
          .equals(other.getGpuOptions())) return false;
    }
    if (getAllowSoftPlacement()
        != other.getAllowSoftPlacement()) return false;
    if (getLogDevicePlacement()
        != other.getLogDevicePlacement()) return false;
    if (hasGraphOptions() != other.hasGraphOptions()) return false;
    if (hasGraphOptions()) {
      if (!getGraphOptions()
          .equals(other.getGraphOptions())) return false;
    }
    if (getOperationTimeoutInMs()
        != other.getOperationTimeoutInMs()) return false;
    if (hasRpcOptions() != other.hasRpcOptions()) return false;
    if (hasRpcOptions()) {
      if (!getRpcOptions()
          .equals(other.getRpcOptions())) return false;
    }
    if (hasClusterDef() != other.hasClusterDef()) return false;
    if (hasClusterDef()) {
      if (!getClusterDef()
          .equals(other.getClusterDef())) return false;
    }
    if (getIsolateSessionState()
        != other.getIsolateSessionState()) return false;
    if (getShareClusterDevicesInSession()
        != other.getShareClusterDevicesInSession()) return false;
    if (hasExperimental() != other.hasExperimental()) return false;
    if (hasExperimental()) {
      if (!getExperimental()
          .equals(other.getExperimental())) return false;
    }
    if (!unknownFields.equals(other.unknownFields)) return false;
    return true;
  }

  @java.lang.Override
  public int hashCode() {
    if (memoizedHashCode != 0) {
      return memoizedHashCode;
    }
    int hash = 41;
    hash = (19 * hash) + getDescriptor().hashCode();
    if (!internalGetDeviceCount().getMap().isEmpty()) {
      hash = (37 * hash) + DEVICE_COUNT_FIELD_NUMBER;
      hash = (53 * hash) + internalGetDeviceCount().hashCode();
    }
    hash = (37 * hash) + INTRA_OP_PARALLELISM_THREADS_FIELD_NUMBER;
    hash = (53 * hash) + getIntraOpParallelismThreads();
    hash = (37 * hash) + INTER_OP_PARALLELISM_THREADS_FIELD_NUMBER;
    hash = (53 * hash) + getInterOpParallelismThreads();
    hash = (37 * hash) + USE_PER_SESSION_THREADS_FIELD_NUMBER;
    hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
        getUsePerSessionThreads());
    if (getSessionInterOpThreadPoolCount() > 0) {
      hash = (37 * hash) + SESSION_INTER_OP_THREAD_POOL_FIELD_NUMBER;
      hash = (53 * hash) + getSessionInterOpThreadPoolList().hashCode();
    }
    hash = (37 * hash) + PLACEMENT_PERIOD_FIELD_NUMBER;
    hash = (53 * hash) + getPlacementPeriod();
    if (getDeviceFiltersCount() > 0) {
      hash = (37 * hash) + DEVICE_FILTERS_FIELD_NUMBER;
      hash = (53 * hash) + getDeviceFiltersList().hashCode();
    }
    if (hasGpuOptions()) {
      hash = (37 * hash) + GPU_OPTIONS_FIELD_NUMBER;
      hash = (53 * hash) + getGpuOptions().hashCode();
    }
    hash = (37 * hash) + ALLOW_SOFT_PLACEMENT_FIELD_NUMBER;
    hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
        getAllowSoftPlacement());
    hash = (37 * hash) + LOG_DEVICE_PLACEMENT_FIELD_NUMBER;
    hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
        getLogDevicePlacement());
    if (hasGraphOptions()) {
      hash = (37 * hash) + GRAPH_OPTIONS_FIELD_NUMBER;
      hash = (53 * hash) + getGraphOptions().hashCode();
    }
    hash = (37 * hash) + OPERATION_TIMEOUT_IN_MS_FIELD_NUMBER;
    hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
        getOperationTimeoutInMs());
    if (hasRpcOptions()) {
      hash = (37 * hash) + RPC_OPTIONS_FIELD_NUMBER;
      hash = (53 * hash) + getRpcOptions().hashCode();
    }
    if (hasClusterDef()) {
      hash = (37 * hash) + CLUSTER_DEF_FIELD_NUMBER;
      hash = (53 * hash) + getClusterDef().hashCode();
    }
    hash = (37 * hash) + ISOLATE_SESSION_STATE_FIELD_NUMBER;
    hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
        getIsolateSessionState());
    hash = (37 * hash) + SHARE_CLUSTER_DEVICES_IN_SESSION_FIELD_NUMBER;
    hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
        getShareClusterDevicesInSession());
    if (hasExperimental()) {
      hash = (37 * hash) + EXPERIMENTAL_FIELD_NUMBER;
      hash = (53 * hash) + getExperimental().hashCode();
    }
    hash = (29 * hash) + unknownFields.hashCode();
    memoizedHashCode = hash;
    return hash;
  }

  public static org.tensorflow.proto.framework.ConfigProto parseFrom(
      java.nio.ByteBuffer data)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data);
  }
  public static org.tensorflow.proto.framework.ConfigProto parseFrom(
      java.nio.ByteBuffer data,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data, extensionRegistry);
  }
  public static org.tensorflow.proto.framework.ConfigProto parseFrom(
      com.google.protobuf.ByteString data)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data);
  }
  public static org.tensorflow.proto.framework.ConfigProto parseFrom(
      com.google.protobuf.ByteString data,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data, extensionRegistry);
  }
  public static org.tensorflow.proto.framework.ConfigProto parseFrom(byte[] data)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data);
  }
  public static org.tensorflow.proto.framework.ConfigProto parseFrom(
      byte[] data,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data, extensionRegistry);
  }
  public static org.tensorflow.proto.framework.ConfigProto parseFrom(java.io.InputStream input)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageV3
        .parseWithIOException(PARSER, input);
  }
  public static org.tensorflow.proto.framework.ConfigProto parseFrom(
      java.io.InputStream input,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageV3
        .parseWithIOException(PARSER, input, extensionRegistry);
  }
  public static org.tensorflow.proto.framework.ConfigProto parseDelimitedFrom(java.io.InputStream input)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageV3
        .parseDelimitedWithIOException(PARSER, input);
  }
  public static org.tensorflow.proto.framework.ConfigProto parseDelimitedFrom(
      java.io.InputStream input,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageV3
        .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
  }
  public static org.tensorflow.proto.framework.ConfigProto parseFrom(
      com.google.protobuf.CodedInputStream input)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageV3
        .parseWithIOException(PARSER, input);
  }
  public static org.tensorflow.proto.framework.ConfigProto parseFrom(
      com.google.protobuf.CodedInputStream input,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageV3
        .parseWithIOException(PARSER, input, extensionRegistry);
  }

  @java.lang.Override
  public Builder newBuilderForType() { return newBuilder(); }
  public static Builder newBuilder() {
    return DEFAULT_INSTANCE.toBuilder();
  }
  public static Builder newBuilder(org.tensorflow.proto.framework.ConfigProto prototype) {
    return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
  }
  @java.lang.Override
  public Builder toBuilder() {
    return this == DEFAULT_INSTANCE
        ? new Builder() : new Builder().mergeFrom(this);
  }

  @java.lang.Override
  protected Builder newBuilderForType(
      com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
    Builder builder = new Builder(parent);
    return builder;
  }
  /**
   * <pre>
   * Session configuration parameters.
   * The system picks appropriate values for fields that are not set.
   * </pre>
   *
   * Protobuf type {@code tensorflow.ConfigProto}
   */
  public static final class Builder extends
      com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
      // @@protoc_insertion_point(builder_implements:tensorflow.ConfigProto)
      org.tensorflow.proto.framework.ConfigProtoOrBuilder {
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.tensorflow.proto.framework.ConfigProtos.internal_static_tensorflow_ConfigProto_descriptor;
    }

    @SuppressWarnings({"rawtypes"})
    protected com.google.protobuf.MapField internalGetMapField(
        int number) {
      switch (number) {
        case 1:
          return internalGetDeviceCount();
        default:
          throw new RuntimeException(
              "Invalid map field number: " + number);
      }
    }
    @SuppressWarnings({"rawtypes"})
    protected com.google.protobuf.MapField internalGetMutableMapField(
        int number) {
      switch (number) {
        case 1:
          return internalGetMutableDeviceCount();
        default:
          throw new RuntimeException(
              "Invalid map field number: " + number);
      }
    }
    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.tensorflow.proto.framework.ConfigProtos.internal_static_tensorflow_ConfigProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.tensorflow.proto.framework.ConfigProto.class, org.tensorflow.proto.framework.ConfigProto.Builder.class);
    }

    // Construct using org.tensorflow.proto.framework.ConfigProto.newBuilder()
    private Builder() {
      maybeForceBuilderInitialization();
    }

    private Builder(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      super(parent);
      maybeForceBuilderInitialization();
    }
    private void maybeForceBuilderInitialization() {
      if (com.google.protobuf.GeneratedMessageV3
              .alwaysUseFieldBuilders) {
        getSessionInterOpThreadPoolFieldBuilder();
      }
    }
    @java.lang.Override
    public Builder clear() {
      super.clear();
      internalGetMutableDeviceCount().clear();
      intraOpParallelismThreads_ = 0;

      interOpParallelismThreads_ = 0;

      usePerSessionThreads_ = false;

      if (sessionInterOpThreadPoolBuilder_ == null) {
        sessionInterOpThreadPool_ = java.util.Collections.emptyList();
        bitField0_ = (bitField0_ & ~0x00000002);
      } else {
        sessionInterOpThreadPoolBuilder_.clear();
      }
      placementPeriod_ = 0;

      deviceFilters_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      bitField0_ = (bitField0_ & ~0x00000004);
      if (gpuOptionsBuilder_ == null) {
        gpuOptions_ = null;
      } else {
        gpuOptions_ = null;
        gpuOptionsBuilder_ = null;
      }
      allowSoftPlacement_ = false;

      logDevicePlacement_ = false;

      if (graphOptionsBuilder_ == null) {
        graphOptions_ = null;
      } else {
        graphOptions_ = null;
        graphOptionsBuilder_ = null;
      }
      operationTimeoutInMs_ = 0L;

      if (rpcOptionsBuilder_ == null) {
        rpcOptions_ = null;
      } else {
        rpcOptions_ = null;
        rpcOptionsBuilder_ = null;
      }
      if (clusterDefBuilder_ == null) {
        clusterDef_ = null;
      } else {
        clusterDef_ = null;
        clusterDefBuilder_ = null;
      }
      isolateSessionState_ = false;

      shareClusterDevicesInSession_ = false;

      if (experimentalBuilder_ == null) {
        experimental_ = null;
      } else {
        experimental_ = null;
        experimentalBuilder_ = null;
      }
      return this;
    }

    @java.lang.Override
    public com.google.protobuf.Descriptors.Descriptor
        getDescriptorForType() {
      return org.tensorflow.proto.framework.ConfigProtos.internal_static_tensorflow_ConfigProto_descriptor;
    }

    @java.lang.Override
    public org.tensorflow.proto.framework.ConfigProto getDefaultInstanceForType() {
      return org.tensorflow.proto.framework.ConfigProto.getDefaultInstance();
    }

    @java.lang.Override
    public org.tensorflow.proto.framework.ConfigProto build() {
      org.tensorflow.proto.framework.ConfigProto result = buildPartial();
      if (!result.isInitialized()) {
        throw newUninitializedMessageException(result);
      }
      return result;
    }

    @java.lang.Override
    public org.tensorflow.proto.framework.ConfigProto buildPartial() {
      org.tensorflow.proto.framework.ConfigProto result = new org.tensorflow.proto.framework.ConfigProto(this);
      int from_bitField0_ = bitField0_;
      result.deviceCount_ = internalGetDeviceCount();
      result.deviceCount_.makeImmutable();
      result.intraOpParallelismThreads_ = intraOpParallelismThreads_;
      result.interOpParallelismThreads_ = interOpParallelismThreads_;
      result.usePerSessionThreads_ = usePerSessionThreads_;
      if (sessionInterOpThreadPoolBuilder_ == null) {
        if (((bitField0_ & 0x00000002) != 0)) {
          sessionInterOpThreadPool_ = java.util.Collections.unmodifiableList(sessionInterOpThreadPool_);
          bitField0_ = (bitField0_ & ~0x00000002);
        }
        result.sessionInterOpThreadPool_ = sessionInterOpThreadPool_;
      } else {
        result.sessionInterOpThreadPool_ = sessionInterOpThreadPoolBuilder_.build();
      }
      result.placementPeriod_ = placementPeriod_;
      if (((bitField0_ & 0x00000004) != 0)) {
        deviceFilters_ = deviceFilters_.getUnmodifiableView();
        bitField0_ = (bitField0_ & ~0x00000004);
      }
      result.deviceFilters_ = deviceFilters_;
      if (gpuOptionsBuilder_ == null) {
        result.gpuOptions_ = gpuOptions_;
      } else {
        result.gpuOptions_ = gpuOptionsBuilder_.build();
      }
      result.allowSoftPlacement_ = allowSoftPlacement_;
      result.logDevicePlacement_ = logDevicePlacement_;
      if (graphOptionsBuilder_ == null) {
        result.graphOptions_ = graphOptions_;
      } else {
        result.graphOptions_ = graphOptionsBuilder_.build();
      }
      result.operationTimeoutInMs_ = operationTimeoutInMs_;
      if (rpcOptionsBuilder_ == null) {
        result.rpcOptions_ = rpcOptions_;
      } else {
        result.rpcOptions_ = rpcOptionsBuilder_.build();
      }
      if (clusterDefBuilder_ == null) {
        result.clusterDef_ = clusterDef_;
      } else {
        result.clusterDef_ = clusterDefBuilder_.build();
      }
      result.isolateSessionState_ = isolateSessionState_;
      result.shareClusterDevicesInSession_ = shareClusterDevicesInSession_;
      if (experimentalBuilder_ == null) {
        result.experimental_ = experimental_;
      } else {
        result.experimental_ = experimentalBuilder_.build();
      }
      onBuilt();
      return result;
    }

    @java.lang.Override
    public Builder clone() {
      return super.clone();
    }
    @java.lang.Override
    public Builder setField(
        com.google.protobuf.Descriptors.FieldDescriptor field,
        java.lang.Object value) {
      return super.setField(field, value);
    }
    @java.lang.Override
    public Builder clearField(
        com.google.protobuf.Descriptors.FieldDescriptor field) {
      return super.clearField(field);
    }
    @java.lang.Override
    public Builder clearOneof(
        com.google.protobuf.Descriptors.OneofDescriptor oneof) {
      return super.clearOneof(oneof);
    }
    @java.lang.Override
    public Builder setRepeatedField(
        com.google.protobuf.Descriptors.FieldDescriptor field,
        int index, java.lang.Object value) {
      return super.setRepeatedField(field, index, value);
    }
    @java.lang.Override
    public Builder addRepeatedField(
        com.google.protobuf.Descriptors.FieldDescriptor field,
        java.lang.Object value) {
      return super.addRepeatedField(field, value);
    }
    @java.lang.Override
    public Builder mergeFrom(com.google.protobuf.Message other) {
      if (other instanceof org.tensorflow.proto.framework.ConfigProto) {
        return mergeFrom((org.tensorflow.proto.framework.ConfigProto)other);
      } else {
        super.mergeFrom(other);
        return this;
      }
    }

    public Builder mergeFrom(org.tensorflow.proto.framework.ConfigProto other) {
      if (other == org.tensorflow.proto.framework.ConfigProto.getDefaultInstance()) return this;
      internalGetMutableDeviceCount().mergeFrom(
          other.internalGetDeviceCount());
      if (other.getIntraOpParallelismThreads() != 0) {
        setIntraOpParallelismThreads(other.getIntraOpParallelismThreads());
      }
      if (other.getInterOpParallelismThreads() != 0) {
        setInterOpParallelismThreads(other.getInterOpParallelismThreads());
      }
      if (other.getUsePerSessionThreads() != false) {
        setUsePerSessionThreads(other.getUsePerSessionThreads());
      }
      if (sessionInterOpThreadPoolBuilder_ == null) {
        if (!other.sessionInterOpThreadPool_.isEmpty()) {
          if (sessionInterOpThreadPool_.isEmpty()) {
            sessionInterOpThreadPool_ = other.sessionInterOpThreadPool_;
            bitField0_ = (bitField0_ & ~0x00000002);
          } else {
            ensureSessionInterOpThreadPoolIsMutable();
            sessionInterOpThreadPool_.addAll(other.sessionInterOpThreadPool_);
          }
          onChanged();
        }
      } else {
        if (!other.sessionInterOpThreadPool_.isEmpty()) {
          if (sessionInterOpThreadPoolBuilder_.isEmpty()) {
            sessionInterOpThreadPoolBuilder_.dispose();
            sessionInterOpThreadPoolBuilder_ = null;
            sessionInterOpThreadPool_ = other.sessionInterOpThreadPool_;
            bitField0_ = (bitField0_ & ~0x00000002);
            sessionInterOpThreadPoolBuilder_ = 
              com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                 getSessionInterOpThreadPoolFieldBuilder() : null;
          } else {
            sessionInterOpThreadPoolBuilder_.addAllMessages(other.sessionInterOpThreadPool_);
          }
        }
      }
      if (other.getPlacementPeriod() != 0) {
        setPlacementPeriod(other.getPlacementPeriod());
      }
      if (!other.deviceFilters_.isEmpty()) {
        if (deviceFilters_.isEmpty()) {
          deviceFilters_ = other.deviceFilters_;
          bitField0_ = (bitField0_ & ~0x00000004);
        } else {
          ensureDeviceFiltersIsMutable();
          deviceFilters_.addAll(other.deviceFilters_);
        }
        onChanged();
      }
      if (other.hasGpuOptions()) {
        mergeGpuOptions(other.getGpuOptions());
      }
      if (other.getAllowSoftPlacement() != false) {
        setAllowSoftPlacement(other.getAllowSoftPlacement());
      }
      if (other.getLogDevicePlacement() != false) {
        setLogDevicePlacement(other.getLogDevicePlacement());
      }
      if (other.hasGraphOptions()) {
        mergeGraphOptions(other.getGraphOptions());
      }
      if (other.getOperationTimeoutInMs() != 0L) {
        setOperationTimeoutInMs(other.getOperationTimeoutInMs());
      }
      if (other.hasRpcOptions()) {
        mergeRpcOptions(other.getRpcOptions());
      }
      if (other.hasClusterDef()) {
        mergeClusterDef(other.getClusterDef());
      }
      if (other.getIsolateSessionState() != false) {
        setIsolateSessionState(other.getIsolateSessionState());
      }
      if (other.getShareClusterDevicesInSession() != false) {
        setShareClusterDevicesInSession(other.getShareClusterDevicesInSession());
      }
      if (other.hasExperimental()) {
        mergeExperimental(other.getExperimental());
      }
      this.mergeUnknownFields(other.unknownFields);
      onChanged();
      return this;
    }

    @java.lang.Override
    public final boolean isInitialized() {
      return true;
    }

    @java.lang.Override
    public Builder mergeFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      org.tensorflow.proto.framework.ConfigProto parsedMessage = null;
      try {
        parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        parsedMessage = (org.tensorflow.proto.framework.ConfigProto) e.getUnfinishedMessage();
        throw e.unwrapIOException();
      } finally {
        if (parsedMessage != null) {
          mergeFrom(parsedMessage);
        }
      }
      return this;
    }
    private int bitField0_;

    private com.google.protobuf.MapField<
        java.lang.String, java.lang.Integer> deviceCount_;
    private com.google.protobuf.MapField<java.lang.String, java.lang.Integer>
    internalGetDeviceCount() {
      if (deviceCount_ == null) {
        return com.google.protobuf.MapField.emptyMapField(
            DeviceCountDefaultEntryHolder.defaultEntry);
      }
      return deviceCount_;
    }
    private com.google.protobuf.MapField<java.lang.String, java.lang.Integer>
    internalGetMutableDeviceCount() {
      onChanged();;
      if (deviceCount_ == null) {
        deviceCount_ = com.google.protobuf.MapField.newMapField(
            DeviceCountDefaultEntryHolder.defaultEntry);
      }
      if (!deviceCount_.isMutable()) {
        deviceCount_ = deviceCount_.copy();
      }
      return deviceCount_;
    }

    public int getDeviceCountCount() {
      return internalGetDeviceCount().getMap().size();
    }
    /**
     * <pre>
     * Map from device type name (e.g., "CPU" or "GPU" ) to maximum
     * number of devices of that type to use.  If a particular device
     * type is not found in the map, the system picks an appropriate
     * number.
     * </pre>
     *
     * <code>map&lt;string, int32&gt; device_count = 1;</code>
     */

    public boolean containsDeviceCount(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      return internalGetDeviceCount().getMap().containsKey(key);
    }
    /**
     * Use {@link #getDeviceCountMap()} instead.
     */
    @java.lang.Deprecated
    public java.util.Map<java.lang.String, java.lang.Integer> getDeviceCount() {
      return getDeviceCountMap();
    }
    /**
     * <pre>
     * Map from device type name (e.g., "CPU" or "GPU" ) to maximum
     * number of devices of that type to use.  If a particular device
     * type is not found in the map, the system picks an appropriate
     * number.
     * </pre>
     *
     * <code>map&lt;string, int32&gt; device_count = 1;</code>
     */

    public java.util.Map<java.lang.String, java.lang.Integer> getDeviceCountMap() {
      return internalGetDeviceCount().getMap();
    }
    /**
     * <pre>
     * Map from device type name (e.g., "CPU" or "GPU" ) to maximum
     * number of devices of that type to use.  If a particular device
     * type is not found in the map, the system picks an appropriate
     * number.
     * </pre>
     *
     * <code>map&lt;string, int32&gt; device_count = 1;</code>
     */

    public int getDeviceCountOrDefault(
        java.lang.String key,
        int defaultValue) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.Integer> map =
          internalGetDeviceCount().getMap();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <pre>
     * Map from device type name (e.g., "CPU" or "GPU" ) to maximum
     * number of devices of that type to use.  If a particular device
     * type is not found in the map, the system picks an appropriate
     * number.
     * </pre>
     *
     * <code>map&lt;string, int32&gt; device_count = 1;</code>
     */

    public int getDeviceCountOrThrow(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.Integer> map =
          internalGetDeviceCount().getMap();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }

    public Builder clearDeviceCount() {
      internalGetMutableDeviceCount().getMutableMap()
          .clear();
      return this;
    }
    /**
     * <pre>
     * Map from device type name (e.g., "CPU" or "GPU" ) to maximum
     * number of devices of that type to use.  If a particular device
     * type is not found in the map, the system picks an appropriate
     * number.
     * </pre>
     *
     * <code>map&lt;string, int32&gt; device_count = 1;</code>
     */

    public Builder removeDeviceCount(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      internalGetMutableDeviceCount().getMutableMap()
          .remove(key);
      return this;
    }
    /**
     * Use alternate mutation accessors instead.
     */
    @java.lang.Deprecated
    public java.util.Map<java.lang.String, java.lang.Integer>
    getMutableDeviceCount() {
      return internalGetMutableDeviceCount().getMutableMap();
    }
    /**
     * <pre>
     * Map from device type name (e.g., "CPU" or "GPU" ) to maximum
     * number of devices of that type to use.  If a particular device
     * type is not found in the map, the system picks an appropriate
     * number.
     * </pre>
     *
     * <code>map&lt;string, int32&gt; device_count = 1;</code>
     */
    public Builder putDeviceCount(
        java.lang.String key,
        int value) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      
      internalGetMutableDeviceCount().getMutableMap()
          .put(key, value);
      return this;
    }
    /**
     * <pre>
     * Map from device type name (e.g., "CPU" or "GPU" ) to maximum
     * number of devices of that type to use.  If a particular device
     * type is not found in the map, the system picks an appropriate
     * number.
     * </pre>
     *
     * <code>map&lt;string, int32&gt; device_count = 1;</code>
     */

    public Builder putAllDeviceCount(
        java.util.Map<java.lang.String, java.lang.Integer> values) {
      internalGetMutableDeviceCount().getMutableMap()
          .putAll(values);
      return this;
    }

    private int intraOpParallelismThreads_ ;
    /**
     * <pre>
     * The execution of an individual op (for some op types) can be
     * parallelized on a pool of intra_op_parallelism_threads.
     * 0 means the system picks an appropriate number.
     * If you create an ordinary session, e.g., from Python or C++,
     * then there is exactly one intra op thread pool per process.
     * The first session created determines the number of threads in this pool.
     * All subsequent sessions reuse/share this one global pool.
     * There are notable exceptions to the default behavior described above:
     * 1. There is an environment variable  for overriding this thread pool,
     *    named TF_OVERRIDE_GLOBAL_THREADPOOL.
     * 2. When connecting to a server, such as a remote `tf.train.Server`
     *    instance, then this option will be ignored altogether.
     * </pre>
     *
     * <code>int32 intra_op_parallelism_threads = 2;</code>
     */
    public int getIntraOpParallelismThreads() {
      return intraOpParallelismThreads_;
    }
    /**
     * <pre>
     * The execution of an individual op (for some op types) can be
     * parallelized on a pool of intra_op_parallelism_threads.
     * 0 means the system picks an appropriate number.
     * If you create an ordinary session, e.g., from Python or C++,
     * then there is exactly one intra op thread pool per process.
     * The first session created determines the number of threads in this pool.
     * All subsequent sessions reuse/share this one global pool.
     * There are notable exceptions to the default behavior described above:
     * 1. There is an environment variable  for overriding this thread pool,
     *    named TF_OVERRIDE_GLOBAL_THREADPOOL.
     * 2. When connecting to a server, such as a remote `tf.train.Server`
     *    instance, then this option will be ignored altogether.
     * </pre>
     *
     * <code>int32 intra_op_parallelism_threads = 2;</code>
     */
    public Builder setIntraOpParallelismThreads(int value) {
      
      intraOpParallelismThreads_ = value;
      onChanged();
      return this;
    }
    /**
     * <pre>
     * The execution of an individual op (for some op types) can be
     * parallelized on a pool of intra_op_parallelism_threads.
     * 0 means the system picks an appropriate number.
     * If you create an ordinary session, e.g., from Python or C++,
     * then there is exactly one intra op thread pool per process.
     * The first session created determines the number of threads in this pool.
     * All subsequent sessions reuse/share this one global pool.
     * There are notable exceptions to the default behavior described above:
     * 1. There is an environment variable  for overriding this thread pool,
     *    named TF_OVERRIDE_GLOBAL_THREADPOOL.
     * 2. When connecting to a server, such as a remote `tf.train.Server`
     *    instance, then this option will be ignored altogether.
     * </pre>
     *
     * <code>int32 intra_op_parallelism_threads = 2;</code>
     */
    public Builder clearIntraOpParallelismThreads() {
      
      intraOpParallelismThreads_ = 0;
      onChanged();
      return this;
    }

    private int interOpParallelismThreads_ ;
    /**
     * <pre>
     * Nodes that perform blocking operations are enqueued on a pool of
     * inter_op_parallelism_threads available in each process.
     * 0 means the system picks an appropriate number.
     * Negative means all operations are performed in caller's thread.
     * Note that the first Session created in the process sets the
     * number of threads for all future sessions unless use_per_session_threads is
     * true or session_inter_op_thread_pool is configured.
     * </pre>
     *
     * <code>int32 inter_op_parallelism_threads = 5;</code>
     */
    public int getInterOpParallelismThreads() {
      return interOpParallelismThreads_;
    }
    /**
     * <pre>
     * Nodes that perform blocking operations are enqueued on a pool of
     * inter_op_parallelism_threads available in each process.
     * 0 means the system picks an appropriate number.
     * Negative means all operations are performed in caller's thread.
     * Note that the first Session created in the process sets the
     * number of threads for all future sessions unless use_per_session_threads is
     * true or session_inter_op_thread_pool is configured.
     * </pre>
     *
     * <code>int32 inter_op_parallelism_threads = 5;</code>
     */
    public Builder setInterOpParallelismThreads(int value) {
      
      interOpParallelismThreads_ = value;
      onChanged();
      return this;
    }
    /**
     * <pre>
     * Nodes that perform blocking operations are enqueued on a pool of
     * inter_op_parallelism_threads available in each process.
     * 0 means the system picks an appropriate number.
     * Negative means all operations are performed in caller's thread.
     * Note that the first Session created in the process sets the
     * number of threads for all future sessions unless use_per_session_threads is
     * true or session_inter_op_thread_pool is configured.
     * </pre>
     *
     * <code>int32 inter_op_parallelism_threads = 5;</code>
     */
    public Builder clearInterOpParallelismThreads() {
      
      interOpParallelismThreads_ = 0;
      onChanged();
      return this;
    }

    private boolean usePerSessionThreads_ ;
    /**
     * <pre>
     * If true, use a new set of threads for this session rather than the global
     * pool of threads. Only supported by direct sessions.
     * If false, use the global threads created by the first session, or the
     * per-session thread pools configured by session_inter_op_thread_pool.
     * This option is deprecated. The same effect can be achieved by setting
     * session_inter_op_thread_pool to have one element, whose num_threads equals
     * inter_op_parallelism_threads.
     * </pre>
     *
     * <code>bool use_per_session_threads = 9;</code>
     */
    public boolean getUsePerSessionThreads() {
      return usePerSessionThreads_;
    }
    /**
     * <pre>
     * If true, use a new set of threads for this session rather than the global
     * pool of threads. Only supported by direct sessions.
     * If false, use the global threads created by the first session, or the
     * per-session thread pools configured by session_inter_op_thread_pool.
     * This option is deprecated. The same effect can be achieved by setting
     * session_inter_op_thread_pool to have one element, whose num_threads equals
     * inter_op_parallelism_threads.
     * </pre>
     *
     * <code>bool use_per_session_threads = 9;</code>
     */
    public Builder setUsePerSessionThreads(boolean value) {
      
      usePerSessionThreads_ = value;
      onChanged();
      return this;
    }
    /**
     * <pre>
     * If true, use a new set of threads for this session rather than the global
     * pool of threads. Only supported by direct sessions.
     * If false, use the global threads created by the first session, or the
     * per-session thread pools configured by session_inter_op_thread_pool.
     * This option is deprecated. The same effect can be achieved by setting
     * session_inter_op_thread_pool to have one element, whose num_threads equals
     * inter_op_parallelism_threads.
     * </pre>
     *
     * <code>bool use_per_session_threads = 9;</code>
     */
    public Builder clearUsePerSessionThreads() {
      
      usePerSessionThreads_ = false;
      onChanged();
      return this;
    }

    private java.util.List<org.tensorflow.proto.framework.ThreadPoolOptionProto> sessionInterOpThreadPool_ =
      java.util.Collections.emptyList();
    private void ensureSessionInterOpThreadPoolIsMutable() {
      if (!((bitField0_ & 0x00000002) != 0)) {
        sessionInterOpThreadPool_ = new java.util.ArrayList<org.tensorflow.proto.framework.ThreadPoolOptionProto>(sessionInterOpThreadPool_);
        bitField0_ |= 0x00000002;
       }
    }

    private com.google.protobuf.RepeatedFieldBuilderV3<
        org.tensorflow.proto.framework.ThreadPoolOptionProto, org.tensorflow.proto.framework.ThreadPoolOptionProto.Builder, org.tensorflow.proto.framework.ThreadPoolOptionProtoOrBuilder> sessionInterOpThreadPoolBuilder_;

    /**
     * <pre>
     * This option is experimental - it may be replaced with a different mechanism
     * in the future.
     * Configures session thread pools. If this is configured, then RunOptions for
     * a Run call can select the thread pool to use.
     * The intended use is for when some session invocations need to run in a
     * background pool limited to a small number of threads:
     * - For example, a session may be configured to have one large pool (for
     * regular compute) and one small pool (for periodic, low priority work);
     * using the small pool is currently the mechanism for limiting the inter-op
     * parallelism of the low priority work.  Note that it does not limit the
     * parallelism of work spawned by a single op kernel implementation.
     * - Using this setting is normally not needed in training, but may help some
     * serving use cases.
     * - It is also generally recommended to set the global_name field of this
     * proto, to avoid creating multiple large pools. It is typically better to
     * run the non-low-priority work, even across sessions, in a single large
     * pool.
     * </pre>
     *
     * <code>repeated .tensorflow.ThreadPoolOptionProto session_inter_op_thread_pool = 12;</code>
     */
    public java.util.List<org.tensorflow.proto.framework.ThreadPoolOptionProto> getSessionInterOpThreadPoolList() {
      if (sessionInterOpThreadPoolBuilder_ == null) {
        return java.util.Collections.unmodifiableList(sessionInterOpThreadPool_);
      } else {
        return sessionInterOpThreadPoolBuilder_.getMessageList();
      }
    }
    /**
     * <pre>
     * This option is experimental - it may be replaced with a different mechanism
     * in the future.
     * Configures session thread pools. If this is configured, then RunOptions for
     * a Run call can select the thread pool to use.
     * The intended use is for when some session invocations need to run in a
     * background pool limited to a small number of threads:
     * - For example, a session may be configured to have one large pool (for
     * regular compute) and one small pool (for periodic, low priority work);
     * using the small pool is currently the mechanism for limiting the inter-op
     * parallelism of the low priority work.  Note that it does not limit the
     * parallelism of work spawned by a single op kernel implementation.
     * - Using this setting is normally not needed in training, but may help some
     * serving use cases.
     * - It is also generally recommended to set the global_name field of this
     * proto, to avoid creating multiple large pools. It is typically better to
     * run the non-low-priority work, even across sessions, in a single large
     * pool.
     * </pre>
     *
     * <code>repeated .tensorflow.ThreadPoolOptionProto session_inter_op_thread_pool = 12;</code>
     */
    public int getSessionInterOpThreadPoolCount() {
      if (sessionInterOpThreadPoolBuilder_ == null) {
        return sessionInterOpThreadPool_.size();
      } else {
        return sessionInterOpThreadPoolBuilder_.getCount();
      }
    }
    /**
     * <pre>
     * This option is experimental - it may be replaced with a different mechanism
     * in the future.
     * Configures session thread pools. If this is configured, then RunOptions for
     * a Run call can select the thread pool to use.
     * The intended use is for when some session invocations need to run in a
     * background pool limited to a small number of threads:
     * - For example, a session may be configured to have one large pool (for
     * regular compute) and one small pool (for periodic, low priority work);
     * using the small pool is currently the mechanism for limiting the inter-op
     * parallelism of the low priority work.  Note that it does not limit the
     * parallelism of work spawned by a single op kernel implementation.
     * - Using this setting is normally not needed in training, but may help some
     * serving use cases.
     * - It is also generally recommended to set the global_name field of this
     * proto, to avoid creating multiple large pools. It is typically better to
     * run the non-low-priority work, even across sessions, in a single large
     * pool.
     * </pre>
     *
     * <code>repeated .tensorflow.ThreadPoolOptionProto session_inter_op_thread_pool = 12;</code>
     */
    public org.tensorflow.proto.framework.ThreadPoolOptionProto getSessionInterOpThreadPool(int index) {
      if (sessionInterOpThreadPoolBuilder_ == null) {
        return sessionInterOpThreadPool_.get(index);
      } else {
        return sessionInterOpThreadPoolBuilder_.getMessage(index);
      }
    }
    /**
     * <pre>
     * This option is experimental - it may be replaced with a different mechanism
     * in the future.
     * Configures session thread pools. If this is configured, then RunOptions for
     * a Run call can select the thread pool to use.
     * The intended use is for when some session invocations need to run in a
     * background pool limited to a small number of threads:
     * - For example, a session may be configured to have one large pool (for
     * regular compute) and one small pool (for periodic, low priority work);
     * using the small pool is currently the mechanism for limiting the inter-op
     * parallelism of the low priority work.  Note that it does not limit the
     * parallelism of work spawned by a single op kernel implementation.
     * - Using this setting is normally not needed in training, but may help some
     * serving use cases.
     * - It is also generally recommended to set the global_name field of this
     * proto, to avoid creating multiple large pools. It is typically better to
     * run the non-low-priority work, even across sessions, in a single large
     * pool.
     * </pre>
     *
     * <code>repeated .tensorflow.ThreadPoolOptionProto session_inter_op_thread_pool = 12;</code>
     */
    public Builder setSessionInterOpThreadPool(
        int index, org.tensorflow.proto.framework.ThreadPoolOptionProto value) {
      if (sessionInterOpThreadPoolBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        ensureSessionInterOpThreadPoolIsMutable();
        sessionInterOpThreadPool_.set(index, value);
        onChanged();
      } else {
        sessionInterOpThreadPoolBuilder_.setMessage(index, value);
      }
      return this;
    }
    /**
     * <pre>
     * This option is experimental - it may be replaced with a different mechanism
     * in the future.
     * Configures session thread pools. If this is configured, then RunOptions for
     * a Run call can select the thread pool to use.
     * The intended use is for when some session invocations need to run in a
     * background pool limited to a small number of threads:
     * - For example, a session may be configured to have one large pool (for
     * regular compute) and one small pool (for periodic, low priority work);
     * using the small pool is currently the mechanism for limiting the inter-op
     * parallelism of the low priority work.  Note that it does not limit the
     * parallelism of work spawned by a single op kernel implementation.
     * - Using this setting is normally not needed in training, but may help some
     * serving use cases.
     * - It is also generally recommended to set the global_name field of this
     * proto, to avoid creating multiple large pools. It is typically better to
     * run the non-low-priority work, even across sessions, in a single large
     * pool.
     * </pre>
     *
     * <code>repeated .tensorflow.ThreadPoolOptionProto session_inter_op_thread_pool = 12;</code>
     */
    public Builder setSessionInterOpThreadPool(
        int index, org.tensorflow.proto.framework.ThreadPoolOptionProto.Builder builderForValue) {
      if (sessionInterOpThreadPoolBuilder_ == null) {
        ensureSessionInterOpThreadPoolIsMutable();
        sessionInterOpThreadPool_.set(index, builderForValue.build());
        onChanged();
      } else {
        sessionInterOpThreadPoolBuilder_.setMessage(index, builderForValue.build());
      }
      return this;
    }
    /**
     * <pre>
     * This option is experimental - it may be replaced with a different mechanism
     * in the future.
     * Configures session thread pools. If this is configured, then RunOptions for
     * a Run call can select the thread pool to use.
     * The intended use is for when some session invocations need to run in a
     * background pool limited to a small number of threads:
     * - For example, a session may be configured to have one large pool (for
     * regular compute) and one small pool (for periodic, low priority work);
     * using the small pool is currently the mechanism for limiting the inter-op
     * parallelism of the low priority work.  Note that it does not limit the
     * parallelism of work spawned by a single op kernel implementation.
     * - Using this setting is normally not needed in training, but may help some
     * serving use cases.
     * - It is also generally recommended to set the global_name field of this
     * proto, to avoid creating multiple large pools. It is typically better to
     * run the non-low-priority work, even across sessions, in a single large
     * pool.
     * </pre>
     *
     * <code>repeated .tensorflow.ThreadPoolOptionProto session_inter_op_thread_pool = 12;</code>
     */
    public Builder addSessionInterOpThreadPool(org.tensorflow.proto.framework.ThreadPoolOptionProto value) {
      if (sessionInterOpThreadPoolBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        ensureSessionInterOpThreadPoolIsMutable();
        sessionInterOpThreadPool_.add(value);
        onChanged();
      } else {
        sessionInterOpThreadPoolBuilder_.addMessage(value);
      }
      return this;
    }
    /**
     * <pre>
     * This option is experimental - it may be replaced with a different mechanism
     * in the future.
     * Configures session thread pools. If this is configured, then RunOptions for
     * a Run call can select the thread pool to use.
     * The intended use is for when some session invocations need to run in a
     * background pool limited to a small number of threads:
     * - For example, a session may be configured to have one large pool (for
     * regular compute) and one small pool (for periodic, low priority work);
     * using the small pool is currently the mechanism for limiting the inter-op
     * parallelism of the low priority work.  Note that it does not limit the
     * parallelism of work spawned by a single op kernel implementation.
     * - Using this setting is normally not needed in training, but may help some
     * serving use cases.
     * - It is also generally recommended to set the global_name field of this
     * proto, to avoid creating multiple large pools. It is typically better to
     * run the non-low-priority work, even across sessions, in a single large
     * pool.
     * </pre>
     *
     * <code>repeated .tensorflow.ThreadPoolOptionProto session_inter_op_thread_pool = 12;</code>
     */
    public Builder addSessionInterOpThreadPool(
        int index, org.tensorflow.proto.framework.ThreadPoolOptionProto value) {
      if (sessionInterOpThreadPoolBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        ensureSessionInterOpThreadPoolIsMutable();
        sessionInterOpThreadPool_.add(index, value);
        onChanged();
      } else {
        sessionInterOpThreadPoolBuilder_.addMessage(index, value);
      }
      return this;
    }
    /**
     * <pre>
     * This option is experimental - it may be replaced with a different mechanism
     * in the future.
     * Configures session thread pools. If this is configured, then RunOptions for
     * a Run call can select the thread pool to use.
     * The intended use is for when some session invocations need to run in a
     * background pool limited to a small number of threads:
     * - For example, a session may be configured to have one large pool (for
     * regular compute) and one small pool (for periodic, low priority work);
     * using the small pool is currently the mechanism for limiting the inter-op
     * parallelism of the low priority work.  Note that it does not limit the
     * parallelism of work spawned by a single op kernel implementation.
     * - Using this setting is normally not needed in training, but may help some
     * serving use cases.
     * - It is also generally recommended to set the global_name field of this
     * proto, to avoid creating multiple large pools. It is typically better to
     * run the non-low-priority work, even across sessions, in a single large
     * pool.
     * </pre>
     *
     * <code>repeated .tensorflow.ThreadPoolOptionProto session_inter_op_thread_pool = 12;</code>
     */
    public Builder addSessionInterOpThreadPool(
        org.tensorflow.proto.framework.ThreadPoolOptionProto.Builder builderForValue) {
      if (sessionInterOpThreadPoolBuilder_ == null) {
        ensureSessionInterOpThreadPoolIsMutable();
        sessionInterOpThreadPool_.add(builderForValue.build());
        onChanged();
      } else {
        sessionInterOpThreadPoolBuilder_.addMessage(builderForValue.build());
      }
      return this;
    }
    /**
     * <pre>
     * This option is experimental - it may be replaced with a different mechanism
     * in the future.
     * Configures session thread pools. If this is configured, then RunOptions for
     * a Run call can select the thread pool to use.
     * The intended use is for when some session invocations need to run in a
     * background pool limited to a small number of threads:
     * - For example, a session may be configured to have one large pool (for
     * regular compute) and one small pool (for periodic, low priority work);
     * using the small pool is currently the mechanism for limiting the inter-op
     * parallelism of the low priority work.  Note that it does not limit the
     * parallelism of work spawned by a single op kernel implementation.
     * - Using this setting is normally not needed in training, but may help some
     * serving use cases.
     * - It is also generally recommended to set the global_name field of this
     * proto, to avoid creating multiple large pools. It is typically better to
     * run the non-low-priority work, even across sessions, in a single large
     * pool.
     * </pre>
     *
     * <code>repeated .tensorflow.ThreadPoolOptionProto session_inter_op_thread_pool = 12;</code>
     */
    public Builder addSessionInterOpThreadPool(
        int index, org.tensorflow.proto.framework.ThreadPoolOptionProto.Builder builderForValue) {
      if (sessionInterOpThreadPoolBuilder_ == null) {
        ensureSessionInterOpThreadPoolIsMutable();
        sessionInterOpThreadPool_.add(index, builderForValue.build());
        onChanged();
      } else {
        sessionInterOpThreadPoolBuilder_.addMessage(index, builderForValue.build());
      }
      return this;
    }
    /**
     * <pre>
     * This option is experimental - it may be replaced with a different mechanism
     * in the future.
     * Configures session thread pools. If this is configured, then RunOptions for
     * a Run call can select the thread pool to use.
     * The intended use is for when some session invocations need to run in a
     * background pool limited to a small number of threads:
     * - For example, a session may be configured to have one large pool (for
     * regular compute) and one small pool (for periodic, low priority work);
     * using the small pool is currently the mechanism for limiting the inter-op
     * parallelism of the low priority work.  Note that it does not limit the
     * parallelism of work spawned by a single op kernel implementation.
     * - Using this setting is normally not needed in training, but may help some
     * serving use cases.
     * - It is also generally recommended to set the global_name field of this
     * proto, to avoid creating multiple large pools. It is typically better to
     * run the non-low-priority work, even across sessions, in a single large
     * pool.
     * </pre>
     *
     * <code>repeated .tensorflow.ThreadPoolOptionProto session_inter_op_thread_pool = 12;</code>
     */
    public Builder addAllSessionInterOpThreadPool(
        java.lang.Iterable<? extends org.tensorflow.proto.framework.ThreadPoolOptionProto> values) {
      if (sessionInterOpThreadPoolBuilder_ == null) {
        ensureSessionInterOpThreadPoolIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, sessionInterOpThreadPool_);
        onChanged();
      } else {
        sessionInterOpThreadPoolBuilder_.addAllMessages(values);
      }
      return this;
    }
    /**
     * <pre>
     * This option is experimental - it may be replaced with a different mechanism
     * in the future.
     * Configures session thread pools. If this is configured, then RunOptions for
     * a Run call can select the thread pool to use.
     * The intended use is for when some session invocations need to run in a
     * background pool limited to a small number of threads:
     * - For example, a session may be configured to have one large pool (for
     * regular compute) and one small pool (for periodic, low priority work);
     * using the small pool is currently the mechanism for limiting the inter-op
     * parallelism of the low priority work.  Note that it does not limit the
     * parallelism of work spawned by a single op kernel implementation.
     * - Using this setting is normally not needed in training, but may help some
     * serving use cases.
     * - It is also generally recommended to set the global_name field of this
     * proto, to avoid creating multiple large pools. It is typically better to
     * run the non-low-priority work, even across sessions, in a single large
     * pool.
     * </pre>
     *
     * <code>repeated .tensorflow.ThreadPoolOptionProto session_inter_op_thread_pool = 12;</code>
     */
    public Builder clearSessionInterOpThreadPool() {
      if (sessionInterOpThreadPoolBuilder_ == null) {
        sessionInterOpThreadPool_ = java.util.Collections.emptyList();
        bitField0_ = (bitField0_ & ~0x00000002);
        onChanged();
      } else {
        sessionInterOpThreadPoolBuilder_.clear();
      }
      return this;
    }
    /**
     * <pre>
     * This option is experimental - it may be replaced with a different mechanism
     * in the future.
     * Configures session thread pools. If this is configured, then RunOptions for
     * a Run call can select the thread pool to use.
     * The intended use is for when some session invocations need to run in a
     * background pool limited to a small number of threads:
     * - For example, a session may be configured to have one large pool (for
     * regular compute) and one small pool (for periodic, low priority work);
     * using the small pool is currently the mechanism for limiting the inter-op
     * parallelism of the low priority work.  Note that it does not limit the
     * parallelism of work spawned by a single op kernel implementation.
     * - Using this setting is normally not needed in training, but may help some
     * serving use cases.
     * - It is also generally recommended to set the global_name field of this
     * proto, to avoid creating multiple large pools. It is typically better to
     * run the non-low-priority work, even across sessions, in a single large
     * pool.
     * </pre>
     *
     * <code>repeated .tensorflow.ThreadPoolOptionProto session_inter_op_thread_pool = 12;</code>
     */
    public Builder removeSessionInterOpThreadPool(int index) {
      if (sessionInterOpThreadPoolBuilder_ == null) {
        ensureSessionInterOpThreadPoolIsMutable();
        sessionInterOpThreadPool_.remove(index);
        onChanged();
      } else {
        sessionInterOpThreadPoolBuilder_.remove(index);
      }
      return this;
    }
    /**
     * <pre>
     * This option is experimental - it may be replaced with a different mechanism
     * in the future.
     * Configures session thread pools. If this is configured, then RunOptions for
     * a Run call can select the thread pool to use.
     * The intended use is for when some session invocations need to run in a
     * background pool limited to a small number of threads:
     * - For example, a session may be configured to have one large pool (for
     * regular compute) and one small pool (for periodic, low priority work);
     * using the small pool is currently the mechanism for limiting the inter-op
     * parallelism of the low priority work.  Note that it does not limit the
     * parallelism of work spawned by a single op kernel implementation.
     * - Using this setting is normally not needed in training, but may help some
     * serving use cases.
     * - It is also generally recommended to set the global_name field of this
     * proto, to avoid creating multiple large pools. It is typically better to
     * run the non-low-priority work, even across sessions, in a single large
     * pool.
     * </pre>
     *
     * <code>repeated .tensorflow.ThreadPoolOptionProto session_inter_op_thread_pool = 12;</code>
     */
    public org.tensorflow.proto.framework.ThreadPoolOptionProto.Builder getSessionInterOpThreadPoolBuilder(
        int index) {
      return getSessionInterOpThreadPoolFieldBuilder().getBuilder(index);
    }
    /**
     * <pre>
     * This option is experimental - it may be replaced with a different mechanism
     * in the future.
     * Configures session thread pools. If this is configured, then RunOptions for
     * a Run call can select the thread pool to use.
     * The intended use is for when some session invocations need to run in a
     * background pool limited to a small number of threads:
     * - For example, a session may be configured to have one large pool (for
     * regular compute) and one small pool (for periodic, low priority work);
     * using the small pool is currently the mechanism for limiting the inter-op
     * parallelism of the low priority work.  Note that it does not limit the
     * parallelism of work spawned by a single op kernel implementation.
     * - Using this setting is normally not needed in training, but may help some
     * serving use cases.
     * - It is also generally recommended to set the global_name field of this
     * proto, to avoid creating multiple large pools. It is typically better to
     * run the non-low-priority work, even across sessions, in a single large
     * pool.
     * </pre>
     *
     * <code>repeated .tensorflow.ThreadPoolOptionProto session_inter_op_thread_pool = 12;</code>
     */
    public org.tensorflow.proto.framework.ThreadPoolOptionProtoOrBuilder getSessionInterOpThreadPoolOrBuilder(
        int index) {
      if (sessionInterOpThreadPoolBuilder_ == null) {
        return sessionInterOpThreadPool_.get(index);  } else {
        return sessionInterOpThreadPoolBuilder_.getMessageOrBuilder(index);
      }
    }
    /**
     * <pre>
     * This option is experimental - it may be replaced with a different mechanism
     * in the future.
     * Configures session thread pools. If this is configured, then RunOptions for
     * a Run call can select the thread pool to use.
     * The intended use is for when some session invocations need to run in a
     * background pool limited to a small number of threads:
     * - For example, a session may be configured to have one large pool (for
     * regular compute) and one small pool (for periodic, low priority work);
     * using the small pool is currently the mechanism for limiting the inter-op
     * parallelism of the low priority work.  Note that it does not limit the
     * parallelism of work spawned by a single op kernel implementation.
     * - Using this setting is normally not needed in training, but may help some
     * serving use cases.
     * - It is also generally recommended to set the global_name field of this
     * proto, to avoid creating multiple large pools. It is typically better to
     * run the non-low-priority work, even across sessions, in a single large
     * pool.
     * </pre>
     *
     * <code>repeated .tensorflow.ThreadPoolOptionProto session_inter_op_thread_pool = 12;</code>
     */
    public java.util.List<? extends org.tensorflow.proto.framework.ThreadPoolOptionProtoOrBuilder> 
         getSessionInterOpThreadPoolOrBuilderList() {
      if (sessionInterOpThreadPoolBuilder_ != null) {
        return sessionInterOpThreadPoolBuilder_.getMessageOrBuilderList();
      } else {
        return java.util.Collections.unmodifiableList(sessionInterOpThreadPool_);
      }
    }
    /**
     * <pre>
     * This option is experimental - it may be replaced with a different mechanism
     * in the future.
     * Configures session thread pools. If this is configured, then RunOptions for
     * a Run call can select the thread pool to use.
     * The intended use is for when some session invocations need to run in a
     * background pool limited to a small number of threads:
     * - For example, a session may be configured to have one large pool (for
     * regular compute) and one small pool (for periodic, low priority work);
     * using the small pool is currently the mechanism for limiting the inter-op
     * parallelism of the low priority work.  Note that it does not limit the
     * parallelism of work spawned by a single op kernel implementation.
     * - Using this setting is normally not needed in training, but may help some
     * serving use cases.
     * - It is also generally recommended to set the global_name field of this
     * proto, to avoid creating multiple large pools. It is typically better to
     * run the non-low-priority work, even across sessions, in a single large
     * pool.
     * </pre>
     *
     * <code>repeated .tensorflow.ThreadPoolOptionProto session_inter_op_thread_pool = 12;</code>
     */
    public org.tensorflow.proto.framework.ThreadPoolOptionProto.Builder addSessionInterOpThreadPoolBuilder() {
      return getSessionInterOpThreadPoolFieldBuilder().addBuilder(
          org.tensorflow.proto.framework.ThreadPoolOptionProto.getDefaultInstance());
    }
    /**
     * <pre>
     * This option is experimental - it may be replaced with a different mechanism
     * in the future.
     * Configures session thread pools. If this is configured, then RunOptions for
     * a Run call can select the thread pool to use.
     * The intended use is for when some session invocations need to run in a
     * background pool limited to a small number of threads:
     * - For example, a session may be configured to have one large pool (for
     * regular compute) and one small pool (for periodic, low priority work);
     * using the small pool is currently the mechanism for limiting the inter-op
     * parallelism of the low priority work.  Note that it does not limit the
     * parallelism of work spawned by a single op kernel implementation.
     * - Using this setting is normally not needed in training, but may help some
     * serving use cases.
     * - It is also generally recommended to set the global_name field of this
     * proto, to avoid creating multiple large pools. It is typically better to
     * run the non-low-priority work, even across sessions, in a single large
     * pool.
     * </pre>
     *
     * <code>repeated .tensorflow.ThreadPoolOptionProto session_inter_op_thread_pool = 12;</code>
     */
    public org.tensorflow.proto.framework.ThreadPoolOptionProto.Builder addSessionInterOpThreadPoolBuilder(
        int index) {
      return getSessionInterOpThreadPoolFieldBuilder().addBuilder(
          index, org.tensorflow.proto.framework.ThreadPoolOptionProto.getDefaultInstance());
    }
    /**
     * <pre>
     * This option is experimental - it may be replaced with a different mechanism
     * in the future.
     * Configures session thread pools. If this is configured, then RunOptions for
     * a Run call can select the thread pool to use.
     * The intended use is for when some session invocations need to run in a
     * background pool limited to a small number of threads:
     * - For example, a session may be configured to have one large pool (for
     * regular compute) and one small pool (for periodic, low priority work);
     * using the small pool is currently the mechanism for limiting the inter-op
     * parallelism of the low priority work.  Note that it does not limit the
     * parallelism of work spawned by a single op kernel implementation.
     * - Using this setting is normally not needed in training, but may help some
     * serving use cases.
     * - It is also generally recommended to set the global_name field of this
     * proto, to avoid creating multiple large pools. It is typically better to
     * run the non-low-priority work, even across sessions, in a single large
     * pool.
     * </pre>
     *
     * <code>repeated .tensorflow.ThreadPoolOptionProto session_inter_op_thread_pool = 12;</code>
     */
    public java.util.List<org.tensorflow.proto.framework.ThreadPoolOptionProto.Builder> 
         getSessionInterOpThreadPoolBuilderList() {
      return getSessionInterOpThreadPoolFieldBuilder().getBuilderList();
    }
    private com.google.protobuf.RepeatedFieldBuilderV3<
        org.tensorflow.proto.framework.ThreadPoolOptionProto, org.tensorflow.proto.framework.ThreadPoolOptionProto.Builder, org.tensorflow.proto.framework.ThreadPoolOptionProtoOrBuilder> 
        getSessionInterOpThreadPoolFieldBuilder() {
      if (sessionInterOpThreadPoolBuilder_ == null) {
        sessionInterOpThreadPoolBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
            org.tensorflow.proto.framework.ThreadPoolOptionProto, org.tensorflow.proto.framework.ThreadPoolOptionProto.Builder, org.tensorflow.proto.framework.ThreadPoolOptionProtoOrBuilder>(
                sessionInterOpThreadPool_,
                ((bitField0_ & 0x00000002) != 0),
                getParentForChildren(),
                isClean());
        sessionInterOpThreadPool_ = null;
      }
      return sessionInterOpThreadPoolBuilder_;
    }

    private int placementPeriod_ ;
    /**
     * <pre>
     * Assignment of Nodes to Devices is recomputed every placement_period
     * steps until the system warms up (at which point the recomputation
     * typically slows down automatically).
     * </pre>
     *
     * <code>int32 placement_period = 3;</code>
     */
    public int getPlacementPeriod() {
      return placementPeriod_;
    }
    /**
     * <pre>
     * Assignment of Nodes to Devices is recomputed every placement_period
     * steps until the system warms up (at which point the recomputation
     * typically slows down automatically).
     * </pre>
     *
     * <code>int32 placement_period = 3;</code>
     */
    public Builder setPlacementPeriod(int value) {
      
      placementPeriod_ = value;
      onChanged();
      return this;
    }
    /**
     * <pre>
     * Assignment of Nodes to Devices is recomputed every placement_period
     * steps until the system warms up (at which point the recomputation
     * typically slows down automatically).
     * </pre>
     *
     * <code>int32 placement_period = 3;</code>
     */
    public Builder clearPlacementPeriod() {
      
      placementPeriod_ = 0;
      onChanged();
      return this;
    }

    private com.google.protobuf.LazyStringList deviceFilters_ = com.google.protobuf.LazyStringArrayList.EMPTY;
    private void ensureDeviceFiltersIsMutable() {
      if (!((bitField0_ & 0x00000004) != 0)) {
        deviceFilters_ = new com.google.protobuf.LazyStringArrayList(deviceFilters_);
        bitField0_ |= 0x00000004;
       }
    }
    /**
     * <pre>
     * When any filters are present sessions will ignore all devices which do not
     * match the filters. Each filter can be partially specified, e.g. "/job:ps"
     * "/job:worker/replica:3", etc.
     * </pre>
     *
     * <code>repeated string device_filters = 4;</code>
     */
    public com.google.protobuf.ProtocolStringList
        getDeviceFiltersList() {
      return deviceFilters_.getUnmodifiableView();
    }
    /**
     * <pre>
     * When any filters are present sessions will ignore all devices which do not
     * match the filters. Each filter can be partially specified, e.g. "/job:ps"
     * "/job:worker/replica:3", etc.
     * </pre>
     *
     * <code>repeated string device_filters = 4;</code>
     */
    public int getDeviceFiltersCount() {
      return deviceFilters_.size();
    }
    /**
     * <pre>
     * When any filters are present sessions will ignore all devices which do not
     * match the filters. Each filter can be partially specified, e.g. "/job:ps"
     * "/job:worker/replica:3", etc.
     * </pre>
     *
     * <code>repeated string device_filters = 4;</code>
     */
    public java.lang.String getDeviceFilters(int index) {
      return deviceFilters_.get(index);
    }
    /**
     * <pre>
     * When any filters are present sessions will ignore all devices which do not
     * match the filters. Each filter can be partially specified, e.g. "/job:ps"
     * "/job:worker/replica:3", etc.
     * </pre>
     *
     * <code>repeated string device_filters = 4;</code>
     */
    public com.google.protobuf.ByteString
        getDeviceFiltersBytes(int index) {
      return deviceFilters_.getByteString(index);
    }
    /**
     * <pre>
     * When any filters are present sessions will ignore all devices which do not
     * match the filters. Each filter can be partially specified, e.g. "/job:ps"
     * "/job:worker/replica:3", etc.
     * </pre>
     *
     * <code>repeated string device_filters = 4;</code>
     */
    public Builder setDeviceFilters(
        int index, java.lang.String value) {
      if (value == null) {
    throw new NullPointerException();
  }
  ensureDeviceFiltersIsMutable();
      deviceFilters_.set(index, value);
      onChanged();
      return this;
    }
    /**
     * <pre>
     * When any filters are present sessions will ignore all devices which do not
     * match the filters. Each filter can be partially specified, e.g. "/job:ps"
     * "/job:worker/replica:3", etc.
     * </pre>
     *
     * <code>repeated string device_filters = 4;</code>
     */
    public Builder addDeviceFilters(
        java.lang.String value) {
      if (value == null) {
    throw new NullPointerException();
  }
  ensureDeviceFiltersIsMutable();
      deviceFilters_.add(value);
      onChanged();
      return this;
    }
    /**
     * <pre>
     * When any filters are present sessions will ignore all devices which do not
     * match the filters. Each filter can be partially specified, e.g. "/job:ps"
     * "/job:worker/replica:3", etc.
     * </pre>
     *
     * <code>repeated string device_filters = 4;</code>
     */
    public Builder addAllDeviceFilters(
        java.lang.Iterable<java.lang.String> values) {
      ensureDeviceFiltersIsMutable();
      com.google.protobuf.AbstractMessageLite.Builder.addAll(
          values, deviceFilters_);
      onChanged();
      return this;
    }
    /**
     * <pre>
     * When any filters are present sessions will ignore all devices which do not
     * match the filters. Each filter can be partially specified, e.g. "/job:ps"
     * "/job:worker/replica:3", etc.
     * </pre>
     *
     * <code>repeated string device_filters = 4;</code>
     */
    public Builder clearDeviceFilters() {
      deviceFilters_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      bitField0_ = (bitField0_ & ~0x00000004);
      onChanged();
      return this;
    }
    /**
     * <pre>
     * When any filters are present sessions will ignore all devices which do not
     * match the filters. Each filter can be partially specified, e.g. "/job:ps"
     * "/job:worker/replica:3", etc.
     * </pre>
     *
     * <code>repeated string device_filters = 4;</code>
     */
    public Builder addDeviceFiltersBytes(
        com.google.protobuf.ByteString value) {
      if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
      ensureDeviceFiltersIsMutable();
      deviceFilters_.add(value);
      onChanged();
      return this;
    }

    private org.tensorflow.proto.framework.GPUOptions gpuOptions_;
    private com.google.protobuf.SingleFieldBuilderV3<
        org.tensorflow.proto.framework.GPUOptions, org.tensorflow.proto.framework.GPUOptions.Builder, org.tensorflow.proto.framework.GPUOptionsOrBuilder> gpuOptionsBuilder_;
    /**
     * <pre>
     * Options that apply to all GPUs.
     * </pre>
     *
     * <code>.tensorflow.GPUOptions gpu_options = 6;</code>
     */
    public boolean hasGpuOptions() {
      return gpuOptionsBuilder_ != null || gpuOptions_ != null;
    }
    /**
     * <pre>
     * Options that apply to all GPUs.
     * </pre>
     *
     * <code>.tensorflow.GPUOptions gpu_options = 6;</code>
     */
    public org.tensorflow.proto.framework.GPUOptions getGpuOptions() {
      if (gpuOptionsBuilder_ == null) {
        return gpuOptions_ == null ? org.tensorflow.proto.framework.GPUOptions.getDefaultInstance() : gpuOptions_;
      } else {
        return gpuOptionsBuilder_.getMessage();
      }
    }
    /**
     * <pre>
     * Options that apply to all GPUs.
     * </pre>
     *
     * <code>.tensorflow.GPUOptions gpu_options = 6;</code>
     */
    public Builder setGpuOptions(org.tensorflow.proto.framework.GPUOptions value) {
      if (gpuOptionsBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        gpuOptions_ = value;
        onChanged();
      } else {
        gpuOptionsBuilder_.setMessage(value);
      }

      return this;
    }
    /**
     * <pre>
     * Options that apply to all GPUs.
     * </pre>
     *
     * <code>.tensorflow.GPUOptions gpu_options = 6;</code>
     */
    public Builder setGpuOptions(
        org.tensorflow.proto.framework.GPUOptions.Builder builderForValue) {
      if (gpuOptionsBuilder_ == null) {
        gpuOptions_ = builderForValue.build();
        onChanged();
      } else {
        gpuOptionsBuilder_.setMessage(builderForValue.build());
      }

      return this;
    }
    /**
     * <pre>
     * Options that apply to all GPUs.
     * </pre>
     *
     * <code>.tensorflow.GPUOptions gpu_options = 6;</code>
     */
    public Builder mergeGpuOptions(org.tensorflow.proto.framework.GPUOptions value) {
      if (gpuOptionsBuilder_ == null) {
        if (gpuOptions_ != null) {
          gpuOptions_ =
            org.tensorflow.proto.framework.GPUOptions.newBuilder(gpuOptions_).mergeFrom(value).buildPartial();
        } else {
          gpuOptions_ = value;
        }
        onChanged();
      } else {
        gpuOptionsBuilder_.mergeFrom(value);
      }

      return this;
    }
    /**
     * <pre>
     * Options that apply to all GPUs.
     * </pre>
     *
     * <code>.tensorflow.GPUOptions gpu_options = 6;</code>
     */
    public Builder clearGpuOptions() {
      if (gpuOptionsBuilder_ == null) {
        gpuOptions_ = null;
        onChanged();
      } else {
        gpuOptions_ = null;
        gpuOptionsBuilder_ = null;
      }

      return this;
    }
    /**
     * <pre>
     * Options that apply to all GPUs.
     * </pre>
     *
     * <code>.tensorflow.GPUOptions gpu_options = 6;</code>
     */
    public org.tensorflow.proto.framework.GPUOptions.Builder getGpuOptionsBuilder() {
      
      onChanged();
      return getGpuOptionsFieldBuilder().getBuilder();
    }
    /**
     * <pre>
     * Options that apply to all GPUs.
     * </pre>
     *
     * <code>.tensorflow.GPUOptions gpu_options = 6;</code>
     */
    public org.tensorflow.proto.framework.GPUOptionsOrBuilder getGpuOptionsOrBuilder() {
      if (gpuOptionsBuilder_ != null) {
        return gpuOptionsBuilder_.getMessageOrBuilder();
      } else {
        return gpuOptions_ == null ?
            org.tensorflow.proto.framework.GPUOptions.getDefaultInstance() : gpuOptions_;
      }
    }
    /**
     * <pre>
     * Options that apply to all GPUs.
     * </pre>
     *
     * <code>.tensorflow.GPUOptions gpu_options = 6;</code>
     */
    private com.google.protobuf.SingleFieldBuilderV3<
        org.tensorflow.proto.framework.GPUOptions, org.tensorflow.proto.framework.GPUOptions.Builder, org.tensorflow.proto.framework.GPUOptionsOrBuilder> 
        getGpuOptionsFieldBuilder() {
      if (gpuOptionsBuilder_ == null) {
        gpuOptionsBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
            org.tensorflow.proto.framework.GPUOptions, org.tensorflow.proto.framework.GPUOptions.Builder, org.tensorflow.proto.framework.GPUOptionsOrBuilder>(
                getGpuOptions(),
                getParentForChildren(),
                isClean());
        gpuOptions_ = null;
      }
      return gpuOptionsBuilder_;
    }

    private boolean allowSoftPlacement_ ;
    /**
     * <pre>
     * Whether soft placement is allowed. If allow_soft_placement is true,
     * an op will be placed on CPU if
     *   1. there's no GPU implementation for the OP
     * or
     *   2. no GPU devices are known or registered
     * or
     *   3. need to co-locate with reftype input(s) which are from CPU.
     * </pre>
     *
     * <code>bool allow_soft_placement = 7;</code>
     */
    public boolean getAllowSoftPlacement() {
      return allowSoftPlacement_;
    }
    /**
     * <pre>
     * Whether soft placement is allowed. If allow_soft_placement is true,
     * an op will be placed on CPU if
     *   1. there's no GPU implementation for the OP
     * or
     *   2. no GPU devices are known or registered
     * or
     *   3. need to co-locate with reftype input(s) which are from CPU.
     * </pre>
     *
     * <code>bool allow_soft_placement = 7;</code>
     */
    public Builder setAllowSoftPlacement(boolean value) {
      
      allowSoftPlacement_ = value;
      onChanged();
      return this;
    }
    /**
     * <pre>
     * Whether soft placement is allowed. If allow_soft_placement is true,
     * an op will be placed on CPU if
     *   1. there's no GPU implementation for the OP
     * or
     *   2. no GPU devices are known or registered
     * or
     *   3. need to co-locate with reftype input(s) which are from CPU.
     * </pre>
     *
     * <code>bool allow_soft_placement = 7;</code>
     */
    public Builder clearAllowSoftPlacement() {
      
      allowSoftPlacement_ = false;
      onChanged();
      return this;
    }

    private boolean logDevicePlacement_ ;
    /**
     * <pre>
     * Whether device placements should be logged.
     * </pre>
     *
     * <code>bool log_device_placement = 8;</code>
     */
    public boolean getLogDevicePlacement() {
      return logDevicePlacement_;
    }
    /**
     * <pre>
     * Whether device placements should be logged.
     * </pre>
     *
     * <code>bool log_device_placement = 8;</code>
     */
    public Builder setLogDevicePlacement(boolean value) {
      
      logDevicePlacement_ = value;
      onChanged();
      return this;
    }
    /**
     * <pre>
     * Whether device placements should be logged.
     * </pre>
     *
     * <code>bool log_device_placement = 8;</code>
     */
    public Builder clearLogDevicePlacement() {
      
      logDevicePlacement_ = false;
      onChanged();
      return this;
    }

    private org.tensorflow.proto.framework.GraphOptions graphOptions_;
    private com.google.protobuf.SingleFieldBuilderV3<
        org.tensorflow.proto.framework.GraphOptions, org.tensorflow.proto.framework.GraphOptions.Builder, org.tensorflow.proto.framework.GraphOptionsOrBuilder> graphOptionsBuilder_;
    /**
     * <pre>
     * Options that apply to all graphs.
     * </pre>
     *
     * <code>.tensorflow.GraphOptions graph_options = 10;</code>
     */
    public boolean hasGraphOptions() {
      return graphOptionsBuilder_ != null || graphOptions_ != null;
    }
    /**
     * <pre>
     * Options that apply to all graphs.
     * </pre>
     *
     * <code>.tensorflow.GraphOptions graph_options = 10;</code>
     */
    public org.tensorflow.proto.framework.GraphOptions getGraphOptions() {
      if (graphOptionsBuilder_ == null) {
        return graphOptions_ == null ? org.tensorflow.proto.framework.GraphOptions.getDefaultInstance() : graphOptions_;
      } else {
        return graphOptionsBuilder_.getMessage();
      }
    }
    /**
     * <pre>
     * Options that apply to all graphs.
     * </pre>
     *
     * <code>.tensorflow.GraphOptions graph_options = 10;</code>
     */
    public Builder setGraphOptions(org.tensorflow.proto.framework.GraphOptions value) {
      if (graphOptionsBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        graphOptions_ = value;
        onChanged();
      } else {
        graphOptionsBuilder_.setMessage(value);
      }

      return this;
    }
    /**
     * <pre>
     * Options that apply to all graphs.
     * </pre>
     *
     * <code>.tensorflow.GraphOptions graph_options = 10;</code>
     */
    public Builder setGraphOptions(
        org.tensorflow.proto.framework.GraphOptions.Builder builderForValue) {
      if (graphOptionsBuilder_ == null) {
        graphOptions_ = builderForValue.build();
        onChanged();
      } else {
        graphOptionsBuilder_.setMessage(builderForValue.build());
      }

      return this;
    }
    /**
     * <pre>
     * Options that apply to all graphs.
     * </pre>
     *
     * <code>.tensorflow.GraphOptions graph_options = 10;</code>
     */
    public Builder mergeGraphOptions(org.tensorflow.proto.framework.GraphOptions value) {
      if (graphOptionsBuilder_ == null) {
        if (graphOptions_ != null) {
          graphOptions_ =
            org.tensorflow.proto.framework.GraphOptions.newBuilder(graphOptions_).mergeFrom(value).buildPartial();
        } else {
          graphOptions_ = value;
        }
        onChanged();
      } else {
        graphOptionsBuilder_.mergeFrom(value);
      }

      return this;
    }
    /**
     * <pre>
     * Options that apply to all graphs.
     * </pre>
     *
     * <code>.tensorflow.GraphOptions graph_options = 10;</code>
     */
    public Builder clearGraphOptions() {
      if (graphOptionsBuilder_ == null) {
        graphOptions_ = null;
        onChanged();
      } else {
        graphOptions_ = null;
        graphOptionsBuilder_ = null;
      }

      return this;
    }
    /**
     * <pre>
     * Options that apply to all graphs.
     * </pre>
     *
     * <code>.tensorflow.GraphOptions graph_options = 10;</code>
     */
    public org.tensorflow.proto.framework.GraphOptions.Builder getGraphOptionsBuilder() {
      
      onChanged();
      return getGraphOptionsFieldBuilder().getBuilder();
    }
    /**
     * <pre>
     * Options that apply to all graphs.
     * </pre>
     *
     * <code>.tensorflow.GraphOptions graph_options = 10;</code>
     */
    public org.tensorflow.proto.framework.GraphOptionsOrBuilder getGraphOptionsOrBuilder() {
      if (graphOptionsBuilder_ != null) {
        return graphOptionsBuilder_.getMessageOrBuilder();
      } else {
        return graphOptions_ == null ?
            org.tensorflow.proto.framework.GraphOptions.getDefaultInstance() : graphOptions_;
      }
    }
    /**
     * <pre>
     * Options that apply to all graphs.
     * </pre>
     *
     * <code>.tensorflow.GraphOptions graph_options = 10;</code>
     */
    private com.google.protobuf.SingleFieldBuilderV3<
        org.tensorflow.proto.framework.GraphOptions, org.tensorflow.proto.framework.GraphOptions.Builder, org.tensorflow.proto.framework.GraphOptionsOrBuilder> 
        getGraphOptionsFieldBuilder() {
      if (graphOptionsBuilder_ == null) {
        graphOptionsBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
            org.tensorflow.proto.framework.GraphOptions, org.tensorflow.proto.framework.GraphOptions.Builder, org.tensorflow.proto.framework.GraphOptionsOrBuilder>(
                getGraphOptions(),
                getParentForChildren(),
                isClean());
        graphOptions_ = null;
      }
      return graphOptionsBuilder_;
    }

    private long operationTimeoutInMs_ ;
    /**
     * <pre>
     * Global timeout for all blocking operations in this session.  If non-zero,
     * and not overridden on a per-operation basis, this value will be used as the
     * deadline for all blocking operations.
     * </pre>
     *
     * <code>int64 operation_timeout_in_ms = 11;</code>
     */
    public long getOperationTimeoutInMs() {
      return operationTimeoutInMs_;
    }
    /**
     * <pre>
     * Global timeout for all blocking operations in this session.  If non-zero,
     * and not overridden on a per-operation basis, this value will be used as the
     * deadline for all blocking operations.
     * </pre>
     *
     * <code>int64 operation_timeout_in_ms = 11;</code>
     */
    public Builder setOperationTimeoutInMs(long value) {
      
      operationTimeoutInMs_ = value;
      onChanged();
      return this;
    }
    /**
     * <pre>
     * Global timeout for all blocking operations in this session.  If non-zero,
     * and not overridden on a per-operation basis, this value will be used as the
     * deadline for all blocking operations.
     * </pre>
     *
     * <code>int64 operation_timeout_in_ms = 11;</code>
     */
    public Builder clearOperationTimeoutInMs() {
      
      operationTimeoutInMs_ = 0L;
      onChanged();
      return this;
    }

    private org.tensorflow.proto.framework.RPCOptions rpcOptions_;
    private com.google.protobuf.SingleFieldBuilderV3<
        org.tensorflow.proto.framework.RPCOptions, org.tensorflow.proto.framework.RPCOptions.Builder, org.tensorflow.proto.framework.RPCOptionsOrBuilder> rpcOptionsBuilder_;
    /**
     * <pre>
     * Options that apply when this session uses the distributed runtime.
     * </pre>
     *
     * <code>.tensorflow.RPCOptions rpc_options = 13;</code>
     */
    public boolean hasRpcOptions() {
      return rpcOptionsBuilder_ != null || rpcOptions_ != null;
    }
    /**
     * <pre>
     * Options that apply when this session uses the distributed runtime.
     * </pre>
     *
     * <code>.tensorflow.RPCOptions rpc_options = 13;</code>
     */
    public org.tensorflow.proto.framework.RPCOptions getRpcOptions() {
      if (rpcOptionsBuilder_ == null) {
        return rpcOptions_ == null ? org.tensorflow.proto.framework.RPCOptions.getDefaultInstance() : rpcOptions_;
      } else {
        return rpcOptionsBuilder_.getMessage();
      }
    }
    /**
     * <pre>
     * Options that apply when this session uses the distributed runtime.
     * </pre>
     *
     * <code>.tensorflow.RPCOptions rpc_options = 13;</code>
     */
    public Builder setRpcOptions(org.tensorflow.proto.framework.RPCOptions value) {
      if (rpcOptionsBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        rpcOptions_ = value;
        onChanged();
      } else {
        rpcOptionsBuilder_.setMessage(value);
      }

      return this;
    }
    /**
     * <pre>
     * Options that apply when this session uses the distributed runtime.
     * </pre>
     *
     * <code>.tensorflow.RPCOptions rpc_options = 13;</code>
     */
    public Builder setRpcOptions(
        org.tensorflow.proto.framework.RPCOptions.Builder builderForValue) {
      if (rpcOptionsBuilder_ == null) {
        rpcOptions_ = builderForValue.build();
        onChanged();
      } else {
        rpcOptionsBuilder_.setMessage(builderForValue.build());
      }

      return this;
    }
    /**
     * <pre>
     * Options that apply when this session uses the distributed runtime.
     * </pre>
     *
     * <code>.tensorflow.RPCOptions rpc_options = 13;</code>
     */
    public Builder mergeRpcOptions(org.tensorflow.proto.framework.RPCOptions value) {
      if (rpcOptionsBuilder_ == null) {
        if (rpcOptions_ != null) {
          rpcOptions_ =
            org.tensorflow.proto.framework.RPCOptions.newBuilder(rpcOptions_).mergeFrom(value).buildPartial();
        } else {
          rpcOptions_ = value;
        }
        onChanged();
      } else {
        rpcOptionsBuilder_.mergeFrom(value);
      }

      return this;
    }
    /**
     * <pre>
     * Options that apply when this session uses the distributed runtime.
     * </pre>
     *
     * <code>.tensorflow.RPCOptions rpc_options = 13;</code>
     */
    public Builder clearRpcOptions() {
      if (rpcOptionsBuilder_ == null) {
        rpcOptions_ = null;
        onChanged();
      } else {
        rpcOptions_ = null;
        rpcOptionsBuilder_ = null;
      }

      return this;
    }
    /**
     * <pre>
     * Options that apply when this session uses the distributed runtime.
     * </pre>
     *
     * <code>.tensorflow.RPCOptions rpc_options = 13;</code>
     */
    public org.tensorflow.proto.framework.RPCOptions.Builder getRpcOptionsBuilder() {
      
      onChanged();
      return getRpcOptionsFieldBuilder().getBuilder();
    }
    /**
     * <pre>
     * Options that apply when this session uses the distributed runtime.
     * </pre>
     *
     * <code>.tensorflow.RPCOptions rpc_options = 13;</code>
     */
    public org.tensorflow.proto.framework.RPCOptionsOrBuilder getRpcOptionsOrBuilder() {
      if (rpcOptionsBuilder_ != null) {
        return rpcOptionsBuilder_.getMessageOrBuilder();
      } else {
        return rpcOptions_ == null ?
            org.tensorflow.proto.framework.RPCOptions.getDefaultInstance() : rpcOptions_;
      }
    }
    /**
     * <pre>
     * Options that apply when this session uses the distributed runtime.
     * </pre>
     *
     * <code>.tensorflow.RPCOptions rpc_options = 13;</code>
     */
    private com.google.protobuf.SingleFieldBuilderV3<
        org.tensorflow.proto.framework.RPCOptions, org.tensorflow.proto.framework.RPCOptions.Builder, org.tensorflow.proto.framework.RPCOptionsOrBuilder> 
        getRpcOptionsFieldBuilder() {
      if (rpcOptionsBuilder_ == null) {
        rpcOptionsBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
            org.tensorflow.proto.framework.RPCOptions, org.tensorflow.proto.framework.RPCOptions.Builder, org.tensorflow.proto.framework.RPCOptionsOrBuilder>(
                getRpcOptions(),
                getParentForChildren(),
                isClean());
        rpcOptions_ = null;
      }
      return rpcOptionsBuilder_;
    }

    private org.tensorflow.proto.distruntime.ClusterDef clusterDef_;
    private com.google.protobuf.SingleFieldBuilderV3<
        org.tensorflow.proto.distruntime.ClusterDef, org.tensorflow.proto.distruntime.ClusterDef.Builder, org.tensorflow.proto.distruntime.ClusterDefOrBuilder> clusterDefBuilder_;
    /**
     * <pre>
     * Optional list of all workers to use in this session.
     * </pre>
     *
     * <code>.tensorflow.ClusterDef cluster_def = 14;</code>
     */
    public boolean hasClusterDef() {
      return clusterDefBuilder_ != null || clusterDef_ != null;
    }
    /**
     * <pre>
     * Optional list of all workers to use in this session.
     * </pre>
     *
     * <code>.tensorflow.ClusterDef cluster_def = 14;</code>
     */
    public org.tensorflow.proto.distruntime.ClusterDef getClusterDef() {
      if (clusterDefBuilder_ == null) {
        return clusterDef_ == null ? org.tensorflow.proto.distruntime.ClusterDef.getDefaultInstance() : clusterDef_;
      } else {
        return clusterDefBuilder_.getMessage();
      }
    }
    /**
     * <pre>
     * Optional list of all workers to use in this session.
     * </pre>
     *
     * <code>.tensorflow.ClusterDef cluster_def = 14;</code>
     */
    public Builder setClusterDef(org.tensorflow.proto.distruntime.ClusterDef value) {
      if (clusterDefBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        clusterDef_ = value;
        onChanged();
      } else {
        clusterDefBuilder_.setMessage(value);
      }

      return this;
    }
    /**
     * <pre>
     * Optional list of all workers to use in this session.
     * </pre>
     *
     * <code>.tensorflow.ClusterDef cluster_def = 14;</code>
     */
    public Builder setClusterDef(
        org.tensorflow.proto.distruntime.ClusterDef.Builder builderForValue) {
      if (clusterDefBuilder_ == null) {
        clusterDef_ = builderForValue.build();
        onChanged();
      } else {
        clusterDefBuilder_.setMessage(builderForValue.build());
      }

      return this;
    }
    /**
     * <pre>
     * Optional list of all workers to use in this session.
     * </pre>
     *
     * <code>.tensorflow.ClusterDef cluster_def = 14;</code>
     */
    public Builder mergeClusterDef(org.tensorflow.proto.distruntime.ClusterDef value) {
      if (clusterDefBuilder_ == null) {
        if (clusterDef_ != null) {
          clusterDef_ =
            org.tensorflow.proto.distruntime.ClusterDef.newBuilder(clusterDef_).mergeFrom(value).buildPartial();
        } else {
          clusterDef_ = value;
        }
        onChanged();
      } else {
        clusterDefBuilder_.mergeFrom(value);
      }

      return this;
    }
    /**
     * <pre>
     * Optional list of all workers to use in this session.
     * </pre>
     *
     * <code>.tensorflow.ClusterDef cluster_def = 14;</code>
     */
    public Builder clearClusterDef() {
      if (clusterDefBuilder_ == null) {
        clusterDef_ = null;
        onChanged();
      } else {
        clusterDef_ = null;
        clusterDefBuilder_ = null;
      }

      return this;
    }
    /**
     * <pre>
     * Optional list of all workers to use in this session.
     * </pre>
     *
     * <code>.tensorflow.ClusterDef cluster_def = 14;</code>
     */
    public org.tensorflow.proto.distruntime.ClusterDef.Builder getClusterDefBuilder() {
      
      onChanged();
      return getClusterDefFieldBuilder().getBuilder();
    }
    /**
     * <pre>
     * Optional list of all workers to use in this session.
     * </pre>
     *
     * <code>.tensorflow.ClusterDef cluster_def = 14;</code>
     */
    public org.tensorflow.proto.distruntime.ClusterDefOrBuilder getClusterDefOrBuilder() {
      if (clusterDefBuilder_ != null) {
        return clusterDefBuilder_.getMessageOrBuilder();
      } else {
        return clusterDef_ == null ?
            org.tensorflow.proto.distruntime.ClusterDef.getDefaultInstance() : clusterDef_;
      }
    }
    /**
     * <pre>
     * Optional list of all workers to use in this session.
     * </pre>
     *
     * <code>.tensorflow.ClusterDef cluster_def = 14;</code>
     */
    private com.google.protobuf.SingleFieldBuilderV3<
        org.tensorflow.proto.distruntime.ClusterDef, org.tensorflow.proto.distruntime.ClusterDef.Builder, org.tensorflow.proto.distruntime.ClusterDefOrBuilder> 
        getClusterDefFieldBuilder() {
      if (clusterDefBuilder_ == null) {
        clusterDefBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
            org.tensorflow.proto.distruntime.ClusterDef, org.tensorflow.proto.distruntime.ClusterDef.Builder, org.tensorflow.proto.distruntime.ClusterDefOrBuilder>(
                getClusterDef(),
                getParentForChildren(),
                isClean());
        clusterDef_ = null;
      }
      return clusterDefBuilder_;
    }

    private boolean isolateSessionState_ ;
    /**
     * <pre>
     * If true, any resources such as Variables used in the session will not be
     * shared with other sessions. However, when clusterspec propagation is
     * enabled, this field is ignored and sessions are always isolated.
     * </pre>
     *
     * <code>bool isolate_session_state = 15;</code>
     */
    public boolean getIsolateSessionState() {
      return isolateSessionState_;
    }
    /**
     * <pre>
     * If true, any resources such as Variables used in the session will not be
     * shared with other sessions. However, when clusterspec propagation is
     * enabled, this field is ignored and sessions are always isolated.
     * </pre>
     *
     * <code>bool isolate_session_state = 15;</code>
     */
    public Builder setIsolateSessionState(boolean value) {
      
      isolateSessionState_ = value;
      onChanged();
      return this;
    }
    /**
     * <pre>
     * If true, any resources such as Variables used in the session will not be
     * shared with other sessions. However, when clusterspec propagation is
     * enabled, this field is ignored and sessions are always isolated.
     * </pre>
     *
     * <code>bool isolate_session_state = 15;</code>
     */
    public Builder clearIsolateSessionState() {
      
      isolateSessionState_ = false;
      onChanged();
      return this;
    }

    private boolean shareClusterDevicesInSession_ ;
    /**
     * <pre>
     * When true, WorkerSessions are created with device attributes from the
     * full cluster.
     * This is helpful when a worker wants to partition a graph
     * (for example during a PartitionedCallOp).
     * </pre>
     *
     * <code>bool share_cluster_devices_in_session = 17;</code>
     */
    public boolean getShareClusterDevicesInSession() {
      return shareClusterDevicesInSession_;
    }
    /**
     * <pre>
     * When true, WorkerSessions are created with device attributes from the
     * full cluster.
     * This is helpful when a worker wants to partition a graph
     * (for example during a PartitionedCallOp).
     * </pre>
     *
     * <code>bool share_cluster_devices_in_session = 17;</code>
     */
    public Builder setShareClusterDevicesInSession(boolean value) {
      
      shareClusterDevicesInSession_ = value;
      onChanged();
      return this;
    }
    /**
     * <pre>
     * When true, WorkerSessions are created with device attributes from the
     * full cluster.
     * This is helpful when a worker wants to partition a graph
     * (for example during a PartitionedCallOp).
     * </pre>
     *
     * <code>bool share_cluster_devices_in_session = 17;</code>
     */
    public Builder clearShareClusterDevicesInSession() {
      
      shareClusterDevicesInSession_ = false;
      onChanged();
      return this;
    }

    private org.tensorflow.proto.framework.ConfigProto.Experimental experimental_;
    private com.google.protobuf.SingleFieldBuilderV3<
        org.tensorflow.proto.framework.ConfigProto.Experimental, org.tensorflow.proto.framework.ConfigProto.Experimental.Builder, org.tensorflow.proto.framework.ConfigProto.ExperimentalOrBuilder> experimentalBuilder_;
    /**
     * <code>.tensorflow.ConfigProto.Experimental experimental = 16;</code>
     */
    public boolean hasExperimental() {
      return experimentalBuilder_ != null || experimental_ != null;
    }
    /**
     * <code>.tensorflow.ConfigProto.Experimental experimental = 16;</code>
     */
    public org.tensorflow.proto.framework.ConfigProto.Experimental getExperimental() {
      if (experimentalBuilder_ == null) {
        return experimental_ == null ? org.tensorflow.proto.framework.ConfigProto.Experimental.getDefaultInstance() : experimental_;
      } else {
        return experimentalBuilder_.getMessage();
      }
    }
    /**
     * <code>.tensorflow.ConfigProto.Experimental experimental = 16;</code>
     */
    public Builder setExperimental(org.tensorflow.proto.framework.ConfigProto.Experimental value) {
      if (experimentalBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        experimental_ = value;
        onChanged();
      } else {
        experimentalBuilder_.setMessage(value);
      }

      return this;
    }
    /**
     * <code>.tensorflow.ConfigProto.Experimental experimental = 16;</code>
     */
    public Builder setExperimental(
        org.tensorflow.proto.framework.ConfigProto.Experimental.Builder builderForValue) {
      if (experimentalBuilder_ == null) {
        experimental_ = builderForValue.build();
        onChanged();
      } else {
        experimentalBuilder_.setMessage(builderForValue.build());
      }

      return this;
    }
    /**
     * <code>.tensorflow.ConfigProto.Experimental experimental = 16;</code>
     */
    public Builder mergeExperimental(org.tensorflow.proto.framework.ConfigProto.Experimental value) {
      if (experimentalBuilder_ == null) {
        if (experimental_ != null) {
          experimental_ =
            org.tensorflow.proto.framework.ConfigProto.Experimental.newBuilder(experimental_).mergeFrom(value).buildPartial();
        } else {
          experimental_ = value;
        }
        onChanged();
      } else {
        experimentalBuilder_.mergeFrom(value);
      }

      return this;
    }
    /**
     * <code>.tensorflow.ConfigProto.Experimental experimental = 16;</code>
     */
    public Builder clearExperimental() {
      if (experimentalBuilder_ == null) {
        experimental_ = null;
        onChanged();
      } else {
        experimental_ = null;
        experimentalBuilder_ = null;
      }

      return this;
    }
    /**
     * <code>.tensorflow.ConfigProto.Experimental experimental = 16;</code>
     */
    public org.tensorflow.proto.framework.ConfigProto.Experimental.Builder getExperimentalBuilder() {
      
      onChanged();
      return getExperimentalFieldBuilder().getBuilder();
    }
    /**
     * <code>.tensorflow.ConfigProto.Experimental experimental = 16;</code>
     */
    public org.tensorflow.proto.framework.ConfigProto.ExperimentalOrBuilder getExperimentalOrBuilder() {
      if (experimentalBuilder_ != null) {
        return experimentalBuilder_.getMessageOrBuilder();
      } else {
        return experimental_ == null ?
            org.tensorflow.proto.framework.ConfigProto.Experimental.getDefaultInstance() : experimental_;
      }
    }
    /**
     * <code>.tensorflow.ConfigProto.Experimental experimental = 16;</code>
     */
    private com.google.protobuf.SingleFieldBuilderV3<
        org.tensorflow.proto.framework.ConfigProto.Experimental, org.tensorflow.proto.framework.ConfigProto.Experimental.Builder, org.tensorflow.proto.framework.ConfigProto.ExperimentalOrBuilder> 
        getExperimentalFieldBuilder() {
      if (experimentalBuilder_ == null) {
        experimentalBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
            org.tensorflow.proto.framework.ConfigProto.Experimental, org.tensorflow.proto.framework.ConfigProto.Experimental.Builder, org.tensorflow.proto.framework.ConfigProto.ExperimentalOrBuilder>(
                getExperimental(),
                getParentForChildren(),
                isClean());
        experimental_ = null;
      }
      return experimentalBuilder_;
    }
    @java.lang.Override
    public final Builder setUnknownFields(
        final com.google.protobuf.UnknownFieldSet unknownFields) {
      return super.setUnknownFields(unknownFields);
    }

    @java.lang.Override
    public final Builder mergeUnknownFields(
        final com.google.protobuf.UnknownFieldSet unknownFields) {
      return super.mergeUnknownFields(unknownFields);
    }


    // @@protoc_insertion_point(builder_scope:tensorflow.ConfigProto)
  }

  // @@protoc_insertion_point(class_scope:tensorflow.ConfigProto)
  private static final org.tensorflow.proto.framework.ConfigProto DEFAULT_INSTANCE;
  static {
    DEFAULT_INSTANCE = new org.tensorflow.proto.framework.ConfigProto();
  }

  public static org.tensorflow.proto.framework.ConfigProto getDefaultInstance() {
    return DEFAULT_INSTANCE;
  }

  private static final com.google.protobuf.Parser<ConfigProto>
      PARSER = new com.google.protobuf.AbstractParser<ConfigProto>() {
    @java.lang.Override
    public ConfigProto parsePartialFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return new ConfigProto(input, extensionRegistry);
    }
  };

  public static com.google.protobuf.Parser<ConfigProto> parser() {
    return PARSER;
  }

  @java.lang.Override
  public com.google.protobuf.Parser<ConfigProto> getParserForType() {
    return PARSER;
  }

  @java.lang.Override
  public org.tensorflow.proto.framework.ConfigProto getDefaultInstanceForType() {
    return DEFAULT_INSTANCE;
  }

}

